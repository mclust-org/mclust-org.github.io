[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Model-Based Clustering, Classification, and Density Estimation Using mclust in R",
    "section": "",
    "text": "Welcome\nThis is the website containing the 1st edition of “Model-Based Clustering, Classification, and Density Estimation Using mclust in R” by Luca Scrucca, Chris Fraley, T. Brendan Murphy, and Adrian E. Raftery, published by Chapman & Hall/CRC Press on 2023.\nThis book presents a systematic statistical approach to clustering, classification, and density estimation via Gaussian mixture modeling. This model-based framework allows the problems of choosing or developing an appropriate clustering or classification method to be understood within the context of statistical modeling. The mclust package for the statistical environment R is a widely adopted platform implementing these model-based strategies. The package includes both summary and visual functionality, complementing procedures for estimating and choosing models.\n\nR scripts\nR source code for all the chapters can be downloaded here   \n\n\nGet the book\nIf you like the book and would like to get a physical copy of the book, you can order it from the publisher Chapman & Hall/CRC or on Amazon.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/00_preface.html",
    "href": "chapters/00_preface.html",
    "title": "Preface",
    "section": "",
    "text": "Who is this book for?\nThe book is written to appeal to quantitatively trained readers from a wide range of backgrounds. An understanding of basic statistical methods, including statistical inference and statistical computing, is required. Throughout the book, examples and code are used extensively in an expository style to demonstrate the use of mclust for model-based clustering, classification, and density estimation.\nAdditionally, the book can serve as a reference for courses in multivariate analysis, statistical learning, machine learning, and data mining. It would also be a useful reference for advanced quantitative courses in application areas, including social sciences, physical sciences, and business.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00_preface.html#companion-website",
    "href": "chapters/00_preface.html#companion-website",
    "title": "Preface",
    "section": "Companion website",
    "text": "Companion website\nA companion website for this book is available at https://mclust-org.github.io/book\nThe website contains the R code to reproduce the examples and figures presented in the book, errata and various supplementary material.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00_preface.html#software-information-and-conventions",
    "href": "chapters/00_preface.html#software-information-and-conventions",
    "title": "Preface",
    "section": "Software information and conventions",
    "text": "Software information and conventions\nThe R session information when compiling this book is shown below:\n\nsessionInfo()\n## R version 4.4.0 (2024-04-24)\n## Platform: x86_64-apple-darwin20\n## Running under: macOS Ventura 13.6\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: Europe/Rome\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  utils     datasets  grDevices methods   base     \n## \n## loaded via a namespace (and not attached):\n##  [1] gtable_0.3.5        jsonlite_1.8.8      dplyr_1.1.4        \n##  [4] compiler_4.4.0      tidyselect_1.2.1    Rcpp_1.0.13        \n##  [7] parallel_4.4.0      gridExtra_2.3       scales_1.3.0       \n## [10] fastmap_1.2.0       ggplot2_3.5.1       R6_2.5.1           \n## [13] generics_0.1.3      curl_5.2.1          knitr_1.48         \n## [16] htmlwidgets_1.6.4   tibble_3.2.1        munsell_0.5.1      \n## [19] pillar_1.9.0        rlang_1.1.4         utf8_1.2.4         \n## [22] V8_4.4.2            inline_0.3.19       xfun_0.46          \n## [25] rstan_2.32.6        RcppParallel_5.1.8  cli_3.6.3          \n## [28] magrittr_2.0.3      digest_0.6.36       grid_4.4.0         \n## [31] rstudioapi_0.16.0   lifecycle_1.0.4     StanHeaders_2.32.10\n## [34] vctrs_0.6.5         evaluate_0.24.0     glue_1.7.0         \n## [37] QuickJSR_1.3.1      codetools_0.2-20    stats4_4.4.0       \n## [40] pkgbuild_1.4.4      fansi_1.0.6         colorspace_2.1-1   \n## [43] rmarkdown_2.27      tools_4.4.0         matrixStats_1.3.0  \n## [46] loo_2.8.0           pkgconfig_2.0.3     htmltools_0.5.8.1\n\nEvery R input command starts on a new line without any additional prompt (as &gt; or +). The corresponding output is shown on lines starting with two hashes ##, as it can be seen from the R session information above. Package names are in bold text (e.g., mclust), and inline code and file names are formatted in a typewriter font (e.g., data(\"iris\", package = \"datasets\")). Function names are followed by parentheses (e.g., Mclust()).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00_preface.html#about-the-authors",
    "href": "chapters/00_preface.html#about-the-authors",
    "title": "Preface",
    "section": "About the authors",
    "text": "About the authors\nLuca Scrucca Associate Professor of Statistics at Università degli Studi di Perugia, his research interests include: mixture models, model-based clustering and classification, statistical learning, dimension reduction methods, genetic and evolutionary algorithms. He is currently Associate Editor for the Journal of Statistical Software and Statistics and Computing. He has developed and he is the maintainer of several high profile R packages available on The Comprehensive R Archive Network (CRAN). His webpage is at https://luca-scr.github.io.\nChris Fraley\nMost recently a research staff member at Tableau, she previously held research positions in Statistics at the University of Washington and at Insightful from its early days as Statistical Sciences. She has contributed to computational methods in a number of areas of applied statistics, and is the principal author of several widely-used R packages. She was the originator (at Statistical Sciences) of numerical functions such as nlminb that have long been available in the R core stats package.\nT. Brendan Murphy Professor of Statistics at University College Dublin, his research interests include: model-based clustering, classification, network modeling, and latent variable modeling. He is interested in applications in social science, political science, medicine, food science, and biology. He served as Associate Editor for the journal Statistics and Computing}, he is currently Editor for the Annals of Applied Statistics and Associate Editor for Statistical Analysis and Data Mining. His webpage is at http://mathsci.ucd.ie/~brendan.\nAdrian Raftery Boeing International Professor of Statistics and Sociology, and Adjunct Professor of Atmospheric Sciences at the University of Washington, Seattle. He is also a faculty affiliate of the Center for Statistics and the Social Sciences and the Center for Studies in Demography and Ecology at University of Washington. He was one of the founding researchers in model-based clustering, having published in the area since 1984. His research interests include: model-based clustering, Bayesian statistics, social network analysis, and statistical demography. He is interested in applications in social, environmental, biological, and health sciences. He is a member of the U.S. National Academy of Sciences and was identified by Thomson-Reuter as the most cited researcher in mathematics in the world for the decade 1995–-2005. He served as Editor of the Journal of the American Statistical Association (JASA). His webpage is at http://www.stat.washington.edu/raftery.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/00_preface.html#acknowledgments",
    "href": "chapters/00_preface.html#acknowledgments",
    "title": "Preface",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe idea for writing this book arose during one of the yearly meetings of the Working Group on Model-Based Clustering, which constitutes a small but very active place for scholars from all over the world interested in mixture modeling. We thank all of the participants for providing the stimulating environment in which we started this project.\nWe are also fortunate to have benefited from a thorough review contributed by Bettina Grün, a leading expert in mixture modeling.\nWe have many others to thank for their contributions to mclust as users, collaborators, and developers. Thanks also to the R core team, and to those responsible for the many packages we have leveraged.\nThe development of the mclust package was supported over many years by the U.S. Office of Naval Research (ONR), and we acknowledge the encouragement and enthusiasm of our successive ONR program officers, Julia Abrahams and Wendy Martinez.\nChris Fraley is indebted to Tableau for supporting her efforts as co-author.\nBrendan Murphy’s research was supported by the Science Foundation Ireland (SFI) Insight Research Centre (SFI/12/RC/2289\\(\\_\\)P2), Vistamilk Research Centre (16/RC/3835) and Collegium de Lyon — Institut d’Études Avancées, Université de Lyon.\nAdrian Raftery’s research was supported by the Eunice Kennedy Shriver National Institute for Child Health and Human Development (NICHD) under grant number R01 HD070936, by the Blumstein-Jordan and Boeing International Professorships at the University of Washington,and by the Fondation des Sciences Mathématiques de Paris (FSMP) and Université Paris-Cité.\nFinally, special thanks to Rob Calver, Senior Publisher at Chapman & Hall/CRC, for his encouragement and enthusiastic support for this book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01_intro.html",
    "href": "chapters/01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Model-Based Clustering and Finite Mixture Modeling\nCluster analysis is the automatic categorization of objects into groups based on their measured characteristics. This book is about model-based approaches to cluster analysis and their implementation. It is also about how these approaches can be applied to classification and density estimation.\nThe grouping of objects based on what they have in common is a human universal and is inherent in language itself. Plato formalized the idea with his Theory of Forms, and Aristotle may have been the first to implement it empirically, classifying animals into groups based on their characteristics in his History of Animals. This was extended by Linneaus in the 18th century with his system of biological classification or taxonomy of animals and plants.\nAristotle and Linneaus classified objects subjectively, but cluster analysis is something more, using systematic numerical methods. It seems to be have been invented by Czekanowski (1909), using measures of similarity between objects based on multiple measurements. In the 1950s, there was renewed interest in the area due to the invention of new hierarchical clustering methods, including the single, average and complete linkage methods.\nThese methods are heuristic and algorithmic, leaving several key questions unanswered, such as: Which clustering method should we use? How many clusters are there? How should we treat outliers — objects that do not fall into any group? How should we assess uncertainty about an estimated clustering partition?\nEarly clustering developments were largely separate from mainstream statistics, much of which was based on a probability model for the data. The main statistical model for clustering is a finite mixture model, in which each group is modeled by its own probability distribution. The first method of this kind was latent class analysis for multivariate discrete data, developed by Paul Lazarsfeld (Lazarsfeld 1950a, 1950b).\nThe most popular model for clustering continuous-valued data is the mixture of multivariate normal distributions, introduced for this purpose by John Wolfe (Wolfe 1963, 1965, 1967, 1970), which is the main focus of this book. This modeling approach reduces the questions we mentioned to standard statistical problems such as parameter estimation and model selection. Different clustering methods often correspond approximately to different mixture models, and so choosing a method can often be done by selecting the best model. Each number of clusters corresponds to a different mixture model, so that choosing the number of clusters also becomes a model selection problem. Outliers can also be accounted for in the probability model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_intro.html#mclust",
    "href": "chapters/01_intro.html#mclust",
    "title": "1  Introduction",
    "section": "\n1.2 mclust",
    "text": "1.2 mclust\nmclust (Fraley, Raftery, and Scrucca 2022) is a popular R (R Core Team 2022) package for model-based clustering, classification, and density estimation based on finite Gaussian mixture models (GMMs). It provides an integrated approach to finite mixture models, with functions that combine model-based hierarchical clustering, the EM (Expectation-Maximization) algorithm for mixture estimation (Dempster, Laird, and Rubin 1977; McLachlan and Krishnan 2008), and several tools for model selection. A variety of covariance structures and cross-component constraints are available (see Section 2.2.1). Also included are functions for performing individual E and M steps, for simulating data from each available model, and for displaying and visualizing fitted models along with the associated clustering, classification, and density estimation results. The most recent versions of the package provide dimension reduction for visualization, resampling-based inference, additional model selection criteria, and more options for initializing the EM algorithm. A web page for the mclust package, and other related R packages, can be accessed at the URL https://mclust-org.github.io.\nmclust was first developed in 1991 by Chris Fraley and Adrian Raftery for model-based hierarchical clustering with geometric constraints (Banfield and Raftery 1993), and subsequently expanded to include constrained Gaussian mixture modeling via EM (Celeux and Govaert 1995). This extended the original methodology in John Wolfe’s NORMIX software (Wolfe 1967) by including a range of more parsimonious and statistically efficient models, and by adding methods for choosing the number of clusters and the best model, and for identifying outliers. mclust was originally implemented in the S-Plus statistical computing environment, calling Fortran for numerical operations, and using the BLAS and LAPACK numerical subroutines, which at the time were not widely available outside of Fortran. It was later ported to R by Ron Wehrens.\nEarlier versions of the package were described in Fraley and Raftery (1999), Fraley and Raftery (2003), and Fraley et al. (2012). More recent versions of the package are described in Scrucca et al. (2016). The current version at the time of writing is 6.1.2.\nmclust offers a comprehensive strategy for clustering, classification, and density estimation, and is in increasingly high demand as shown in Figure 1.1. This graph shows the monthly downloads from the RStudio CRAN mirror over the last few years, with figures calculated using the database provided by the R package cranlogs (Csárdi 2019).\n\n\n\n\n\nFigure 1.1: Number of mclust monthly downloads from the RStudio CRAN mirror over the last few years and total downloads by year.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_intro.html#overview",
    "href": "chapters/01_intro.html#overview",
    "title": "1  Introduction",
    "section": "\n1.3 Overview",
    "text": "1.3 Overview\nmclust currently includes the following features:\n\nNormal (Gaussian) mixture modeling via EM for fourteen specifications of covariance structures and cross-component constraints (structured GMMs).\nSimulation from all available model specifications.\nModel-based clustering that combines model fitting via structured GMMs with model selection using BIC and other options.\nDensity estimation using GMMs.\nMethods for combining mixture components for clustering.\nDiscriminant analysis (classification) based on structured GMMs (EDDA, MclustDA) and semi-supervised classification.\nDimension reduction methods for model-based clustering and classification.\nDisplays, including uncertainty plots, random projections, contour and perspective plots, classification plots, and density curves in one and two dimensions.\n\nmclust is a package for the R language available on CRAN at https://cran.r-project.org/web/packages/mclust and licensed under the GPL https://www.gnu.org/licenses/gpl.html. There are ready to install versions, both in binary and in source format, for several machines and operating systems. The simplest way to install the latest version of mclust from CRAN is to use the following command from the R console:\n\ninstall.packages(\"mclust\")\n\nOnce the package is installed, it can be loaded into an R session using the command:\n\nlibrary(\"mclust\")\n##                    __           __ \n##    ____ ___  _____/ /_  _______/ /_\n##   / __ `__ \\/ ___/ / / / / ___/ __/\n##  / / / / / / /__/ / /_/ (__  ) /_  \n## /_/ /_/ /_/\\___/_/\\__,_/____/\\__/   version 6.1.2\n## Type 'citation(\"mclust\")' for citing this R package in publications.\n\n\n1.3.1 Color-Blind Accessibility\nmclust includes various options to accommodate color-blind users. For details, see Section 6.6.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_intro.html#organization-of-the-book",
    "href": "chapters/01_intro.html#organization-of-the-book",
    "title": "1  Introduction",
    "section": "\n1.4 Organization of the Book",
    "text": "1.4 Organization of the Book\nThe book is organized as follows. Chapter 2 gives a general introduction to finite mixture models and the special case of Gaussian mixture models (GMMs) which is emphasized in this book. It describes common methods for parameter estimation and model selection.\nChapter 3 describes the general methodology for model-based clustering, including model estimation and selection. It discusses algorithm initialization at some length, as this is a major issue for model-based clustering.\nChapter 4 describes mixture-based classification or supervised learning. It describes various ways of assessing classifier performance, and also discusses semi-supervised classification, in which only some of the training data have known labels.\nChapter 5 describes methods for model-based univariate and multivariate density estimation. Chapter 6 describes ways of visualizing the results of model-based clustering and discusses the underlying considerations.\nFinally, Chapter 7 concludes by discussing a range of other issues, including accounting for outliers and noise. It describes Bayesian methods for avoiding the singularities that can arise in mixture modeling by adding a prior. It also describes two approaches to the common situation where clusters are not Gaussian: using an entropy criterion to combine GMM mixture components and identifying connected components. Simulation from mixture models is also discussed briefly, as well as handling large datasets, high-dimensional data, and missing data.\n\n\n\n\nBanfield, J., and Adrian E. Raftery. 1993. “Model-Based Gaussian and Non-Gaussian Clustering.” Biometrics 49: 803–21.\n\n\nCeleux, G., and G. Govaert. 1995. “Gaussian Parsimonious Clustering Models.” Pattern Recognition 28: 781–93.\n\n\nCsárdi, Gábor. 2019. cranlogs: Download Logs from the ’RStudio’ ’CRAN’ Mirror. https://CRAN.R-project.org/package=cranlogs.\n\n\nCzekanowski, J. 1909. “Zur Differential-Diagnose Der Neadertalgruppe.” Korrespondenz-Blatt Der Deutschen Geselleschaft Für Anthropologie, Ethnologie, Und Urgeschichte 40: 44–47.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm (with Discussion).” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 39: 1–38.\n\n\nFraley, Chris, and Adrian E Raftery. 1999. “MCLUST: Software for Model-Based Cluster Analysis.” Journal of Classification 16 (2): 297–306.\n\n\n———. 2003. “Enhanced Model-Based Clustering, Density Estimation, and Discriminant Analysis Software: MCLUST.” Journal of Classification 20 (2): 263–86.\n\n\nFraley, Chris, Adrian E. Raftery, Thomas Brendan Murphy, and Luca Scrucca. 2012. “MCLUST Version 4 for R: Normal Mixture Modeling for Model-Based Clustering, Classification, and Density Estimation.” Technical Report 597. Department of Statistics, University of Washington.\n\n\nFraley, Chris, Adrian E. Raftery, and Luca Scrucca. 2022. mclust: Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation. https://CRAN.R-project.org/package=mclust.\n\n\nLazarsfeld, Paul F. 1950a. “The Logical and Mathematical Foundation of Latent Structure Analysis.” In Measurement and Prediction, Volume IV of the American Soldier: Studies in Social Psychology in World War II, edited by S. A. Stouffer. Princeton University Press.\n\n\n———. 1950b. “The Logical and Mathematical Foundation of Latent Structure Analysis.” In Measurement and Prediction, edited by S. A. Stouffer, 362–412. Princeton University Press.\n\n\nMcLachlan, G. J., and T. Krishnan. 2008. The EM Algorithm and Extensions. 2nd ed. Hoboken, New Jersey: Wiley-Interscience.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nScrucca, Luca, Michael Fop, Thomas Brendan Murphy, and Adrian E. Raftery. 2016. “mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.” The R Journal 8 (1): 205–33. https://journal.r-project.org/archive/2016-1/scrucca-fop-murphy-etal.pdf.\n\n\nWolfe, John H. 1963. “Object Cluster Analysis of Social Areas.” PhD thesis, Berkeley: University of California.\n\n\n———. 1965. “A Computer Program for the Maximum Likelihood Analysis of Types.” {USNPRA} {Technical} {Bulletin} 65-15. U.S. Naval Personnel Research Activity, San Diego, CA.\n\n\n———. 1967. “NORMIX: Computational Methods for Estimating the Parameters of Multivariate Normal Mixtures of Distributions.” Naval Personnel Research Activity San Diego CA.\n\n\n———. 1970. “Pattern Clustering by Multivariate Mixture Analysis.” Multivariate Behavioral Research 5: 329–50.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02_mixture.html",
    "href": "chapters/02_mixture.html",
    "title": "2  Finite Mixture Models",
    "section": "",
    "text": "2.1 Finite Mixture Models\nMixture models encompass a powerful set of statistical tools for cluster analysis, classification, and density estimation. They provide a widely-used family of models that have proved to be an effective and computationally convenient way to model data arising in many fields, from agriculture to astronomy, economics to medicine, marketing to bioinformatics, among others. Details of finite mixture models and their applications can be found in Titterington, Smith, and Makov (1985), Geoffrey J. McLachlan and Basford (1988), G. J. McLachlan and Peel (2000), Bishop (2006, chap. 9), Frühwirth-Schnatter (2006), McNicholas (2016), Bouveyron et al. (2019). In this book our interest in mixture models will be mostly in their use for statistical learning problems, mostly unsupervised, but also supervised.\nA mixture distribution is a probability distribution obtained as a convex linear combination1 of probability density functions2.\nThe individual distributions that are combined to form the mixture distribution are called mixture components, and the weights associated with each component are called mixture weights or mixture proportions. The number of mixture components is often restricted to being finite, although in some cases it may be countably infinite.\nThe general form of the density of a finite mixture distribution  for a \\(d\\)-dimensional random vector \\(\\boldsymbol{x}\\) can be written in the form \\[\n\\sum_{k = 1}^G \\pi_k f_k(\\boldsymbol{x}; \\boldsymbol{\\theta}_k),\n\\tag{2.1}\\] where \\(G\\) is the number of mixture components, \\(f_k(\\cdot)\\) is the density of the \\(k\\)th component of the mixture (with \\(k=1, \\dots, G\\)), the \\(\\pi_k\\)’s are the mixture weights (\\(\\pi_k &gt; 0\\) and \\(\\sum_{k=1}^G \\pi_k = 1\\)), and \\(\\boldsymbol{\\theta}_k\\) represents the parameters of the \\(k\\)th density component. Typically, the component densities are taken to be known up to the parameters \\(\\boldsymbol{\\theta}_k\\) for \\(k=1,\\dots,G\\).\nIt is usually assumed that all of the densities \\(f_k(\\boldsymbol{x};\\boldsymbol{\\theta}_k)\\) belong to the same parametric family of distributions, but with different parameters. However, in some circumstances, different parametric forms are appropriate, such as in zero-inflated models where a component is introduced for modeling an excess of zeros. In Section 7.1, we introduce an additional component with a Poisson distribution to account for noise in the data.\nMixture distributions can be used to model a wide variety of random phenomena, in particular those that cannot be adequately described by a single parametric distribution. For instance, they are suitable for dealing with unobserved heterogeneity,  which occurs when a sample is drawn from a statistical population without knowledge of the presence of underlying sub-populations. In this case, the mixture components can be seen as the densities of the sub-populations, and the mixing weights are the proportions of each sub-population in the overall population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Finite Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/02_mixture.html#finite-mixture-models",
    "href": "chapters/02_mixture.html#finite-mixture-models",
    "title": "2  Finite Mixture Models",
    "section": "",
    "text": "Example 2.1   Using Gaussian mixtures to explain fish length heterogeneity\nConsider the fish length measurements (in inches) for 256 snappers attributed to Cassie (1954). The data, available as Snapper in the R package FSAdata (Ogle 2022), show a certain amount of heterogeneity with the presence of several modes. A possible explanation is that the fish belong to different age groups, but age is hard to measure, so no information is collected about this characteristic. Mixtures of Gaussian distributions with up to four components were fitted to this data, and the resulting mixture densities are shown in Figure 2.1.\n\n\n\n\n\n\n\nFigure 2.1: Distribution of fish lengths for a sample of snappers with estimated Gaussian mixtures from 1 up to 4 number of mixture components. Dashed lines represent weighed component densities, solid lines the mixture densities.\n\n\n\n\n\n\n2.1.1 Maximum Likelihood Estimation and the EM Algorithm\nGiven a random sample of observations \\(\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_n\\), the likelihood of a finite mixture model  with \\(G\\) components is given by \\[\nL(\\boldsymbol{\\Psi}) = \\prod_{i=1}^n\n\\left\\{\n  \\sum_{k=1}^G \\pi_k f_k(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_k)\n\\right\\},\n\\] where \\(\\boldsymbol{\\Psi}= (\\pi_1, \\dots, \\pi_{G-1}, \\boldsymbol{\\theta}_1, \\dots, \\boldsymbol{\\theta}_G)\\) are the parameters to be estimated. The corresponding log-likelihood is \\[\n\\ell(\\boldsymbol{\\Psi}) = \\sum_{i=1}^n \\log\\left\\{ \\sum_{k=1}^G \\pi_k f_k(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_k) \\right\\}.\n\\tag{2.2}\\] The maximum likelihood estimate (MLE)  of \\(\\boldsymbol{\\Psi}\\) is defined as a stationary point of the likelihood in the interior of the parameter space, and is thus a root of the likelihood equation \\(\\partial \\ell(\\boldsymbol{\\Psi}) / \\partial \\boldsymbol{\\Psi}= \\boldsymbol{0}\\) corresponding to a finite local maximum. However, the log-likelihood in Equation 2.2 is hard to maximize directly, even numerically (see G. J. McLachlan and Peel 2000, sec. 2.8.1). As a consequence, mixture models are usually fitted by reformulating the mixture problem as an incomplete-data problem within the EM framework.\nThe Expectation-Maximization (EM) algorithm  (Dempster, Laird, and Rubin 1977) is a general approach to maximum likelihood estimation when the data can be seen as the realization of multivariate observations \\((\\boldsymbol{x}_i, \\boldsymbol{z}_i)\\) for \\(i=1, \\dots, n\\), where the \\(\\boldsymbol{x}_i\\) are observed and the \\(\\boldsymbol{z}_i\\) are latent, unobserved variables. In the case of finite mixture models, \\(\\boldsymbol{z}_i = (z_{i1}, \\dots, z_{iG}){}^{\\!\\top}\\), where \\[\nz_{ik} =\n\\begin{cases}\n1 & \\text{if $\\boldsymbol{x}_i$ belongs to the $k$th component of the mixture,} \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n\\]\nUnder the i.i.d. (independent and identically distributed) assumption for the random variables \\((\\boldsymbol{x}_i, \\boldsymbol{z}_i)\\), the complete-data likelihood  is given by \\[\nL_C(\\boldsymbol{\\Psi}) = \\prod_{i=1}^n f(\\boldsymbol{x}_i, \\boldsymbol{z}_i ; \\boldsymbol{\\Psi})\n           = \\prod_{i=1}^n p(\\boldsymbol{z}_i)f(\\boldsymbol{x}_i ; \\boldsymbol{z}_i, \\boldsymbol{\\Psi}).\n\\]\nAssuming that the \\(\\boldsymbol{z}_i\\) are i.i.d. according to the multinomial distribution with probabilities \\((\\pi_1, \\dots, \\pi_G)\\), it follows that \\[\np(\\boldsymbol{z}_i) \\propto \\prod_{k=1}^G \\pi_k^{z_{ik}},\n\\] and \\[\nf(\\boldsymbol{x}_i ; \\boldsymbol{z}_i, \\boldsymbol{\\Psi}) = \\prod_{k=1}^G f_k(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_k)^{z_{ik}}.\n\\] Thus the complete-data log-likelihood is given by \\[\n\\ell_C(\\boldsymbol{\\Psi}) = \\sum_{i=1}^n \\sum_{k=1}^G z_{ik} \\left\\{ \\log\\pi_k + \\log f_k(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_k) \\right\\},\n\\] where \\(\\boldsymbol{\\Psi}= (\\pi_1, \\dots, \\pi_{G-1}, \\boldsymbol{\\theta}_1, \\dots, \\boldsymbol{\\theta}_G)\\) are the unknown parameters.\nThe EM algorithm is an iterative procedure whose objective function at each iteration is the conditional expectation of the complete-data log-likelihood, the Q-function, which for finite mixtures takes the form: \\[\nQ(\\boldsymbol{\\Psi}; \\boldsymbol{\\Psi}^{(t)}) =\n  \\sum_{i=1}^{n} \\sum_{k=1}^G \\widehat{z}_{ik}^{(t)}\n                 \\{ \\log\\pi_k + \\log f_k(\\boldsymbol{x}_i; \\boldsymbol{\\theta}_k) \\},\n\\] where \\(\\widehat{z}_{ik}^{(t)} = \\Exp(z_{ik} = 1 | \\boldsymbol{x}_i, \\boldsymbol{\\Psi}^{(t)})\\), the estimated conditional probability that \\(\\boldsymbol{x}_i\\) belongs to the \\(k\\)th component at iteration \\(t\\) of the EM algorithm.\nIn general, the EM algorithm for finite mixtures consists of the following steps:\n\nInitialization: set \\(t = 0\\) and choose initial values for the parameters, \\(\\boldsymbol{\\Psi}^{(0)}\\).\nE-step — estimate the latent component memberships: \\[\n\\widehat{z}_{ik}^{(t)} =\n\\widehat{\\Pr}(z_{ik} = 1 | \\boldsymbol{x}_i, \\widehat{\\boldsymbol{\\Psi}}^{(t)}) =\n\\displaystyle\n\\frac{\\pi_k^{(t)} f_k(\\boldsymbol{x}_i; \\boldsymbol{\\theta}_k^{(t)})}\n{\\sum_{j=1}^G \\pi_j^{(t)} f_j(\\boldsymbol{x}_i; \\boldsymbol{\\theta}_j^{(t)})}.\n\\]\nM-step — obtain the updated parameter estimates: \\[\n\\boldsymbol{\\Psi}^{(t+1)} = \\argmax_{\\boldsymbol{\\Psi}} Q(\\boldsymbol{\\Psi}; \\boldsymbol{\\Psi}^{(t)}).\n\\] Note that, for finite mixture models, \\[\n\\pi_k^{(t+1)} = \\displaystyle \\frac{\\sum_{i=1}^n \\widehat{z}_{ik}^{(t)}}{n}.\n\\]\nIf convergence criteria are not satisfied, set \\(t = t+1\\) and perform another E-step followed by an M-step.\n\nAs an alternative to specifying initial values for the parameters, the EM algorithm for finite mixture models can be invoked with an initial assignment of observations to mixture components. The latter is equivalent to starting EM from the M-step with \\(\\widehat{z}_{ik}^{(0)}\\) set to \\(1\\) if the \\(i\\)th observation is assigned to component \\(k\\), and \\(0\\) otherwise. More details on initialization are given in Section 2.2.3.\nProperties of the EM algorithm have been extensively studied in the literature; for a review see G. J. McLachlan and Krishnan (2008). Some of the main advantages are the following:\n\nUnless a stationary point of the log-likelihood has been reached, each EM iteration increases the log-likelihood. Although the likelihood surface for a GMM is unbounded wherever a covariance is singular, EM tends to converge to finite local maxima.\nIn many cases of practical interest, the E-steps and M-steps are more tractable in terms of implementation than direct maximization of the log-likelihood, and the cost per iteration is often relatively low.\nFor mixture models, probabilities are guaranteed to remain in \\([0,1]\\), and it is possible to implement EM for Gaussian mixture models (GMMs) in such a way that the covariance matrices cannot have negative eigenvalues.\n\nUnfortunately, there are also drawbacks such as the following:\n\nThe resulting parameter estimates can be highly dependent on their initial values, as well as on the convergence criteria.\nConvergence may be difficult to assess: not only can the asymptotic rate of convergence be slow, but progress can also be slow even when the current value is far away from a stationary point.\nThe advantages of EM may not be fully realized due to numerical issues in the implementation.\nAn estimate of the covariance matrix of the parameter estimates (needed to assess uncertainty) is not available as a byproduct of the EM computations. Methods have been developed to overcome this, such as the resampling approach described in Section 2.4.\n\n2.1.2 Issues in Maximum Likelihood Estimation\nWhen computing the MLE of a finite mixture model, some potential problems may arise. The first issue is that the mixture likelihood may be unbounded (see G. J. McLachlan and Peel 2000, sec. 2.2 and 2.5). For example, a global maximum does not exist for Gaussian mixture models (GMMs) with unequal covariance matrices (G. J. McLachlan and Peel 2000, sec. 3.8.1). Optimization methods may diverge and fail to converge to a finite local optimum. Imposing cross-cluster constraints, as discussed for GMMs in Section 2.2.1, reduces the chances of encountering unboundedness during optimization. Another approach, which can be combined with constraints, is to add a prior distribution for regularization (see Section 7.2). Further alternatives are discussed by Hathaway (1985), Ingrassia and Rocci (2007), Garcı́a-Escudero et al. (2015).\nAnother issue is that the likelihood surface often has many local maxima. If an iterative optimization method does converge to a local maximum, the corresponding parameter values will depend on how that method was initialized. Initialization strategies for the EM algorithm for GMMs are discussed in Section 2.2.3. Moreover, as mentioned above, not only can the asymptotic rate of convergence be slow, but progress can also be slow away from the optimum. As result, convergence criteria, which are typically confined to absolute or relative changes in the log-likelihood and/or parameters, may be satisfied at a non-stationary point.\nIdentifiability of the mixture components poses another potential problem. The log-likelihood in Equation 2.2 is maximized for any permutation of the order of the components (the label switching problem). This is not usually a problem with the EM algorithm for finite mixture models, but it can be a serious problem for Bayesian approaches that rely on sampling from the posterior distribution. For further details and remedies, see Frühwirth-Schnatter (2006).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Finite Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/02_mixture.html#sec-GMM",
    "href": "chapters/02_mixture.html#sec-GMM",
    "title": "2  Finite Mixture Models",
    "section": "\n2.2 Gaussian Mixture Models",
    "text": "2.2 Gaussian Mixture Models\nMixtures of Gaussian distributions  are the most popular model for continuous data, that is, numerical data that can theoretically be measured in infinitely small units. Gaussian mixture models (GMMs) are widely used in statistical learning, pattern recognition, and data mining (Celeux and Govaert 1995; C. Fraley and Raftery 2002; Stahl and Sallis 2012).\nThe probability density function of a GMM can be written as \\[\nf(\\boldsymbol{x}; \\boldsymbol{\\Psi}) = \\sum_{k=1}^G \\pi_k \\phi(\\boldsymbol{x}; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k),\n\\tag{2.3}\\] where \\(\\phi(\\cdot)\\) is the multivariate Gaussian density function with mean \\(\\boldsymbol{\\mu}_k\\) and covariance matrix \\(\\boldsymbol{\\Sigma}_k\\): \\[\n\\phi(\\boldsymbol{x}; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) =\n\\frac{1}{\\sqrt{(2\\pi)^d |\\boldsymbol{\\Sigma}_k|}}\n\\exp\\left\\{\n  -\\frac{1}{2}(\\boldsymbol{x}- \\boldsymbol{\\mu}){}^{\\!\\top}\\boldsymbol{\\Sigma}_k^{-1} (\\boldsymbol{x}- \\boldsymbol{\\mu})\n  \\right\\}.\n\\] In this case the vector of unknown parameters is given by \\(\\boldsymbol{\\Psi}= (\\pi_1, \\dots, \\pi_{G-1}, \\boldsymbol{\\mu}_1, \\dots, \\boldsymbol{\\mu}_G, \\vech\\{\\boldsymbol{\\Sigma}_1\\}, \\dots, \\vech\\{\\boldsymbol{\\Sigma}_G\\}){}^{\\!\\top}\\), where \\(\\vech\\{\\cdot\\}\\) is an operator that forms a vector by extracting the unique elements of a symmetric matrix. Alternatively, the covariance matrix can be parameterized by its Cholesky factor. This latter parameterization is used for most of the models in mclust.\nThe GMM is a flexible model that can serve different purposes. In this book we will mainly discuss applications of Gaussian mixtures in clustering (Chapter 3), classification (Chapter 4), and density estimation (Chapter 5).\n\n2.2.1 Parsimonious Covariance Decomposition\nData generated by a GMM are characterized by groups or clusters centered at the mean \\(\\boldsymbol{\\mu}_k\\), with higher density for points closer to the mean. Isosurfaces of constant density are ellipsoids whose geometric characteristics (such as volume, shape, and orientation) are determined by the covariance matrices \\(\\boldsymbol{\\Sigma}_k\\). The number of parameters per mixture component grows quadratically with the dimensionality of the data for the GMM with unrestricted component covariance matrices. Introducing cross-component constraints may help to avoid issues with near-singular covariance estimates (see Section 2.1.2).\nGeometric characteristics of the GMM components can be controlled by imposing constraints on the covariance matrices through the eigen-decomposition  (Banfield and Raftery 1993; Celeux and Govaert 1995): \\[\n\\boldsymbol{\\Sigma}_k = \\lambda_k \\boldsymbol{U}_k \\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}_k,\n\\tag{2.4}\\] where \\(\\lambda_k = |\\boldsymbol{\\Sigma}_k|^{1/d}\\) is a scalar controlling the volume, \\(\\boldsymbol{\\Delta}_k\\) is a diagonal matrix controlling the shape, such that \\(|\\boldsymbol{\\Delta}_k| = 1\\) and with the normalized eigenvalues of \\(\\boldsymbol{\\Sigma}_k\\) in decreasing order, and \\(\\boldsymbol{U}_k\\) is an orthogonal matrix of eigenvectors of \\(\\boldsymbol{\\Sigma}_k\\) controlling the orientation.\nCharacteristics of component distributions, such as volume, shape, and orientation, are usually estimated from the data, and can be allowed to vary between clusters, or constrained to be the same for all clusters (Murtagh and Raftery 1984; Flury 1988; Banfield and Raftery 1993; Celeux and Govaert 1995). Accordingly, \\((\\lambda_k, \\boldsymbol{\\Delta}_k, \\boldsymbol{U}_k)\\) can be treated as independent sets of parameters. Components that share the same value of \\(\\lambda\\) will have the same volume, while those that share the same value of \\(\\boldsymbol{\\Delta}\\) will have the same shape, and those that have the same value of \\(\\boldsymbol{U}\\) will have the same orientation.\n\n\nTable 2.1: Parameterizations of the covariance matrix \\(\\boldsymbol{\\Sigma}_k\\) for multidimensional data.\n\n\n\n\n\n\n\n\n\n\n\nLabel\nModel\nDistribution\nVolume\nShape\nOrientation\n\n\n\nEII\n\\(\\lambda \\boldsymbol{I}\\)\nSpherical\nEqual\nEqual\n—\n\n\nVII\n\\(\\lambda_k \\boldsymbol{I}\\)\nSpherical\nVariable\nEqual\n—\n\n\nEEI\n\\(\\lambda \\boldsymbol{\\Delta}\\)\nDiagonal\nEqual\nEqual\nCoordinate axes\n\n\nVEI\n\\(\\lambda_k \\boldsymbol{\\Delta}\\)\nDiagonal\nVariable\nEqual\nCoordinate axes\n\n\nEVI\n\\(\\lambda \\boldsymbol{\\Delta}_k\\)\nDiagonal\nEqual\nVariable\nCoordinate axes\n\n\nVVI\n\\(\\lambda_k \\boldsymbol{\\Delta}_k\\)\nDiagonal\nVariable\nVariable\nCoordinate axes\n\n\nEEE\n\\(\\lambda \\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\nEqual\nEqual\nEqual\n\n\nVEE\n\\(\\lambda_k \\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\nVariable\nEqual\nEqual\n\n\nEVE\n\\(\\lambda \\boldsymbol{U}\\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\nEqual\nVariable\nEqual\n\n\nVVE\n\\(\\lambda_k \\boldsymbol{U}\\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\nVariable\nVariable\nEqual\n\n\nEEV\n\\(\\lambda \\boldsymbol{U}_k \\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\nEqual\nEqual\nVariable\n\n\nVEV\n\\(\\lambda_k \\boldsymbol{U}_k \\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\nVariable\nEqual\nVariable\n\n\nEVV\n\\(\\lambda \\boldsymbol{U}_k \\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\nEqual\nVariable\nVariable\n\n\nVVV\n\\(\\lambda_k \\boldsymbol{U}_k \\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\nVariable\nVariable\nVariable\n\n\n\n\n\n\nTable 2.1 lists the 14 possible models that can be obtained for multidimensional data by varying these geometric characteristics of the component distributions. The reference label and its component-covariance model and distributional form, followed by the corresponding characteristics of volume, shape, and orientation, are given for each model. In Figure 2.2 these geometric characteristics are represented graphically for a bivariate case with three groups.\nIn the nomenclature adopted in this book and in the mclust software, E and V indicate, respectively, equal and variable characteristics across groups, while I is the identity matrix. For example, EVI denotes a model in which the volumes of all clusters are equal (E), the shapes of the clusters may vary (V), and the orientation is the identity (I). According to this model specification, clusters have diagonal covariances with orientation parallel to the coordinate axes. In the one-dimensional case there are just two possible models: E for equal variance, and V for varying variance. In all cases, the parameters associated with characteristics designated by E or V are to be determined from the data, as discussed in the next section.\n\n\n\n\n\nFigure 2.2: Ellipses of isodensity for each of the 14 Gaussian models parameterized by the eigen-decomposition of the component covariances for the case of three groups in two dimensions. The first row shows the two spherical models *II, followed by the four diagonal models **I, then the four equal-orientation models **E, and the four varying-orientation models **V.\n\n\n\n2.2.2 EM Algorithm for Gaussian Mixtures\nFor Gaussian component densities, \\(\\phi(\\boldsymbol{x}_i ; \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)\\), the log-likelihood can be written as \\[\n\\ell(\\boldsymbol{\\Psi}) = \\sum_{i=1}^n \\log\\left\\{ \\sum_{k=1}^G \\pi_k \\phi(x_i ; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right\\},\n\\] where \\(\\boldsymbol{\\Psi}\\) is the set of parameters to be estimated as described above. The complete-data log-likelihood is then given by \\[\n\\ell_C(\\boldsymbol{\\Psi}) = \\sum_{i=1}^n \\sum_{k=1}^G z_{ik} \\left\\{ \\log\\pi_k + \\log\\phi(\\boldsymbol{x}_i ; \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k) \\right\\}.\n\\]\nThe EM algorithm for GMMs  follows the general approach outlined in Section 2.1.1, with the following steps (omitting the dependence on iteration \\(t\\) for clarity of exposition):\n\nE-step: \\[\n\\widehat{z}_{ik} = \\frac{\\widehat{\\pi}_k \\phi(\\boldsymbol{x}_i ; \\widehat{\\boldsymbol{\\mu}}_k,\\widehat{\\boldsymbol{\\Sigma}}_k)}{\\sum_{g=1}^G \\widehat{\\pi}_g \\phi(\\boldsymbol{x}_i ; \\widehat{\\boldsymbol{\\mu}}_g,\\widehat{\\boldsymbol{\\Sigma}}_g)},\n\\]\nM-step: \\[\n\\widehat{\\pi}_k = \\frac{n_k}{n} \\quad\\text{and}\\quad\n\\widehat{\\boldsymbol{\\mu}}_k = \\frac{\\sum_{i=1}^n \\widehat{z}_{ik}\\boldsymbol{x}_i}{n_k},\n\\qquad\\text{where }\nn_k = \\sum_{i=1}^n \\widehat{z}_{ik}.\n\\]\n\nEstimation of \\(\\boldsymbol{\\Sigma}_k\\) depends on the adopted parameterization for the component-covariance matrices. Some simple cases are listed in the table below, where \\(\\boldsymbol{W}_k = \\sum_{i=1}^n \\widehat{z}_{ik} (\\boldsymbol{x}_i-\\widehat{\\boldsymbol{\\mu}}_k)(\\boldsymbol{x}_i-\\widehat{\\boldsymbol{\\mu}}_k){}^{\\!\\top}\\), and \\(\\boldsymbol{W}= \\sum_{k=1}^G \\boldsymbol{W}_k\\).\nCeleux and Govaert (1995) discuss the M-step for all 14 models and provide iterative methods for the 5 models (VEI, VEE, VEV, EVE, VVE) for which the M-step does not have a closed form. An alternative based on MM (Minorize-Maximization) optimization is used in mclust for the M-step in the EVE and VVE models (Browne and McNicholas 2014).\nTable 2.2 gives the complexity, measured by the number of parameters to be estimated, and indicates whether the M-step is in closed form (CF), or requires an iterative procedure (IP).\n\n\nTable 2.2: Number of estimated parameters and M-step for GMMs with different covariance parameterizations for multidimensional data.\n\n\n\n\n\n\n\n\n\nLabel\nModel\nNumber of parameters\nM-step\n\n\n\nEII\n\\(\\lambda \\boldsymbol{I}\\)\n\\((G-1) + Gd + 1\\)\nCF\n\n\nVII\n\\(\\lambda^{}_k \\boldsymbol{I}\\)\n\\((G-1) + Gd + G\\)\nCF\n\n\nEEI\n\\(\\lambda \\boldsymbol{\\Delta}\\)\n\\((G-1) + Gd + d\\)\nCF\n\n\nVEI\n\\(\\lambda^{}_k \\boldsymbol{\\Delta}\\)\n\\((G-1) + Gd + G + (d-1)\\)\nIP\n\n\nEVI\n\\(\\lambda \\boldsymbol{\\Delta}_k\\)\n\\((G-1) + Gd + 1 + G(d-1)\\)\nCF\n\n\nVVI\n\\(\\lambda^{}_k \\boldsymbol{\\Delta}_k\\)\n\\((G-1) + Gd + G + G (d-1)\\)\nCF\n\n\nEEE\n\\(\\lambda \\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\)\n\\((G-1) + Gd + 1 + (d-1) + d(d-1)/2\\)\nCF\n\n\nVEE\n\\(\\lambda^{}_k \\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\)\n\\((G-1) + Gd + G + (d-1) + d(d-1)/2\\)\nIP\n\n\nEVE\n\\(\\lambda \\boldsymbol{U}\\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}\\)\n\\((G-1) + Gd + 1 + G(d-1) + d(d-1)/2\\)\nIP\n\n\nVVE\n\\(\\lambda^{}_k \\boldsymbol{U}\\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}\\)\n\\((G-1) + Gd + G + G(d-1) + d(d-1)/2\\)\nIP\n\n\nEEV\n\\(\\lambda \\boldsymbol{U}^{}_k \\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}_k\\)\n\\((G-1) + Gd + 1 + (d-1) + Gd(d-1)/2\\)\nCF\n\n\nVEV\n\\(\\lambda^{}_k \\boldsymbol{U}^{}_k \\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}_k\\)\n\\((G-1) + Gd + G + (d-1) + Gd(d-1)/2\\)\nIP\n\n\nEVV\n\\(\\lambda \\boldsymbol{U}^{}_k \\boldsymbol{\\Delta}^{}_k \\boldsymbol{U}{}^{\\!\\top}_k\\)\n\\((G-1) + Gd + 1 + G(d-1) + Gd(d-1)/2\\)\nCF\n\n\nVVV\n\\(\\lambda^{}_k \\boldsymbol{U}^{}_k \\boldsymbol{\\Delta}^{}_k \\boldsymbol{U}{}^{\\!\\top}_k\\)\n\\((G-1) + Gd + G + G(d-1) + Gd(d-1)/2\\)\nCF\n\n\n\nNote: The number of parameters to be estimated includes \\((G-1)\\) for the mixture weights and \\(Gd\\) for the component means for all models. The number of covariance parameters varies with the model. In the M-step column, CF indicates that the M-step is available in closed form, while IP indicates that the M-step requires an iterative procedure. \n\n\n\nFigure 2.3 shows the increasing complexity of GMMs as a function of the number of mixture components and number of variables for the available models. Clearly, the number of parameters to be estimated grows much faster for more flexible models.\n\n\n\n\n\nFigure 2.3: Number of GMM estimated parameters as a function of the number of mixture components, for different numbers of variables and cross-component covariance constraints.\n\n\nFigure 2.4 shows some steps of the EM algorithm used for fitting a two-component unrestricted Gaussian mixture model to the Old Faithful data (Azzalini and Bowman 1990). More details about the dataset are given in Section 3.3.1. Here the EM algorithm is initialized by a random partition. Points are marked according to the maximum a posteriori (MAP) classification (Section 2.2.4) that assigns each \\(\\boldsymbol{x}_i\\) to the mixture component with the largest posterior conditional probability. Ellipses show the distribution of the current Gaussian components. Initially the component densities overlap to a large extent, but after only a few iterations of the EM algorithm the separation between the components clearly emerges. This is also reflected in the separation of the observed data points into two clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Illustration of some steps of the EM algorithm for fitting a two-component Gaussian mixture model to the Old Faithful data.\n\n\n\n2.2.3 Initialization of EM Algorithm\nThe EM algorithm is an iterative, strictly hill-climbing procedure whose performance can depend strongly on the starting point because the finite mixture likelihood surface tends to have multiple modes. Thus, initialization of the EM algorithm is often crucial, although no method suggested in the literature uniformly outperforms the others. Nevertheless, the EM algorithm is usually able to produce sensible results when started from reasonable starting values .\nIn the case of Gaussian mixtures, several approaches, both stochastic and deterministic, are available for selecting an initial partition of the observations or an initial estimate of the parameters. Broadly speaking, there are two general approaches for starting the EM algorithm.\nIn the first approach, the EM algorithm is initialized using a set of randomly selected parameters.  For instance, a simple strategy is based on generating several candidates by drawing parameter values uniformly at random over the feasible parameter region. Alternatively, membership probabilities can be drawn at random over the unit simplex of dimension equal to the number of mixture components. Since the random-starts strategy has a fair chance of failing to provide good initial starting values, a common suggestion is to run the EM algorithm with several random starts and choose the one resulting in the highest log-likelihood.\nAnother stochastic initialization scheme is the emEM  strategy proposed by Christophe Biernacki, Celeux, and Govaert (2003). This uses several short runs of the EM algorithm initialized with valid random starts as parameter estimates until an overall number of total iterations is exhausted. Then, the one with the highest log-likelihood is chosen to be the initializer for a long-running EM, which runs until the usual strict convergence criteria are met. The R package Rmixmod (Langrognet et al. 2022) uses this by default. However, emEM is computationally intensive and suffers from the same issues mentioned above for random starts, although to a lesser extent.\nAnother approach to initializing the EM algorithm is based on the partition obtained from some other clustering algorithm, such as \\(k\\)-means or hierarchical clustering.  In this case, the final classification is used to start the EM algorithm from the M-step. However, there are drawbacks associated with the use of these partitioning algorithms for initializing EM. For example, some have their own initialization issues, and some have a tendency to artificially impose specific shapes or patterns on clusters.\nIn the mclust R package, the EM algorithm is initialized using the partitions obtained from model-based agglomerative hierarchical clustering (MBAHC).  In this approach, \\(k\\) clusters are obtained from a large number of smaller clusters by recursively merging the two clusters that yield the maximum likelihood of a probability model over all possible merges. Banfield and Raftery (1993) proposed using the Gaussian classification likelihood as the underlying criterion. For the simplest model with equal, spherical covariance matrices, this is the same criterion that underlies the classical sum-of-squares method. Chris Fraley (1998) showed how the structure of some Gaussian models can be exploited to yield efficient regularized algorithms for agglomerative hierarchical clustering. Further details are given in Section 3.6 and Section 3.7.\n\n2.2.4 Maximum A Posteriori (MAP) Classification\nGiven a dataset \\(\\mathcal{X}= \\{ \\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_n \\}\\), a hard partition of the observed data points into \\(G\\) clusters, denoted as \\(\\mathcal{C}= \\{ C_1, C_2, \\dots, C_G \\}\\) such that \\(C_k \\,\\cap\\, C_g = \\emptyset\\) (for \\(k \\ne g\\)) and \\(\\bigcup_{k=1}^{G} C_k = \\mathcal{X}\\), is straightforward to obtain in finite mixture modeling.\nOnce a GMM has been successfully fitted and the MLEs of the parameters obtained, a maximum a posteriori (MAP)  procedure can be applied, assigning each \\(\\boldsymbol{x}_i\\) to the mixture component with the largest posterior conditional probability: \\[\n\\boldsymbol{x}_i \\in C_{k^*}\n\\qquad\\text{with}\\quad\nk^* = \\argmax_k\\; \\widehat{z}_{ik},\n\\] where \\[\n\\widehat{z}_{ik} = \\frac{\\widehat{\\pi}_k \\phi(\\boldsymbol{x}_i; \\widehat{\\boldsymbol{\\mu}}_k, \\widehat{\\boldsymbol{\\Sigma}}_k)}{\\displaystyle\\sum_{g=1}^G \\widehat{\\pi}_g \\phi(\\boldsymbol{x}; \\widehat{\\boldsymbol{\\mu}}_g, \\widehat{\\boldsymbol{\\Sigma}}_g)}\n\\tag{2.5}\\] is the posterior conditional probability of an observation \\(i\\) coming from mixture component \\(k\\) (\\(k= 1, \\dots, G\\)). A measure of classification uncertainty  for each data point can also be computed as \\[\nu_i = 1 - \\max_k \\widehat{z}_{ik},\n\\] which falls within the interval \\([0,1]\\). Values close to zero indicate a low level of uncertainty in the classification of the corresponding observation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Finite Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/02_mixture.html#sec-modsel_mixture",
    "href": "chapters/02_mixture.html#sec-modsel_mixture",
    "title": "2  Finite Mixture Models",
    "section": "\n2.3 Model Selection",
    "text": "2.3 Model Selection\nA central question in finite mixture modeling is that of determining how many components should be included in the mixture. In GMMs we need also to decide which covariance parameterization to adopt. Both questions can be addressed by model selection criteria, such as the Bayesian information criterion (BIC) or the integrated complete-data likelihood (ICL) criterion. The selection of the number of mixture components or clusters can also be done by formal hypothesis testing.\n\n2.3.1 Information Criteria\nInformation criteria are usually based on penalized forms of the likelihood. In general, as the log-likelihood increases with the addition of more parameters in a statistical model, a penalty term for the number of estimated parameters is included to account for the model complexity (Claeskens and Hjort 2008; Konishi and Kitagawa 2008).\nLet \\(\\boldsymbol{x}_1,\\dots,\\boldsymbol{x}_n\\) be a random sample of \\(n\\) independent observations. Consider a parametric family of density functions \\(\\{f(\\boldsymbol{x}; \\boldsymbol{\\theta}); \\boldsymbol{\\theta}\\in \\boldsymbol{\\Theta}\\}\\), for which the log-likelihood can be computed as \\(\\ell(\\boldsymbol{\\theta}; \\boldsymbol{x})  = \\log L(\\boldsymbol{\\theta}; \\boldsymbol{x}) = \\sum_{i=1}^n \\log f(\\boldsymbol{x}_i ; \\boldsymbol{\\theta})\\), where \\(\\widehat{\\boldsymbol{\\theta}}\\) is the MLE (the value that maximizes the log-likelihood). The Bayesian information criterion (BIC),  originally introduced by Schwartz (1978), is a popular criterion for model selection that penalizes the log-likelihood by introducing a penalty term: \\[\n\\BIC = 2\\ell(\\widehat{\\boldsymbol{\\theta}} ; \\boldsymbol{x}) - \\nu_{\\boldsymbol{\\theta}}\\log(n),\n\\] where \\(\\ell(\\widehat{\\boldsymbol{\\theta}} ; \\boldsymbol{x})\\) is the maximized log-likelihood, \\(n\\) is the sample size, and \\(\\nu_{\\boldsymbol{\\theta}}\\) is the number of parameters to be estimated.\nKass and Raftery (1995) showed that, assuming prior unit information, BIC provides an approximation to the Bayes factor for comparing two competing models, say \\(\\Model_1\\) and \\(\\Model_2\\): \\[\n2\\log B_{12} \\approx \\BIC_{\\Model_1} - \\BIC_{\\Model_2} = \\Delta_{12}.\n\\] Assuming that \\(\\Model_2\\) has the smaller BIC value, the strength of the evidence against it can be summarized as follows:\n\n\n\\(\\Delta_{12}\\)\nEvidence to favor \\(\\Model_1\\) over \\(\\Model_2\\)\n\n\n\n\n0 – 2\nNot worth more than a bare mention\n\n\n2 – 6\nPositive\n\n\n6 – 10\nStrong\n\n\n\\(&gt;10\\)\nVery Strong\n\n\n\nFor a review of BIC, its derivation, its properties and applications see Neath and Cavanaugh (2012).\nThe BIC is a widely adopted criterion for model selection in finite mixture models, both for density estimation (Roeder and Wasserman 1997) and for clustering (C. Fraley and Raftery 1998). For mixture models, it takes the following form: \\[\n\\BIC_{\\Model, G} = 2\\ell_{\\Model, G}(\\widehat{\\boldsymbol{\\Psi}} ; \\boldsymbol{x}) - \\nu_{\\Model, G}\\log(n),\n\\] where \\(\\ell_{\\Model, G}(\\widehat{\\boldsymbol{\\Psi}} ; \\boldsymbol{x})\\) is the log-likelihood at the MLE \\(\\widehat{\\boldsymbol{\\Psi}}\\) for model \\(\\Model\\) with \\(G\\) components, \\(n\\) is the sample size, and \\(\\nu_{\\Model, G}\\) is the number of parameters to be estimated. The model \\(\\Model\\) and number of components \\(G\\) are chosen so as to maximize \\(\\BIC_{\\Model, G}\\). Keribin (2000) showed that BIC is consistent for choosing the number of components in a mixture model, under the assumption that the likelihood is bounded. Although GMM likelihoods have an infinite spike wherever one or more covariances is singular, BIC is nevertheless often used for model selection among GMMs.\nThe BIC tends to select the number of mixture components needed to approximate the density, rather than the number of clusters as such. For this reason, other criteria have been proposed for model selection in clustering, like the integrated complete-data likelihood (ICL) criterion  (C. Biernacki, Celeux, and Govaert 2000): \\[\n\\ICL_{\\Model, G} = \\BIC_{\\Model, G} + 2 \\sum_{i=1}^n\\sum_{k=1}^G c_{ik} \\log(\\widehat{z}_{ik}),\n\\] where \\(\\widehat{z}_{ik}\\) is the conditional probability that \\(\\boldsymbol{x}_i\\) arises from the \\(k\\)th mixture component from equation Equation 2.5, and \\(c_{ik} = 1\\) if the \\(i\\)th observation is assigned to cluster \\(k\\), i.e. \\(\\boldsymbol{x}_i \\in C_k\\), and 0 otherwise. ICL penalizes the BIC through an entropy term which measures the overlap between clusters. Provided that the clusters do not overlap too much, ICL has shown good performance in selecting the number of clusters, with a preference for solutions with well-separated groups.\n\n2.3.2 Likelihood Ratio Testing\nIn addition to the information criteria just mentioned, the choice of the number of components in a mixture model for a specific component-covariance parameterization can be carried out by likelihood ratio testing (LRT);  see Geoffrey J. McLachlan and Rathnayake (2014) for a review.\nSuppose we want to test the null hypothesis \\(G = G_0\\) against the alternative \\(G = G_1\\) for some \\(G_1 &gt; G_0\\), so that Usually, \\(G_1 = G_0 + 1\\), so a common procedure is to keep adding components sequentially. Let \\(\\widehat{\\boldsymbol{\\Psi}}_{G_j}\\) be the MLE of \\(\\boldsymbol{\\Psi}\\) calculated under \\(H_j: G = G_j\\) (for \\(j = 0,1\\)). The likelihood ratio test statistic (LRTS) can be written as \\[\n\\LRTS = -2\\log\\{L(\\widehat{\\boldsymbol{\\Psi}}_{G_0})/L(\\widehat{\\boldsymbol{\\Psi}}_{G_1})\\}\n      = 2\\{ \\ell(\\widehat{\\boldsymbol{\\Psi}}_{G_1}) - \\ell(\\widehat{\\boldsymbol{\\Psi}}_{G_0}) \\},\n\\] where large values of LRTS provide evidence against the null hypothesis. For mixture models, however, standard regularity conditions do not hold for the null distribution of the LRTS to have its usual chi-squared distribution . As a result, the significance of the LRT is often assessed using a resampling approach in order to obtain a \\(p\\)-value. Geoffrey J. McLachlan (1987) proposed using the bootstrap to obtain the null distribution of the LRTS. The bootstrap procedure is the following:\n\na bootstrap sample \\(\\boldsymbol{x}_b^*\\) is generated by simulating from the fitted model under the null hypothesis with \\(G_0\\) components, namely, from the GMM distribution with the vector of unknown parameters replaced by MLEs obtained from the original data under \\(H_0\\);\nthe test statistic \\(\\LRTS^*_b\\) is computed for the bootstrap sample \\(\\boldsymbol{x}_b^*\\) after fitting GMMs with \\(G_0\\) and \\(G_1\\) number of components;\nsteps 1. and 2. are replicated \\(B\\) times, say \\(B = 999\\), to obtain the bootstrap null distribution of \\(\\LRTS^*\\).\n\nA bootstrap-based approximation of the \\(p\\)-value may then be computed as \\[\np\\text{-value} \\approx \\frac{1 + \\displaystyle\\sum_{b=1}^B \\mathnormal{I}(\\LRTS^*_b \\ge  \\LRTS_\\text{obs})}{B+1} ,\n\\] where \\(\\LRTS_\\text{obs}\\) is the test statistic computed on the observed sample, and \\(\\mathnormal{I}(\\cdot)\\) denotes the indicator function, which is equal to 1 if its argument is true and 0 otherwise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Finite Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/02_mixture.html#sec-resamp_inference",
    "href": "chapters/02_mixture.html#sec-resamp_inference",
    "title": "2  Finite Mixture Models",
    "section": "\n2.4 Resampling-Based Inference",
    "text": "2.4 Resampling-Based Inference\nThe EM algorithm for Gaussian mixtures provides an efficient way to obtain parameter estimates (see Section 2.2.2). However, as already mentioned in Section 2.1.1, the EM algorithm does not provide estimates of the uncertainty associated with the parameter estimates. Likelihood-based inference in mixture models is usually addressed through either information-based methods or resampling (G. J. McLachlan and Peel 2000; G. J. McLachlan and Krishnan 2008).\nIn information-based methods (Meng and Rubin 1991), the covariance matrix of the MLE \\(\\widehat{\\boldsymbol{\\Psi}}\\) is approximated by the inverse of the observed information matrix \\(I^{-1}(\\widehat{\\boldsymbol{\\Psi}})\\): \\[\n\\Cov (\\widehat{\\boldsymbol{\\Psi}}) \\approx I^{-1}(\\widehat{\\boldsymbol{\\Psi}}).\n\\] Although valid asymptotically (Boldea and Magnus 2009), “the sample size \\(n\\) has to be very large before the asymptotic theory applies to mixture models” (G. J. McLachlan and Peel 2000, ~42). Indeed, Basford et al. (1997) found that standard errors obtained using the expected or the observed information matrix are unstable unless the sample size is very large. For these reasons, they advocate the use of a resampling approach based on the bootstrap.\nThe bootstrap  (Efron 1979) is a general, widely applicable, powerful technique for obtaining an approximation to the sampling distribution of a statistic of interest. The bootstrap distribution is approximated by drawing a large number of samples (bootstrap samples) from the empirical distribution, by resampling with replacement from the observed data (nonparametric bootstrap), or from a parametric distribution with unknown parameters substituted by the corresponding estimates (parametric bootstrap).\nLet \\(\\widehat{\\boldsymbol{\\Psi}}\\) be the estimate of a set of GMM parameters \\(\\boldsymbol{\\Psi}\\) for a given model \\(\\Model\\) and number of mixture components \\(G\\). A bootstrap estimate of the corresponding standard errors can be obtained as follows:\n\n\nObtain the bootstrap distribution for the parameters of interest by:\n\ndrawing a sample of size \\(n\\) with replacement from the empirical distribution \\((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n)\\) to form the bootstrap sample \\((\\boldsymbol{x}^*_1, \\dots, \\boldsymbol{x}^*_n)\\);\nfitting a GMM \\((\\Model, G)\\) to get the bootstrap estimates \\(\\widehat{\\boldsymbol{\\Psi}}^*\\);\nreplicating steps 1–2 a large number of times, say \\(B\\), to obtain \\(\\widehat{\\boldsymbol{\\Psi}}^*_1, \\widehat{\\boldsymbol{\\Psi}}^*_2, \\dots, \\widehat{\\boldsymbol{\\Psi}}^*_B\\) estimates from \\(B\\) resamples.\n\n\nAn approximate covariance matrix for the parameter estimates is then \\[\n\\Cov_{\\boot}(\\widehat{\\boldsymbol{\\Psi}}) =\n\\frac{1}{B-1} \\sum_{b=1}^B\n               (\\widehat{\\boldsymbol{\\Psi}}^*_b - \\overline{\\widehat{\\boldsymbol{\\Psi}}}^*)\n               (\\widehat{\\boldsymbol{\\Psi}}^*_b - \\overline{\\widehat{\\boldsymbol{\\Psi}}}^*){}^{\\!\\top}\n\\] where \\(\\displaystyle\\overline{\\widehat{\\boldsymbol{\\Psi}}}^* = \\frac{1}{B}\\sum_{b=1}^B \\widehat{\\boldsymbol{\\Psi}}^*_b\\).\nThe bootstrap standard errors for the parameter estimates \\(\\widehat{\\boldsymbol{\\Psi}}\\) are computed as the square root of the diagonal elements of the bootstrap covariance matrix: \\[\n\\se_{\\boot}(\\widehat{\\boldsymbol{\\Psi}}) = \\sqrt{ \\diag(\\Cov_{\\boot}(\\widehat{\\boldsymbol{\\Psi}})) }.\n\\]\n\nThe bootstrap procedure outlined above can also be used for estimating confidence intervals. For instance, bootstrap percentile confidence intervals for any GMM parameter \\(\\psi\\) of \\(\\boldsymbol{\\Psi}\\) are computed as \\([\\psi^*_{\\alpha/2}, \\psi^*_{1-\\alpha/2}]\\), where \\(\\psi^*_q\\) is the \\(q\\)th quantile (or the 100\\(q\\)th percentile) of the bootstrap distribution \\((\\widehat{\\psi}^*_1, \\dots, \\widehat{\\psi}^*_B)\\).\nDifferent resampling methods have been considered for Gaussian mixtures, such as the parametric bootstrap (Basford et al. 1997), the nonparametric bootstrap (G. J. McLachlan and Peel 2000), and the jackknife (Efron 1979, Efron:1982). A generalization of the nonparametric bootstrap is the weighted likelihood bootstrap  (Newton and Raftery 1994), which assigns random (positive) weights to sample observations. The weights are obtained from a uniform Dirichlet distribution, by sampling from \\(n\\) independent standard exponential distributions and then rescaling by their average. The weighted likelihood bootstrap can also be viewed as a generalized Bayesian bootstrap. The weighted likelihood bootstrap may yield benefits when one or more components have small mixture proportions. In that case, a nonparametric bootstrap sample may have no representatives of them, whereas the weighted likelihood bootstrap always has representatives of all groups.\nFor a recent review and comparison of these resampling approaches to inference in finite mixture models see O’Hagan et al. (2019).\n\n\n\n\nAzzalini, A, and A W Bowman. 1990. “A Look at Some Data on the Old Faithful Geyser.” Applied Statistics 39 (3): 357–65.\n\n\nBanfield, J., and Adrian E. Raftery. 1993. “Model-Based Gaussian and Non-Gaussian Clustering.” Biometrics 49: 803–21.\n\n\nBasford, K E, D R Greenway, G J McLachlan, and D Peel. 1997. “Standard Errors of Fitted Component Means of Normal Mixtures.” Computational Statistics 12 (1): 1–18.\n\n\nBiernacki, C., G. Celeux, and G. Govaert. 2000. “Assessing a Mixture Model for Clustering with the Integrated Completed Likelihood.” IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (7): 719–25.\n\n\nBiernacki, Christophe, Gilles Celeux, and Gérard Govaert. 2003. “Choosing Starting Values for the EM Algorithm for Getting the Highest Likelihood in Multivariate Gaussian Mixture Models.” Computational Statistics & Data Analysis 41 (3): 561–75.\n\n\nBishop, Christopher. 2006. Pattern Recognition and Machine Learning. New York: Springer-Verlag Inc.\n\n\nBoldea, Otilia, and Jan R Magnus. 2009. “Maximum Likelihood Estimation of the Multivariate Normal Mixture Model.” Journal of the American Statistical Association 104 (488): 1539–49.\n\n\nBouveyron, Charles, Gilles Celeux, T. Brendan Murphy, and Adrian E. Raftery. 2019. Model-Based Clustering and Classification for Data Science: With Applications in r. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge: Cambridge University Press.\n\n\nBrowne, Ryan P, and Paul D McNicholas. 2014. “Estimating Common Principal Components in High Dimensions.” Advances in Data Analysis and Classification 8 (2): 217–26.\n\n\nCassie, Richard Morrison. 1954. “Some Uses of Probability Paper in the Analysis of Size Frequency Distributions.” Marine and Freshwater Research 5 (3): 513–22.\n\n\nCeleux, G., and G. Govaert. 1995. “Gaussian Parsimonious Clustering Models.” Pattern Recognition 28: 781–93.\n\n\nClaeskens, Gerda, and Nils Lid Hjort. 2008. Model Selection and Model Averaging. Cambridge: Cambridge University Press.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm (with Discussion).” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 39: 1–38.\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” Annals of Statistics 7: 1–26.\n\n\nFlury, Bernhard. 1988. Common Principal Components & Related Multivariate Models. John Wiley & Sons, Inc.\n\n\nFraley, Chris. 1998. “Algorithms for Model-Based Gaussian Hierarchical Clustering.” SIAM Journal on Scientific Computing 20 (1): 270–81.\n\n\nFraley, C., and A. E. Raftery. 1998. “How Many Clusters? Which Clustering Method? Answers via Model-Based Cluster Analysis.” The Computer Journal 41: 578–88.\n\n\n———. 2002. “Model-Based Clustering, Discriminant Analysis, and Density Estimation.” Journal of the American Statistical Association 97 (458): 611–31.\n\n\nFrühwirth-Schnatter, Sylvia. 2006. Finite Mixture and Markov Switching Models. Springer.\n\n\nGarcı́a-Escudero, Luis Angel, Alfonso Gordaliza, Carlos Matrán, and Agustı́n Mayo-Iscar. 2015. “Avoiding Spurious Local Maximizers in Mixture Modeling.” Statistics and Computing 25 (3): 619–33.\n\n\nHathaway, Richard J. 1985. “A Constrained Formulation of Maximum-Likelihood Estimation for Normal Mixture Distributions.” Annals of Statistics 13: 795–800.\n\n\nIngrassia, Salvatore, and Roberto Rocci. 2007. “Constrained Monotone EM Algorithms for Finite Mixture of Multivariate Gaussians.” Computational Statistics & Data Analysis 51 (11): 5339–51.\n\n\nKass, R. E., and A. E. Raftery. 1995. “Bayes Factors.” Journal of the American Statistical Association 90: 773–95.\n\n\nKeribin, C. 2000. “Consistent Estimation of the Order of Mixture Models.” Sankhya Series A 62 (1): 49–66.\n\n\nKonishi, Sadanori, and Genshiro Kitagawa. 2008. Information Criteria and Statistical Modeling. Springer Science & Business Media.\n\n\nLangrognet, Florent, Remi Lebret, Christian Poli, Serge Iovleff, Benjamin Auder, and Serge Iovleff. 2022. Rmixmod: Classification with Mixture Modelling. https://CRAN.R-project.org/package=Rmixmod.\n\n\nMcLachlan, G. J., and T. Krishnan. 2008. The EM Algorithm and Extensions. 2nd ed. Hoboken, New Jersey: Wiley-Interscience.\n\n\nMcLachlan, G. J., and D. Peel. 2000. Finite Mixture Models. New York: Wiley.\n\n\nMcLachlan, Geoffrey J. 1987. “On Bootstrapping the Likelihood Ratio Test Statistic for the Number of Components in a Normal Mixture.” Applied Statistics 36: 318–24.\n\n\nMcLachlan, Geoffrey J, and Kaye E Basford. 1988. Mixture Models: Inference and Applications to Clustering. New York: Marcel Dekker Inc.\n\n\nMcLachlan, Geoffrey J, and Suren Rathnayake. 2014. “On the Number of Components in a Gaussian Mixture Model.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 4 (5): 341–55.\n\n\nMcNicholas, Paul D. 2016. Mixture Model-Based Classification. CRC Press.\n\n\nMeng, Xiao-Li, and Donald B Rubin. 1991. “Using EM to Obtain Asymptotic Variance-Covariance Matrices: The SEM Algorithm.” Journal of the American Statistical Association 86 (416): 899–909.\n\n\nMurtagh, Fionn, and Adrian E Raftery. 1984. “Fitting Straight Lines to Point Patterns.” Pattern Recognition 17 (5): 479–83.\n\n\nNeath, Andrew A., and Joseph E. Cavanaugh. 2012. “The Bayesian Information Criterion: Background, Derivation, and Applications.” Wiley Interdisciplinary Reviews: Computational Statistics 4 (2): 199–203. https://doi.org/10.1002/wics.199.\n\n\nNewton, Michael A, and Adrian E Raftery. 1994. “Approximate Bayesian Inference with the Weighted Likelihood Bootstrap (with Discussion).” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 56: 3–48.\n\n\nO’Hagan, Adrian, Thomas Brendan Murphy, Luca Scrucca, and Isobel Claire Gormley. 2019. “Investigation of Parameter Uncertainty in Clustering Using a Gaussian Mixture Model via Jackknife, Bootstrap and Weighted Likelihood Bootst Rap.” Computational Statistics 34 (4): 1779–1813. https://doi.org/10.1007/s00180-019-00897-9.\n\n\nOgle, Derek. 2022. FSAdata: Fisheries Stock Analysis, Datasets.\n\n\nRoeder, K., and L. Wasserman. 1997. “Practical Bayesian Density Estimation Using Mixtures of Normals.” Journal of the American Statistical Association 92 (439): 894–902.\n\n\nSchwartz, G. 1978. “Estimating the Dimension of a Model.” Annals of Statistics 6: 31–38.\n\n\nStahl, D., and H. Sallis. 2012. “Model-Based Cluster Analysis.” Wiley Interdisciplinary Reviews: Computational Statistics 4 (4): 341–58. https://doi.org/10.1002/wics.1204.\n\n\nTitterington, D Michael, Adrian FM Smith, and Udi E Makov. 1985. Statistical Analysis of Finite Mixture Distributions. Chichester; New York: John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Finite Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/02_mixture.html#footnotes",
    "href": "chapters/02_mixture.html#footnotes",
    "title": "2  Finite Mixture Models",
    "section": "",
    "text": "Roughly speaking, a convex linear combination is a weighted sum of terms with non-negative weights that sum to one.↩︎\nA probability density function may be defined with respect to an appropriate measure on \\(\\Real^d\\), which can be the Lebesgue measure, a counting measure, or a combination of the two, depending on the context.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Finite Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html",
    "href": "chapters/03_cluster.html",
    "title": "3  Model-Based Clustering",
    "section": "",
    "text": "3.1 Gaussian Mixture Models for Cluster Analysis\nCluster analysis  refers to a broad set of multivariate statistical methods and techniques which seek to identify homogeneous subgroups of cases in a dataset, that is, to partition similar observations into meaningful and useful clusters. Cluster analysis is an instance of unsupervised learning,  since the presence and number of clusters may not be known a priori, nor is case labeling available.\nMany approaches to clustering have been proposed in the literature for exploring the underlying group structure of data. Traditional methods are combinatorial in nature, and either hierarchical (agglomerative or divisive) or non-hierarchical (for example, \\(k\\)-means). Although some are related to formal statistical models, they typically are based on heuristic procedures which make no explicit assumptions about the structure of the clusters. The choice of clustering method, similarity measures, and interpretation have tended to be informal and often subjective.\nModel-based clustering (MBC) is a probabilistic approach to clustering in which each cluster corresponds to a mixture component described by a probability distribution with unknown parameters. The type of distribution is often specified a priori (most commonly Gaussian), whereas the model structure (including the number of components) remains to be determined by parameter estimation and model-selection techniques. Parameters can be estimated by maximum likelihood, using, for instance, the EM algorithm, or by Bayesian approaches. These model-based methods are increasingly favored over heuristic clustering methods. Moreover, the one-to-one relationship between mixture components and clusters can be relaxed, as discussed in Section 7.3.\nModel-based clustering can be viewed as a generative approach because it attempts to learn generative models from the data, with each model representing one particular cluster. It is possible to simulate from an estimated model, as discussed in Section 7.4, which can be useful for resampling inference, sensitivity, and predictive analysis.\nThe finite mixture of Gaussian distributions described in Section 2.2 is one of the most flexible classes of models for continuous variables and, as such, it has become a method of choice in a wide variety of settings. Component covariance matrices can be parameterized by eigen-decomposition to impose geometric constraints, as described in Section 2.2.1. Constraining some but not all of the quantities in Equation 2.4 to be equal between clusters yields parsimonious and easily interpretable models, appropriate for diverse clustering situations. Once the parameters of a Gaussian mixture model (GMM) are estimated via EM, a partition \\(\\mathcal{C}=\\{ C_1, \\dots, C_G \\}\\) of the data into \\(G\\) clusters can be derived by assigning each observation \\(\\boldsymbol{x}_i\\) to the component with the largest conditional probability that \\(\\boldsymbol{x}_i\\) arises from that component distribution — the MAP principle discussed in Section 2.2.4.\nThe parameterization includes, but is not restricted to, well-known covariance models that are associated with various criteria for hierarchical clustering: equal-volume spherical variance (sum of squares criterion) (Ward 1963), constant variance (Friedman and Rubin 1967), and unconstrained variance (Scott and Symons 1971). Moreover, \\(k\\)-means, although it does not make an explicit assumption about the structure of the data, is related to the GMM with spherical, equal volume components (Celeux and Govaert 1995).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#sec-clust_mclust",
    "href": "chapters/03_cluster.html#sec-clust_mclust",
    "title": "3  Model-Based Clustering",
    "section": "\n3.2 Clustering in mclust",
    "text": "3.2 Clustering in mclust\nThe main function to fit GMMs for clustering is called Mclust().  This function requires a numeric matrix or data frame, with \\(n\\) observations along the rows and \\(d\\) variables along the columns, to be supplied as the data argument. In the univariate case (\\(d = 1\\)), the data can be input as a vector. The number of mixture components \\(G\\), and the model name corresponding to the eigen-decomposition discussed in Section 2.2.1, can also be specified. By default, all of the available models are estimated for up to \\(G=9\\) mixture components, but this number can be increased by specifying the argument G accordingly.\nTable 3.1 lists the models currently implemented in mclust, both in the univariate and multivariate cases. Options for specification of a noise component and a prior distribution are also shown; these will be discussed in Section 7.1 and Section 7.2, respectively.\n\n\nTable 3.1: Models available in the mclust package for hierarchical clustering (HC), Gaussian mixture models (GMM) fitted by the EM algorithm, with noise component or prior (indicates availability).\n\n\n\n\n\n\n\n\n\n\n\n\nLabel\nModel\nDistribution\nHC\nGMM\nGMM + noise\nGMM + prior\n\n\n\nX\n\\(\\sigma^2\\)\nUnivariate\n\n✓\n✓\n✓\n\n\nE\n\\(\\sigma^2\\)\nUnivariate\n✓\n✓\n✓\n✓\n\n\nV\n\\(\\sigma_k^2\\)\nUnivariate\n✓\n✓\n✓\n✓\n\n\nXII\n\\(\\diag(\\sigma^2, \\ldots, \\sigma^2)\\)\nSpherical\n\n✓\n✓\n✓\n\n\nXXI\n\\(\\diag(\\sigma_1^2, \\ldots, \\sigma_d^2)\\)\nDiagonal\n\n✓\n✓\n✓\n\n\nXXX\n\\(\\boldsymbol{\\Sigma}\\)\nEllipsoidal\n\n✓\n✓\n✓\n\n\nEII\n\\(\\lambda \\boldsymbol{I}\\)\nSpherical\n✓\n✓\n✓\n✓\n\n\nVII\n\\(\\lambda_k \\boldsymbol{I}\\)\nSpherical\n✓\n✓\n✓\n✓\n\n\nEEI\n\\(\\lambda \\boldsymbol{\\Delta}\\)\nDiagonal\n\n✓\n✓\n✓\n\n\nVEI\n\\(\\lambda_k \\boldsymbol{\\Delta}\\)\nDiagonal\n\n✓\n✓\n✓\n\n\nEVI\n\\(\\lambda \\boldsymbol{\\Delta}_k\\)\nDiagonal\n\n✓\n✓\n✓\n\n\nVVI\n\\(\\lambda_k \\boldsymbol{\\Delta}_k\\)\nDiagonal\n\n✓\n✓\n✓\n\n\nEEE\n\\(\\lambda \\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\n✓\n✓\n✓\n✓\n\n\nVEE\n\\(\\lambda_k \\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\n\n✓\n✓\n\n\n\nEVE\n\\(\\lambda \\boldsymbol{U}\\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\n\n✓\n✓\n\n\n\nVVE\n\\(\\lambda_k \\boldsymbol{U}\\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}\\)\nEllipsoidal\n\n✓\n✓\n\n\n\nEEV\n\\(\\lambda \\boldsymbol{U}_k \\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\n\n✓\n✓\n✓\n\n\nVEV\n\\(\\lambda_k \\boldsymbol{U}_k \\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\n\n✓\n✓\n✓\n\n\nEVV\n\\(\\lambda \\boldsymbol{U}_k \\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\n\n✓\n✓\n\n\n\nVVV\n\\(\\lambda_k \\boldsymbol{U}_k \\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}_k\\)\nEllipsoidal\n✓\n✓\n✓\n✓\n\n\n\n\n\n\nThe diagram in Figure 3.1 presents the flow chart of the model-based clustering approach  implemented in the function Mclust() of the mclust package.\n\n\n\n\n\nFigure 3.1: Flow chart of the model-based clustering approach implemented in mclust. \\(\\mathcal{G}\\) indicates the set containing the number of mixture components, by default \\(\\mathcal{G} = \\{1,2,\\dots,9\\}\\). \\(\\mathcal{M}\\) is the set of within-cluster covariance models, by default all of the available models listed in Table 3.1. BIC is the default of several model-selection options.\n\n\nGiven numerical data, a model-based agglomerative hierarchical clustering (MBAHC) procedure is first applied, for which details are given in Section 3.6. For a set \\(\\mathcal{G}\\) giving the numbers of mixture components to consider, and a list \\(\\mathcal{M}\\) of models with within-cluster covariance eigen-decomposition chosen from Table 3.1, the hierarchical partitions obtained for any number of components in the set \\(\\mathcal{G}\\) are used to start the EM algorithm. GMMs are then fit for any combination of elements \\(G \\in \\mathcal{G}\\) and \\(M \\in \\mathcal{M}\\). At the end of the procedure, the model with the best value of the model-selection criterion is returned (BIC is the default for model selection; see {#sec-modsel_cluster} for available options).\n\nExample 3.1   Clustering diabetes data\nThe diabetes dataset from Reaven and Miller (1979), available in the R package rrcov (Todorov 2022), contains measurements on 145 non-obese adult subjects. A description of the variables is given in the table below.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nrw\nRelative weight, expressed as the ratio of actual weight to expected weight, given the subject’s height.\n\n\nfpg\nFasting plasma glucose level.\n\n\nglucose\nArea under the plasma glucose curve.\n\n\ninsulin\nArea under the plasma insulin curve.\n\n\nsspg\nSteady state plasma glucose level, a measure of insulin resistance.\n\n\n\nThe glucose and insulin measurements were recorded for a three hour oral glucose tolerance test (OGTT). The subjects are clinically classified into three groups: \"normal\", \"overt\" diabetes (the most advanced stage), and \"chemical\" diabetes (a latent stage preceding overt diabetes).\n\ndata(\"diabetes\", package = \"rrcov\")\nX = diabetes[, 1:5]\nClass = diabetes$group\ntable(Class)\n## Class\n##   normal chemical    overt \n##       76       36       33\n\nThe data can be shown graphically as in Figure 3.2 with the following code:\n\nclp = clPairs(X, Class, lower.panel = NULL)\nclPairsLegend(0.1, 0.3, class = clp$class, col = clp$col, pch = clp$pch)\n\n\n\n\n\n\nFigure 3.2: Pairwise scatterplots for the diabetes data with points marked according to the true classification.\n\n\n\n\nThe clPairs()  function is an enhanced version of the base pairs() function, which allows data points to be represented by different colors and symbols. The function invisibly returns a list information useful for adding a legend via the clPairsLegend() function.\nThe following command performs a cluster analysis of the diabetes dataset for an unconstrained covariance model (VVV in the nomenclature of Table 2.1 and Table 3.1) with three mixture components:\n\nmod = Mclust(X, G = 3, modelNames = \"VVV\")\n\nNext, a summary of the modeling results is printed:\n\nsummary(mod)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVV (ellipsoidal, varying volume, shape, and orientation) model\n## with 3 components: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -2936.8 145 62 -6182.2 -6189.5\n## \n## Clustering table:\n##  1  2  3 \n## 78 40 27\n\nBy adding the optional argument parameters = TRUE, a more detailed summary can be obtained, including the estimated parameters:\n\nsummary(mod, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVV (ellipsoidal, varying volume, shape, and orientation) model\n## with 3 components: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -2936.8 145 62 -6182.2 -6189.5\n## \n## Clustering table:\n##  1  2  3 \n## 78 40 27 \n## \n## Mixing probabilities:\n##       1       2       3 \n## 0.53635 0.27605 0.18760 \n## \n## Means:\n##              [,1]     [,2]       [,3]\n## rw        0.93884   1.0478    0.98353\n## fpg      91.76513 102.9543  236.39448\n## glucose 357.21471 508.7962 1127.76651\n## insulin 166.14835 298.1248   78.38848\n## sspg    107.47991 228.7259  338.06062\n## \n## Variances:\n## [,,1]\n##                 rw      fpg   glucose      insulin      sspg\n## rw       0.0160109   0.3703    1.7519   -0.0072541    2.5199\n## fpg      0.3703007  63.8795  128.5895   29.8548772   58.8162\n## glucose  1.7518634 128.5895 2090.3853  269.2839704  408.9926\n## insulin -0.0072541  29.8549  269.2840 2656.1047748  856.5919\n## sspg     2.5198642  58.8162  408.9926  856.5919127 2131.0809\n## [,,2]\n##                rw        fpg    glucose    insulin       sspg\n## rw       0.011785   -0.25403    -2.4794     1.7088     2.2958\n## fpg     -0.254030  210.19848  1035.1415  -242.8201   -74.2975\n## glucose -2.479438 1035.14145  8151.8109 -1301.0449 -1021.8653\n## insulin  1.708775 -242.82013 -1301.0449 24803.2354  2202.7878\n## sspg     2.295790  -74.29753 -1021.8653  2202.7878  2453.5726\n## [,,3]\n##                 rw        fpg   glucose    insulin      sspg\n## rw        0.013712    -3.1314   -16.683     2.7459    3.2804\n## fpg      -3.131374  4910.7530 17669.044 -2208.7550 2630.6654\n## glucose -16.682509 17669.0438 71474.571 -9214.1986 9005.4355\n## insulin   2.745916 -2208.7550 -9214.199  2124.2757 -296.8184\n## sspg      3.280440  2630.6654  9005.435  -296.8184 6392.4694\n\nThe following table shows the relationship between the true classes and those defined by the fitted GMM:\n\ntable(Class, Cluster = mod$classification)\n##           Cluster\n## Class       1  2  3\n##   normal   70  6  0\n##   chemical  8 28  0\n##   overt     0  6 27\nadjustedRandIndex(Class, mod$classification)\n## [1] 0.65015\n\nRoughly, the first cluster appears to be associated with normal subjects, the second cluster with chemical diabetes, and the last cluster with subjects suffering from overt diabetes. Also shown is the adjusted Rand index  (ARI, Hubert and Arabie 1985), which can be used for evaluating a clustering solution. The ARI is a measure of agreement between two partitions, one that is estimated by a statistical procedure ignoring the labeling of the groups and the other that is the true classification. It has zero expected value in the case of a random partition, corresponding to the hypothesis of independent clusterings with fixed marginals, and is bounded above by 1, with higher values representing better partition accuracy. In this case, a better classification could be obtained by including the full range of available covariance models (the default) in the call to Mclust().\nThe plot()  method associated with Mclust() provides for a variety of displays. For instance, the following code produces a plot showing the clustering partition derived from the fitted GMM:\n\nplot(mod, what = \"classification\")\n\n\n\n\n\n\nFigure 3.3: Scatterplot matrix of variables in the diabetes data with points colored according to the mclust classification, and ellipses corresponding to projections of the mclust cluster covariances.\n\n\n\n\nThe resulting plot is shown in Figure 3.3, with points marked on the basis of the MAP classification (see Section 2.2.4), and ellipses showing the projection of the two-dimensional analog of the standard deviation for the cluster covariances of the estimated GMM. The ellipses can be dropped by specifying the optional argument addEllipses = FALSE, and filled ellipses can be obtained with the code:\n\nplot(mod, what = \"classification\", fillEllipses = TRUE)\n\nThe previous figures showed scatterplot matrices of all pairs of variables. A subset of pairs plots can be specified using the optional argument dimen. For instance, the following code selects the first two variables (see Figure 3.4):\n\nplot(mod, what = \"classification\", dimens = c(3, 4), fillEllipses = TRUE)\n\n\n\n\n\n\nFigure 3.4: Scatterplot of a pair of variables in the diabetes data with points marked according to the mclust classification, and filled ellipses corresponding to mclust cluster covariances.\n\n\n\n\nUncertainty associated with the MAP classification (see Section 2.2.4) can also be represented graphically as follows:\n\nplot(mod, dimens = c(3, 4), what = \"uncertainty\")\n\n\n\n\n\n\nFigure 3.5: Scatterplot of a pair of variables in the diabetes data with points marked according to the clustering, and point size reflecting the corresponding uncertainty of the MAP classification for the mclust model.\n\n\n\n\nThe resulting plot is shown in Figure 3.5, where the size of the data points reflects the classification uncertainty. Subsets of variables can also be specified for this type of graph.\nPlots can be selected interactively by leaving the what argument unspecified, in which case the plot() function will supply a menu of options. Chapter 6 provides a more detailed description of the graphical capabilities available in mclust for further fine-tuning.\n\n\nExample 3.2   Clustering thyroid disease data\nConsider the thyroid disease data (Coomans and Broeckaert 1986) that provides the results of five laboratory tests administered to a sample of 215 patients. The tests are used to predict whether a patient’s thyroid can be classified as euthyroidism (normal thyroid gland function), hypothyroidism (underactive thyroid not producing enough thyroid hormone) or hyperthyroidism (overactive thyroid producing and secreting excessive amounts of the free thyroid hormones T3 and/or thyroxine T4). Diagnosis of thyroid function was based on a complete medical record, including anamnesis, scan, and other measurements.\nThe data is one of several datasets included in the “Thyroid Disease Data Set” of the UCI Machine Learning Repository (Dua and Graff (2017) — see https://archive.ics.uci.edu/ml/datasets/thyroid+disease). The data used here, which corresponds to new-thyroid.data and new-thyroid.names in the data folder at the UCI site, is available as a dataset in mclust. A description of the variables is given in the table below, and a plot of the data is shown in Figure 3.6.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nDiagnosis\nDiagnosis of thyroid operation: Hypo, Normal, and Hyper.\n\n\nRT3U\nT3-resin uptake test (percentage).\n\n\nT4\nTotal serum thyroxin as measured by the isotopic displacement method.\n\n\nT3\nTotal serum triiodothyronine as measured by radioimmuno assay.\n\n\nTSH\nBasal thyroid-stimulating hormone (TSH) as measured by radioimmuno assay.\n\n\nDTSH\nMaximal absolute difference of TSH value after injection of 200 micro grams of thyrotropin-releasing hormone as compared to the basal value.\n\n\n\n\ndata(\"thyroid\", package = \"mclust\")\nX = data.matrix(thyroid[, 2:6])\nClass = thyroid$Diagnosis\n\nclp = clPairs(X, Class, lower.panel = NULL,\n              symbols = c(0, 1, 2), \n              colors = c(\"gray50\", \"black\", \"red3\")) \nclPairsLegend(0.1, 0.3, title = \"Thyroid diagnosis:\", class = clp$class, \n              col = clp$col, pch = clp$pch)\n\n\n\n\n\n\nFigure 3.6: Pairwise scatterplots showing the classification for the thyroid gland data.\n\n\n\n\nIn Example 3.1 we fitted a GMM by specifying both the numbers of mixture components to be considered, using the argument G, and the models for the component-covariance matrices to be used, using the argument modelNames. Users can provide multiple values for both arguments, a vector of integers for G, a vector of character strings for modelNames. By default, G = 1:9, so GMMs with up to nine components are fitted, and all of the available models shown in Table 3.1 are used for modelNames.\nClustering of the thyroid data is obtained using the following R command:\n\nmod = Mclust(X)\n\nSince we have provided only the data in the function call, selection of both the number of mixture components and the covariance parameterization is done automatically using the default model selection criterion, BIC. We can inspect the table of BIC values for all the \\(9 \\times 14 = 126\\) estimated models as follows:\n\nmod$BIC\n## Bayesian Information Criterion (BIC): \n##       EII     VII     EEI     VEI     EVI     VVI     EEE     VEE\n## 1 -7477.4 -7477.4 -6699.7 -6699.7 -6699.7 -6699.7 -6388.4 -6388.4\n## 2 -7265.4 -6780.9 -6399.7 -5475.4 -5446.6 -5276.3 -6231.3 -5429.6\n## 3 -6971.3 -6404.0 -6081.2 -5471.7 -5110.3 -4777.9 -6228.4 -5273.3\n## 4 -6726.5 -6263.1 -5972.8 -5281.3 -5146.0 -4796.8 -6260.7 -5381.4\n## 5 -6604.0 -6077.5 -5920.8 -5248.8 -4897.4 -4810.9 -6292.6 -5276.0\n## 6 -6574.1 -6002.9 -5893.5 -5230.1 -4945.0 -4854.6 -5946.0 -5291.6\n## 7 -6572.8 -5967.0 -6116.2 -5250.0 -4960.5 -4860.0 -5977.3 -5279.3\n## 8 -6500.2 -5947.6 -6065.9 -5253.0 -5003.5 -4881.4 -5924.7 -5327.2\n## 9 -6370.8 -5904.0 -5579.3 -5244.5 -5089.2 -4876.4 -5696.2 -5273.0\n##       EVE     VVE     EEV     VEV     EVV     VVV\n## 1 -6388.4 -6388.4 -6388.4 -6388.4 -6388.4 -6388.4\n## 2 -5286.3 -5163.6 -5354.9 -5166.4 -5241.5 -5150.8\n## 3 -5036.4 -5453.9 -5272.8 -5181.0 -5048.4 -4809.8\n## 4 -4974.8 -5478.8 -5129.3 -4902.6 -5061.9 -5175.3\n## 5 -5022.6 -5474.2 -5233.9 -4972.1      NA -4959.9\n## 6 -4932.2 -4878.3 -5290.6 -5005.4 -5141.0 -5012.5\n## 7 -5039.8 -4888.7 -5359.3 -5209.3 -5250.2 -5096.8\n## 8 -5086.9 -4909.7 -5376.2 -5070.9 -5347.1 -5129.7\n## 9      NA      NA -5287.7 -5091.2      NA      NA\n## \n## Top 3 models based on the BIC criterion: \n##   VVI,3   VVI,4   VVV,3 \n## -4777.9 -4796.8 -4809.8\n\nThe missing values (denoted by NA) that appear in the BIC table indicate that a particular model could not be estimated as initialized. This usually happens when a covariance matrix estimate becomes singular during EM. The Bayesian regularization method proposed by Fraley and Raftery (2007) and implemented in mclust as described in Section 7.2 addresses this problem.\nThe top three models are also listed at the bottom of the table. A summary of the top models according to the BIC criterion can also be obtained as\n\nsummary(mod$BIC, k = 5)\n## Best BIC values:\n##            VVI,3     VVI,4     VVV,3     VVI,5     VVI,6\n## BIC      -4777.9 -4796.773 -4809.761 -4810.928 -4854.644\n## BIC diff     0.0   -18.866   -31.854   -33.021   -76.737\n\nwhere we have specified that the the top five GMMs should be shown through the optional argument k = 5. This summary is useful because it provides the BIC differences with respect to the best model, namely, the one with the largest BIC. For the interpretation of BIC, see the discussion in Section 2.3.1.\nThe BIC values can be displayed graphically using\n\nplot(mod, what = \"BIC\", \n     legendArgs = list(\"bottomright\", ncol = 5))\n\n\n\n\n\n\nFigure 3.7: BIC traces for the GMMs estimated for the thyroid data.\n\n\n\n\nwhere we included the optional argument legendArgs to control the placement and format of the legend. The resulting plot of BIC traces is shown in Figure 3.7.\nThe model selected is (VVI,3), a three-component Gaussian mixture with diagonal covariance structure, in which both the volume and shape vary between clusters. We can obtain a full summary of the model estimates as follows:\n\nsummary(mod, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVI (diagonal, varying volume and shape) model with 3\n## components: \n## \n##  log-likelihood   n df     BIC     ICL\n##           -2303 215 32 -4777.9 -4784.5\n## \n## Clustering table:\n##   1   2   3 \n##  35 152  28 \n## \n## Mixing probabilities:\n##       1       2       3 \n## 0.16310 0.70747 0.12943 \n## \n## Means:\n##           [,1]     [,2]     [,3]\n## RT3U 95.536865 110.3462 123.2078\n## T4   17.678383   9.0880   3.7994\n## T3    4.262681   1.7218   1.0576\n## TSH   0.970155   1.3051  13.8951\n## DTSH -0.017415   2.4960  18.8221\n## \n## Variances:\n## [,,1]\n##        RT3U     T4     T3     TSH     DTSH\n## RT3U 343.92  0.000 0.0000 0.00000 0.000000\n## T4     0.00 17.497 0.0000 0.00000 0.000000\n## T3     0.00  0.000 4.9219 0.00000 0.000000\n## TSH    0.00  0.000 0.0000 0.15325 0.000000\n## DTSH   0.00  0.000 0.0000 0.00000 0.071044\n## [,,2]\n##        RT3U     T4      T3     TSH   DTSH\n## RT3U 66.372 0.0000 0.00000 0.00000 0.0000\n## T4    0.000 4.8234 0.00000 0.00000 0.0000\n## T3    0.000 0.0000 0.23278 0.00000 0.0000\n## TSH   0.000 0.0000 0.00000 0.21394 0.0000\n## DTSH  0.000 0.0000 0.00000 0.00000 3.1892\n## [,,3]\n##       RT3U    T4      T3 TSH   DTSH\n## RT3U 95.27 0.000 0.00000   0   0.00\n## T4    0.00 4.278 0.00000   0   0.00\n## T3    0.00 0.000 0.27698   0   0.00\n## TSH   0.00 0.000 0.00000 147   0.00\n## DTSH  0.00 0.000 0.00000   0 231.29\n\nThe clustering implied by the selected model can then be shown graphically as in Figure 3.8 with the code:\n\nplot(mod, what = \"classification\")\n\n\n\n\n\n\nFigure 3.8: Scatterplot matrix for the thyroid data with points marked according to the GMM (VVI,3) clustering, and ellipses corresponding to projections of the estimated cluster covariances.\n\n\n\n\nThe confusion matrix for the clustering and the corresponding adjusted Rand index are obtained as:\n\ntable(Class, Cluster = mod$classification)\n##         Cluster\n## Class      1   2   3\n##   Hypo     0   4  26\n##   Normal   1 147   2\n##   Hyper   34   1   0\nadjustedRandIndex(Class, mod$classification)\n## [1] 0.87715\n\nThe estimated clustering model turns out to be quite accurate in recovering the true thyroid diagnosis.\nA plot of the posterior conditional probabilities ordered by cluster can be obtained using the following code:\n\nz  = mod$z               # posterior conditional probabilities\ncl = mod$classification  # MAP clustering\nG  = mod$G               # number of clusters\nsclass = 10 # class separation\nsedge = 3   # edge spacing\nL = nrow(z) + G*(sclass+2*sedge)\nplot(1:L, runif(L), ylim = c(0, 1), type = \"n\", axes = FALSE, \n     ylab = \"Posterior conditional probabilities\", xlab = \"\")\naxis(2)\ncol = mclust.options(\"classPlotColors\")\nl = sclass\nfor (k in 1:G)\n{\n i = which(cl == k)\n ord = i[order(z[i, k], decreasing = TRUE)]\n for (j in 1:G)\n    points((l+sedge)+1:length(i), z[ord, j], \n           pch = as.character(j), col = col[j])\n rect(l, 0, l+2*sedge+length(i), 1, \n      border = col[k], col = col[k], lwd = 2, density = 0)\n l = l + 2*sedge + length(i) + sclass\n}\n\n\n\n\n\n\nFigure 3.9: Estimated posterior conditional probabilities of class membership for each of the three clusters determined by the MAP classification of observations in the thyroid data. The panels correspond to the different clusters.\n\n\n\n\nThis yields Figure 3.9, in which a panel of posterior conditional probabilities of class membership is shown for each of the three clusters determined by the MAP classification. The plot shows that observations in the third cluster have the largest probabilities of membership, while there is some uncertainty in membership among the first two clusters.\n\n\nExample 3.3   Clustering Italian wines data\nForina et al. (1986) reported data on 178 wines grown in the same region in Italy but derived from three different cultivars (Barbera, Barolo, Grignolino). For each wine, 13 measurements of chemical and physical properties were made as reported in the table below. This data set is from the UCI Machine Learning Data Repository (Dua and Graff (2017) — see http://archive.ics.uci.edu/ml/datasets/Wine) and is available in the gclus package (Hurley 2019).\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nClass\nWine classes: 1) Barolo; 2) Grignolino; 3) Barbera\n\n\nAlcohol\nAlcohol\n\n\nMalic\nMalic acid\n\n\nAsh\nAsh\n\n\nAlcalinity\nAlcalinity of ash\n\n\nMagnesium\nMagnesium\n\n\nPhenols\nTotal phenols\n\n\nFlavanoids\nFlavanoids\n\n\nNonflavanoid\nNonflavanoid phenols\n\n\nProanthocyanins\nProanthocyanins\n\n\nIntensity\nColor intensity\n\n\nHue\nHue\n\n\nOD280\nOD280/OD315 of diluted wines\n\n\nProline\nProline\n\n\n\n\ndata(\"wine\", package = \"gclus\")\nClass = factor(wine$Class, levels = 1:3,\n               labels = c(\"Barolo\", \"Grignolino\", \"Barbera\"))\nX = data.matrix(wine[, -1])\n\nFor a description of the data, see help(\"wine\", package = \"gclus\").\nWe fit the data with Mclust() for all 14 models, and then summarize the BIC for the top 3 models:\n\nmod = Mclust(X)\nsummary(mod$BIC, k = 3)\n## Best BIC values:\n##            VVE,3     EVE,4     VVE,4\n## BIC      -6849.4 -6873.616 -6885.472\n## BIC diff     0.0   -24.225   -36.081\n\nThe BIC traces are then plotted (see Figure 3.10):\n\nplot(mod, what = \"BIC\", \n     ylim = range(mod$BIC[, -(1:2)], na.rm = TRUE),\n     legendArgs = list(x = \"bottomleft\"))\n\n\n\n\n\n\nFigure 3.10: BIC plot for models fitted to the wine data.\n\n\n\n\nNote that, in this last plot, we adjusted the range of the vertical axis so as to remove models with lower BIC values. The model selected by BIC is a three-component Gaussian mixture with covariances having different volumes and shapes, but the same orientation (VVE). This is a flexible but relatively parsimonious model. A summary of the selected model is then obtained:\n\nsummary(mod)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVE (ellipsoidal, equal orientation) model with 3 components: \n## \n##  log-likelihood   n  df     BIC     ICL\n##         -3015.3 178 158 -6849.4 -6850.7\n## \n## Clustering table:\n##  1  2  3 \n## 59 69 50\ntable(Class, mod$classification)\n##             \n## Class         1  2  3\n##   Barolo     59  0  0\n##   Grignolino  0 69  2\n##   Barbera     0  0 48\nadjustedRandIndex(Class, mod$classification)\n## [1] 0.96674\n\nThe fitted model provides an accurate recovery of the true classes with a high ARI.\nDisplaying the results is difficult because of the high dimensionality of the data. Below we give a simple method for choosing a subset of variables for a matrix of scatterplots. More sophisticated approaches to choosing projections that reveal clustering are discussed in Section 6.4. First we plot the estimated cluster means, normalized to the \\([0,1]\\) range, using the heatmap function  available in base R:\n\nnorm01 = function(x) (x - min(x))/(max(x) - min(x))\nM = apply(t(mod$parameters$mean), 2, norm01)\nheatmap(M, Rowv = NA, scale = \"none\", margins = c(8, 2),\n        labRow = paste(\"Cluster\", 1:mod$G), cexRow = 1.2)\n\n\n\n\n\n\nFigure 3.11: Heatmap of normalized cluster means for the clustering model fitted to the wine data.\n\n\n\n\nThe resulting heatmap (see Figure 3.11) shows which features have the most different cluster means. Based on this, we select a subset of three variables (Alcohol, Malic, and Flavanoids) for the dimens argument specifying coordinate projections for multivariate data in the plot() function call:\n\nplot(mod, what = \"classification\", dimens = c(1, 2, 7))\n\n\n\n\n\n\nFigure 3.12: Coordinate projection plot of selected features showing the clusters for the wine data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#sec-modsel_cluster",
    "href": "chapters/03_cluster.html#sec-modsel_cluster",
    "title": "3  Model-Based Clustering",
    "section": "\n3.3 Model Selection",
    "text": "3.3 Model Selection\n\n3.3.1 BIC\nIn the mclust package, BIC is used by default for model selection. The function mclustBIC(),  and indirectly Mclust(), computes a matrix of BIC values for models chosen from Table 3.1, and numbers of mixture components as specified.\n\nExample 3.4   BIC model selection for the Old Faithful data\nAs an illustration, consider the bivariate faithful dataset (Härdle 1991), included in the base R package datasets. “Old Faithful” is a geyser located in Yellowstone National Park, Wyoming, USA. The faithful dataset provides the duration of Old Faithful’s eruptions (eruptions, time in minutes) and the waiting times between successive eruptions (waiting, time in minutes). A scatterplot of the data is shown in panel Figure 3.13, obtained using the code:\n\ndata(\"faithful\", package = \"datasets\")\nplot(faithful)\n\n\n\n\n\n\nFigure 3.13: Scatterplot of the Old Faithful data available in the faithful dataset.\n\n\n\n\nThe following code can be used to compute the BIC values and plot the corresponding BIC traces:\n\nBIC = mclustBIC(faithful)\nBIC\n## Bayesian Information Criterion (BIC): \n##       EII     VII     EEI     VEI     EVI     VVI     EEE     VEE\n## 1 -4024.7 -4024.7 -3055.8 -3055.8 -3055.8 -3055.8 -2607.6 -2607.6\n## 2 -3453.0 -3458.3 -2354.6 -2350.6 -2352.6 -2346.1 -2325.2 -2323.0\n## 3 -3377.7 -3336.6 -2323.0 -2332.7 -2332.2 -2342.4 -2314.3 -2322.1\n## 4 -3230.3 -3242.8 -2323.7 -2331.3 -2334.7 -2343.5 -2331.2 -2340.2\n## 5 -3149.4 -3129.1 -2327.1 -2350.2 -2347.6 -2351.0 -2360.7 -2347.3\n## 6 -3081.4 -3038.2 -2338.2 -2360.6 -2357.7 -2373.5 -2347.4 -2372.3\n## 7 -2990.4 -2973.4 -2356.5 -2368.5 -2372.9 -2394.7 -2369.3 -2371.2\n## 8 -2978.1 -2935.1 -2364.1 -2384.7 -2389.1 -2413.7 -2376.1 -2390.4\n## 9 -2953.4 -2919.4 -2372.8 -2398.2 -2407.2 -2432.7 -2389.6 -2406.7\n##       EVE     VVE     EEV     VEV     EVV     VVV\n## 1 -2607.6 -2607.6 -2607.6 -2607.6 -2607.6 -2607.6\n## 2 -2324.3 -2320.4 -2329.1 -2325.4 -2327.6 -2322.2\n## 3 -2342.3 -2336.3 -2325.3 -2329.6 -2340.0 -2349.7\n## 4 -2361.8 -2362.5 -2351.5 -2361.1 -2344.7 -2351.5\n## 5 -2351.8 -2368.9 -2356.9 -2368.1 -2364.9 -2379.4\n## 6 -2366.5 -2386.5 -2366.1 -2386.3 -2384.1 -2387.0\n## 7 -2379.8 -2402.2 -2379.1 -2401.3 -2398.7 -2412.4\n## 8 -2403.9 -2426.0 -2393.0 -2425.4 -2415.0 -2442.0\n## 9 -2414.1 -2448.2 -2407.5 -2446.7 -2438.9 -2460.4\n## \n## Top 3 models based on the BIC criterion: \n##   EEE,3   VVE,2   VEE,3 \n## -2314.3 -2320.4 -2322.1\nplot(BIC)\n\n\n\n\n\n\nFigure 3.14: BIC traces for the GMMs estimated for the faithful dataset.\n\n\n\n\nIn this case the BIC supports a three-component GMM with common full covariance matrix (EEE model). Note that, as for the thyroid disease data example, there are missing BIC values (denoted by NA) corresponding to models that could not be estimated as initialized.\nBy default, mclustBIC computes results for up to 9 components and all available models in Table 3.1. Optional arguments can be provided to mclustBIC() allowing fine tuning, such as G for the number of components, and modelNames for specifying the model covariance parameterizations (see Table 3.1 and help(\"mclustModelNames\") for a description of available model names).\n\nAnother optional argument, called x, can be used to provide the output from a previous call to mclustBIC(). This is useful if the model space needs to be enlarged by fitting more models (by increasing the number of mixture components and/or specifying additional covariance parameterizations) without the need to recompute the BIC values for those models already fitted. BIC values already available can be provided analogously to Mclust as follows:\n\nmod1 = Mclust(faithful, x = BIC)\n\n\n\n3.3.2 ICL\nBesides the BIC criterion for model selection, the integrated complete-data likelihood (ICL) criterion, described in Section 2.3.1, is available in mclust through the function mclustICL().  It is invoked similarly to mclustBIC(), and it allows us to select both the model (among those in Table 3.1) and the number of mixture components that maximize the ICL criterion.\n\nExample 3.5   ICL model selection for the Old Faithful data\nConsidering again the data in Example 3.4, ICL values are computed as follows:\n\nICL = mclustICL(faithful)\nICL\n## Integrated Complete-data Likelihood (ICL) criterion: \n##       EII     VII     EEI     VEI     EVI     VVI     EEE     VEE\n## 1 -4024.7 -4024.7 -3055.8 -3055.8 -3055.8 -3055.8 -2607.6 -2607.6\n## 2 -3455.8 -3460.9 -2356.3 -2350.7 -2353.3 -2346.2 -2326.7 -2323.4\n## 3 -3422.8 -3360.3 -2359.5 -2377.3 -2367.5 -2387.7 -2357.8 -2376.5\n## 4 -3265.8 -3272.5 -2372.0 -2413.4 -2402.2 -2436.3 -2468.3 -2452.7\n## 5 -3190.7 -3151.9 -2394.0 -2486.7 -2412.4 -2445.8 -2478.2 -2472.0\n## 6 -3117.4 -3061.3 -2423.0 -2486.8 -2446.9 -2472.6 -2456.2 -2503.9\n## 7 -3022.3 -2995.8 -2476.2 -2519.8 -2446.7 -2496.7 -2464.3 -2466.8\n## 8 -3007.4 -2953.7 -2488.5 -2513.5 -2492.3 -2509.7 -2502.2 -2479.8\n## 9 -2989.1 -2933.1 -2499.9 -2540.4 -2515.0 -2528.6 -2547.1 -2499.9\n##       EVE     VVE     EEV     VEV     EVV     VVV\n## 1 -2607.6 -2607.6 -2607.6 -2607.6 -2607.6 -2607.6\n## 2 -2325.8 -2320.8 -2330.0 -2325.7 -2328.2 -2322.7\n## 3 -2412.0 -2427.0 -2372.4 -2405.3 -2380.3 -2385.2\n## 4 -2459.4 -2440.3 -2414.2 -2419.9 -2385.8 -2407.6\n## 5 -2444.3 -2478.6 -2431.1 -2490.2 -2423.2 -2474.5\n## 6 -2504.8 -2489.1 -2449.6 -2481.4 -2483.8 -2491.6\n## 7 -2499.3 -2496.3 -2465.7 -2506.8 -2490.1 -2519.5\n## 8 -2526.0 -2516.6 -2489.4 -2539.8 -2497.8 -2556.1\n## 9 -2545.7 -2541.7 -2542.9 -2566.7 -2528.6 -2587.2\n## \n## Top 3 models based on the ICL criterion: \n##   VVE,2   VVV,2   VEE,2 \n## -2320.8 -2322.7 -2323.4\n\nA trace of ICL values is plotted and shown in Figure 3.15:\n\nplot(ICL)\n\n\n\n\n\n\nFigure 3.15: ICL traces for the GMMs estimated for the faithful dataset.\n\n\n\n\nNote that, in this case, ICL selects two components, one less than the number selected by BIC. This is reasonable, since ICL penalizes overlapping components more heavily.\nThe model selected by ICL can be fitted using\n\nmod2 = Mclust(faithful, G = 2, modelNames = \"VVE\")\n\nOptional arguments can also be provided in the Mclust() function call, analogous to those described above for mclustBIC().\nThe following code can be used to draw scatterplots of the data with points marked according to the classification obtained by the ``best’’ estimated model selected, respectively, by the BIC and ICL model selection criteria (see Figure 3.16):\nplot(mod1, what = \"classification\", fillEllipses = TRUE)\nplot(mod2, what = \"classification\", fillEllipses = TRUE)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.16: Scatterplots for the faithful dataset with points and ellipses corresponding to the classification from the best estimated GMMs selected by BIC (a) and ICL (b).\n\n\n\n\n3.3.3 Bootstrap Likelihood Ratio Testing\nThe bootstrap procedure discussed in Section 2.3.2 for selecting the number of mixture components is implemented in the mclustBootstrapLRT() function. \n\nExample 3.6   Bootstrap LRT for the Old Faithful data\nUsing the data of {Example 3.4}, a bootstrap LR test is obtained by specifying the input data and the name of the model to test:\n\nLRT = mclustBootstrapLRT(faithful, modelName = \"VVV\")\nLRT\n## ------------------------------------------------------------- \n## Bootstrap sequential LRT for the number of mixture components \n## ------------------------------------------------------------- \n## Model        = VVV \n## Replications = 999 \n##              LRTS bootstrap p-value\n## 1 vs 2   319.0654             0.001\n## 2 vs 3     6.1305             0.572\n\nThe sequential bootstrap procedure terminates when a test is not significant as specified by the argument level (which is set to 0.05 by default). There is also the option to specify the maximum number of mixture components to test via the argument maxG. The number of bootstrap resamples can be set with the optional argument nboot (nboot = 999 is the default).\nIn the example above, the bootstrap \\(p\\)-values clearly indicate the presence of two clusters. Note that models fitted to the original data are estimated via the EM algorithm initialized with unconstrained model-based agglomerative hierarchical clustering (the default). Then, during the bootstrap procedure, models under the null and the alternative hypotheses are fitted to bootstrap samples using again the EM algorithm. However, in this case the algorithm starts with the E-step initialized with the estimated parameters obtained for the original data.\nThe bootstrap distributions of the LRTS can be shown graphically (see Figure 3.17) using the associated plot method:\nplot(LRT, G = 1)\nplot(LRT, G = 2)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.17: Histograms of LRTS bootstrap distributions for testing the number of mixture components in the faithful data assuming the VVV model. The dotted vertical lines refer to the sample values of LRTS.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#sec-resamp",
    "href": "chapters/03_cluster.html#sec-resamp",
    "title": "3  Model-Based Clustering",
    "section": "\n3.4 Resampling-Based Inference in mclust",
    "text": "3.4 Resampling-Based Inference in mclust\nSection 2.4 describes some resampling-based approaches to inference in GMMs, namely the nonparametric bootstrap, the parametric bootstrap, and the weighted likelihood bootstrap. These, together with the jackknife (Efron 1979, Efron:1982), are implemented in the MclustBootstrap() function available in mclust.\n\nExample 3.7   Resampling-based inference for Gaussian mixtures on the hemophilia data\nConsider the hemophilia dataset (Habbema, Hermans, and Broek 1974) available in the package rrcov (Todorov 2022), which contains two measured variables on 75 women belonging to two groups: 30 of them are non-carriers (normal group) and 45 are known hemophilia A carriers (obligatory carriers).\n\ndata(\"hemophilia\", package = \"rrcov\")\nX = hemophilia[, 1:2]\nClass = as.factor(hemophilia$gr) \nclp = clPairs(X, Class, symbols = c(16, 0), colors = \"black\")\nclPairsLegend(0.8, 0.2, class = clp$class, col = clp$col, pch = clp$pch)\n\nThe last command plots the known classification of the observed data (see Figure 3.18 (a)).\nBy analogy with the analysis of Basford et al. (1997, Example II, Section 5), we fitted a two-component GMM with unconstrained covariance matrices:\n\nmod = Mclust(X, G = 2, modelName = \"VVV\")\nsummary(mod, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVV (ellipsoidal, varying volume, shape, and orientation) model\n## with 2 components: \n## \n##  log-likelihood  n df    BIC    ICL\n##          77.029 75 11 106.56 92.855\n## \n## Clustering table:\n##  1  2 \n## 39 36 \n## \n## Mixing probabilities:\n##       1       2 \n## 0.51015 0.48985 \n## \n## Means:\n##                  [,1]      [,2]\n## AHFactivity -0.116124 -0.366387\n## AHFantigen  -0.024573 -0.045323\n## \n## Variances:\n## [,,1]\n##             AHFactivity AHFantigen\n## AHFactivity   0.0113599  0.0065959\n## AHFantigen    0.0065959  0.0123872\n## [,,2]\n##             AHFactivity AHFantigen\n## AHFactivity    0.015874   0.015050\n## AHFantigen     0.015050   0.032341\n\nNote that, in the summary()  function call, we set parameters = TRUE to retrieve the estimated parameters. The clustering structure identified is shown in Figure 3.18 (b) and can be obtained as follows:\n\nplot(mod, what = \"classification\", fillEllipses = TRUE)\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.18: Scatterplots for the hemophilia data displaying the true class membership (a) and the classification obtained from the fit of a GMM (b).\n\n\nA function, MclustBootstrap(),  is available for bootstrap inference for GMMs. This function requires the user to input an object as returned by a call to Mclust(). Optionally, the number of bootstrap resamples (nboot) and the type of bootstrap resampling to use (type), can also be specified. By default, nboot = 999 and type = \"bs\", with the latter specifying the nonparametric bootstrap. Thus, a simple call for computing the bootstrap distribution of the GMM parameters is the following:\n\nboot = MclustBootstrap(mod, nboot = 999, type = \"bs\")\n\nNote that, although we have included the arguments nboot and type, they could have been omitted in this instance since they are both set at their defaults.\nResults of the function MclustBootstrap() are available through the summary() method, which by default returns the standard errors of the GMM parameters:\n\nsummary(boot, what = \"se\")\n## ---------------------------------------------------------- \n## Resampling standard errors \n## ---------------------------------------------------------- \n## Model                      = VVV \n## Num. of mixture components = 2 \n## Replications               = 999 \n## Type                       = nonparametric bootstrap \n## \n## Mixing probabilities:\n##       1       2 \n## 0.12032 0.12032 \n## \n## Means:\n##                    1        2\n## AHFactivity 0.037581 0.041488\n## AHFantigen  0.030828 0.062937\n## \n## Variances:\n## [,,1]\n##             AHFactivity AHFantigen\n## AHFactivity   0.0065336  0.0044027\n## AHFantigen    0.0044027  0.0030834\n## [,,2]\n##             AHFactivity AHFantigen\n## AHFactivity   0.0062451  0.0056968\n## AHFantigen    0.0056968  0.0093190\n\nBootstrap percentile confidence intervals, described in Section 2.4, can also be obtained by specifying what = \"ci\" in the summary() call. The confidence level of the intervals can also be specified (by default, conf.level = 0.95). For instance:\n\nsummary(boot, what = \"ci\", conf.level = 0.9)\n## ---------------------------------------------------------- \n## Resampling confidence intervals \n## ---------------------------------------------------------- \n## Model                      = VVV \n## Num. of mixture components = 2 \n## Replications               = 999 \n## Type                       = nonparametric bootstrap \n## Confidence level           = 0.9 \n## \n## Mixing probabilities:\n##           1       2\n## 5%  0.37514 0.22068\n## 95% 0.77932 0.62486\n## \n## Means:\n## [,,1]\n##     AHFactivity AHFantigen\n## 5%     -0.20610  -0.079998\n## 95%    -0.08424   0.020474\n## [,,2]\n##     AHFactivity AHFantigen\n## 5%     -0.43889  -0.132681\n## 95%    -0.30504   0.097313\n## \n## Variances:\n## [,,1]\n##     AHFactivity AHFantigen\n## 5%    0.0056903  0.0081596\n## 95%   0.0265706  0.0181777\n## [,,2]\n##     AHFactivity AHFantigen\n## 5%    0.0050765  0.0096963\n## 95%   0.0247648  0.0423184\n\nThe bootstrap distribution of the parameters can also be represented graphically. For instance, the following code produces a plot of the bootstrap distribution for the mixing proportions:\nplot(boot, what = \"pro\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.19: Bootstrap distribution for the mixture proportions of the GMM fitted to the hemophilia data. The vertical dotted lines indicate the MLEs, the bottom line indicates the percentile confidence intervals, and the square shows the center of the bootstrap distribution.\n\n\nand for the component means:\nplot(boot, what = \"mean\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 3.20: Bootstrap distribution for the mixture component means of the GMM fitted to the hemophilia data. The vertical dotted lines indicate the MLEs, the bottom line indicates the percentile confidence intervals, and the square shows the center of the bootstrap distribution.\n\n\nThe resulting plots are shown in Figure 3.19 and Figure 3.20.\nAs mentioned, with the function MclustBootstrap(), it is also possible to choose the parametric bootstrap (type = \"pb\"), the weighted likelihood bootstrap (type = \"wlbs\"), or the jackknife (type = \"jk\"). For instance, in our data example the weighted likelihood bootstrap can be obtained as follows:\n\nwlboot = MclustBootstrap(mod, nboot = 999, type = \"wlbs\")\nsummary(wlboot, what = \"se\")\n## ---------------------------------------------------------- \n## Resampling standard errors \n## ---------------------------------------------------------- \n## Model                      = VVV \n## Num. of mixture components = 2 \n## Replications               = 999 \n## Type                       = weighted likelihood bootstrap \n## \n## Mixing probabilities:\n##       1       2 \n## 0.12062 0.12062 \n## \n## Means:\n##                    1        2\n## AHFactivity 0.037307 0.040271\n## AHFantigen  0.029360 0.064102\n## \n## Variances:\n## [,,1]\n##             AHFactivity AHFantigen\n## AHFactivity   0.0063976  0.0042667\n## AHFantigen    0.0042667  0.0029802\n## [,,2]\n##             AHFactivity AHFantigen\n## AHFactivity   0.0056345  0.0057299\n## AHFantigen    0.0057299  0.0093274\n\nIn this case the differences between the nonparametric and the weighted likelihood bootstrap are negligible. We can summarize the inference for the GMM component means obtained under the two approaches graphically, showing the bootstrap percentile confidence intervals for each variable and component (see Figure 3.21):\nboot.ci = summary(boot, what = \"ci\")\nwlboot.ci = summary(wlboot, what = \"ci\")\nfor (j in 1:mod$d)\n{ \n  plot(1:mod$G, mod$parameters$mean[j, ], col = 1:mod$G, pch = 15,\n       ylab = colnames(X)[j], xlab = \"Mixture component\",\n       ylim = range(boot.ci$mean, wlboot.ci$mean), \n       xlim = c(.5, mod$G+.5), xaxt = \"n\")\n  points(1:mod$G+0.2, mod$parameters$mean[j, ], col = 1:mod$G, pch = 15)\n  axis(side = 1, at = 1:mod$G)\n  with(boot.ci, \n       errorBars(1:G, mean[1, j, ], mean[2, j, ], col = 1:G))\n  with(wlboot.ci, \n       errorBars(1:G+0.2, mean[1, j, ], mean[2, j, ], col = 1:G, lty = 2))\n}\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.21: Bootstrap percentile intervals for the means of the GMM fitted to the hemophilia data. Solid lines refer to the nonparametric bootstrap, dashed lines to the weighted likelihood bootstrap.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#sec-clustone",
    "href": "chapters/03_cluster.html#sec-clustone",
    "title": "3  Model-Based Clustering",
    "section": "\n3.5 Clustering Univariate Data",
    "text": "3.5 Clustering Univariate Data\nFor clustering univariate data, the quantiles of the empirical distribution are used to initialize the EM algorithm by default, rather than hierarchical clustering. There are only two possible models, E for equal variance across components and V allowing the variance to vary across the components.\n\nExample 3.8   Univariate clustering of annual precipitation in US cities\nAs an example of univariate clustering, consider the precip dataset (McNeil 1977) included in the package datasets available in the base R distribution:\n\ndata(\"precip\", package = \"datasets\")\ndotchart(sort(precip), cex = 0.6, pch = 19,\n         xlab = \"Average annual rainfall (in inches)\")\n\n\n\n\n\n\nFigure 3.22: Dot chart of average annual rainfall (in inches) for 70 US cities.\n\n\n\n\nThe average amount of rainfall (in inches) for each of 70 US cities is shown as a dot chart in Figure 3.22. A clustering model is then obtained as follows:\n\nmod = Mclust(precip)\nsummary(mod, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust V (univariate, unequal variance) model with 2 components: \n## \n##  log-likelihood  n df     BIC     ICL\n##         -275.47 70  5 -572.19 -575.21\n## \n## Clustering table:\n##  1  2 \n## 13 57 \n## \n## Mixing probabilities:\n##       1       2 \n## 0.18137 0.81863 \n## \n## Means:\n##      1      2 \n## 12.793 39.780 \n## \n## Variances:\n##      1      2 \n## 16.813 90.394\nplot(mod, what = \"BIC\", legendArgs = list(x = \"bottomleft\"))\n\n\n\n\n\n\nFigure 3.23: BIC traces for GMMs fitted to the precip data.\n\n\n\n\nThe selected model is a two-component Gaussian mixture with different variances. The first cluster, which includes 13 cities, is characterized by both smaller means and variances compared to the second cluster. The clustering partition and the corresponding classification uncertainties can be displayed using the following plot commands (see Figure 3.24):\nplot(mod, what = \"classification\")\nplot(mod, what = \"uncertainty\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.24: Classification (a) and uncertainty (b) plots for the precip data.\n\n\nThe final clustering is shown in a dot chart with observations grouped by clusters (see Figure 3.25):\n\nx = data.frame(precip, clusters = mod$classification)\nrownames(x) = make.unique(names(precip)) # correct duplicated names \nx = x[order(x$precip), ]\ndotchart(x$precip, labels = rownames(x),\n         groups = factor(x$clusters, levels = 2:1, \n                         labels = c(\"Cluster 2\", \"Cluster 1\")),\n         cex = 0.6, pch = 19, \n         color = mclust.options(\"classPlotColors\")[x$clusters],\n         xlab = \"Average annual rainfall (in inches)\")\n\n\n\n\n\n\nFigure 3.25: Dot chart of average annual rainfall (in inches) for 70 US cities grouped by the estimated clustering partitions.\n\n\n\n\nEach of the graphs shown indicates that the two groups of cities are separated by about 20 inches of annual rainfall.\n\nFurther graphical capabilities for univariate data are discussed in Section 6.1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#sec-mbahc",
    "href": "chapters/03_cluster.html#sec-mbahc",
    "title": "3  Model-Based Clustering",
    "section": "\n3.6 Model-Based Agglomerative Hierarchical Clustering",
    "text": "3.6 Model-Based Agglomerative Hierarchical Clustering\nModel-based agglomerative hierarchical clustering (MBAHC, Banfield and Raftery 1993) adopts the general ideas of classical agglomerative hierarchical clustering, with the aim of obtaining a hierarchy of clusters of \\(n\\) objects. Given no preliminary information with regard to grouping, agglomerative hierarchical clustering proceeds from \\(n\\) clusters each containing a single observation, to one cluster containing all \\(n\\) observations, by successively merging observations and clusters. In the traditional approach, two groups are merged if they are closest according to a particular distance metric and type of linkage, while in MBAHC, the two groups are merged that optimize a regularized criterion (Fraley 1998) derived from the classification likelihood over all possible merge pairs.\nGiven \\(n\\) observations \\((\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n)\\), let \\((\\ell_1, \\dots, \\ell_n){}^{\\!\\top}\\) denote the classification labels, so that \\(\\ell_i=k\\) if \\(\\boldsymbol{x}_i\\) is assigned to cluster \\(k\\). The unknown parameters are obtained by maximizing the classification likelihood:  \\[\nL_{CL}(\\boldsymbol{\\theta}_1, \\dots, \\boldsymbol{\\theta}_G, \\ell_1, \\dots, \\ell_n ; \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n) =\n\\prod_{i=1}^n f_{\\ell_i}(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_{\\ell_i}).\n\\] Assuming a multivariate Gaussian distribution for the data, so that \\(f_{\\ell_i}(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_{\\ell_i}) = \\phi(\\boldsymbol{x}_i ; \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)\\) if \\(\\ell_i=k\\), the classification likelihood is \\[\nL_{CL}(\\boldsymbol{\\mu}_1, \\dots, \\boldsymbol{\\mu}_G, \\boldsymbol{\\Sigma}_1, \\dots, \\boldsymbol{\\Sigma}_G, \\ell_1, \\dots, \\ell_n ; \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n) = \\\\\n\\prod_{k=1}^G \\prod_{i \\in \\mathcal{I}_k} \\phi(\\boldsymbol{x}_i ; \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k),\n\\] where \\(\\mathcal{I}_k = \\{ i : \\ell_i = k \\}\\) is the set of indices corresponding to the observations that belong to cluster \\(k\\). After replacing the unknown mean vectors \\(\\boldsymbol{\\mu}_k\\) by their MLEs, \\(\\widehat{\\boldsymbol{\\mu}}_k = \\bar{\\boldsymbol{x}}_k\\), the profile (or concentrated) log-likelihood is given by \\[\n\\begin{multline}\n\\log L_{CL}(\\boldsymbol{\\Sigma}_1, \\dots, \\boldsymbol{\\Sigma}_G, \\ell_1, \\dots, \\ell_n ; \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_n) = \\\\\n-\\frac{1}{2} nd \\log(2\\pi) -\\frac{1}{2} \\sum_{k=1}^G \\{ \\tr(\\boldsymbol{W}_k \\boldsymbol{\\Sigma}_k^{-1}) + n_k \\log|\\boldsymbol{\\Sigma}_k| \\},\n\\end{multline}\n\\tag{3.1}\\] where \\(\\boldsymbol{W}_k = \\sum_{i \\in \\mathcal{I}_k} (\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}}_k)(\\boldsymbol{x}_i - \\bar{\\boldsymbol{x}}_k){}^{\\!\\top}\\) is the sample cross-product matrix for the \\(k\\)th cluster, and \\(n_k\\) is the number of observations in cluster \\(k\\).\nBy imposing different covariance structures on \\(\\boldsymbol{\\Sigma}_k\\), different merging criteria can be derived (see Fraley 1998, Table 1). For instance, assuming a common isotropic cluster-covariance, \\(\\boldsymbol{\\Sigma}_k = \\sigma^2\\boldsymbol{I}\\), the log-likelihood in Equation 3.1 is maximized by minimizing the criterion \\[\n\\tr\\left( \\sum_{k=1}^G \\boldsymbol{W}_k \\right),\n\\] the well-known sum of squares criterion of Ward (1963). If the covariances are allowed to be different across clusters, then the criterion to be minimized at each stage of the hierarchical algorithm is \\[\n\\sum_{k=1}^G n_k \\log \\left| \\frac{\\boldsymbol{W}_k}{n_k} \\right|.\n\\] Methods for regularizing these criteria and efficient numerical algorithms for their minimization have been discussed by Fraley (1998). Table 3.1 lists the models for hierarchical clustering available in mclust.\nAt each stage of MBAHC, a pair of groups is merged that minimizes a regularized merge criterion over all possible merge pairs (Fraley 1998). Regularization is necessary because cluster contributions to the merge criterion (and the classification likelihood) are undefined for singletons, as well as for other combinations of observations resulting in a singular or near-singular sample covariance.\nTraditional hierarchical clustering procedures, as implemented, for example, in the base R function hclust(), are typically represented graphically with a dendrogram  that shows the links between objects using a tree diagram, usually with the root at the top and leaves at the bottom. The height of the branches reflects the order in which the clusters are joined, and, to some extent, it also reflects the distances between clusters (see, for example, Hastie, Tibshirani, and Friedman 2009, sec. 14.3.12).\nBecause the regularized merge criterion may either increase or decrease from stage to stage, the association of a positive distance with merges in traditional hierarchical clustering does not apply to MBAHC. However, it is still possible to draw a dendrogram in the which the tree height increases as the number of groups decreases, so that the root is at the top of the hierarchy. Moreover, the classification likelihood often increases in the later stages of merging (small numbers of clusters), in which case the corresponding dendrogram can be drawn.\nmclust provides the function hc()  to perform model-based agglomerative hierarchical clustering. It takes the data matrix to be clustered as the main argument. The principal optional arguments are as follows:\n\nmodelName: for selecting the model to fit among the four models available, which, following the nomenclature in Table 2.1, are denoted as EII, VII, EEE, and VVV (default).\npartition: for providing an initial partition from which to start the agglomeration. The default is to start with partitioning into unique observations.\nuse: for specifying the data transformation to apply before performing hierarchical clustering. By default use = \"VARS\", so the variables are expressed in the original scale. Other possible values, including the default for initialization of the EM algorithm in model-based clustering, are described in Section 3.7.\n\n\nExample 3.9   Model-based hierarchical cluastering of European unemployment data\nConsider the EuroUnemployment dataset which provides unemployment rates for 31 European countries for the year 2014. The considered rates are shown in the table below.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nTUR\nTotal unemployment rate, defined as the percentage of unemployed persons aged 15–74 in the economically active population.\n\n\nYUR\nYouth unemployment rate, defined as the percentage of unemployed persons aged 15–24 in the economically active population.\n\n\nLUR\nLong-term unemployment rate, defined as the percentage of unemployed persons who have been unemployed for 12 months or more.\n\n\n\n\ndata(\"EuroUnemployment\", package = \"mclust\")\nsummary(EuroUnemployment)\n##       TUR             YUR            LUR       \n##  Min.   : 3.50   Min.   : 7.7   Min.   : 0.60  \n##  1st Qu.: 6.35   1st Qu.:15.4   1st Qu.: 2.10  \n##  Median : 8.70   Median :20.5   Median : 3.70  \n##  Mean   :10.05   Mean   :23.2   Mean   : 4.86  \n##  3rd Qu.:11.35   3rd Qu.:24.1   3rd Qu.: 6.80  \n##  Max.   :26.50   Max.   :53.2   Max.   :19.50\n\nThe following code fits the isotropic EII and unconstrained VVV agglomerative hierarchical clustering models to this data:\n\nHC_EII = hc(EuroUnemployment, modelName = \"EII\")\nHC_VVV = hc(EuroUnemployment, modelName = \"VVV\")\n\nMerge dendrograms (with uniform length between the hierarchy levels) for the EII and VVV models are plotted and shown in Figure 3.26.\nplot(HC_EII, what = \"merge\", labels = TRUE, hang = 0.02)\nplot(HC_VVV, what = \"merge\", labels = TRUE, hang = 0.02)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.26: Dendrograms with height corresponding to the number of groups for model-based hierarchical clustering of the EuroUnemployment data with the EII (a) and VVV (b) models.\n\n\nLog-likelihood dendrograms for the EII and VVV models are plotted and shown in Figure 3.27. These dendrograms extend from one cluster only as far as the classification likelihood increases and is defined (no singletons or singular covariances).\nplot(HC_EII, what = \"loglik\")\nplot(HC_VVV, what = \"loglik\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.27: Dendrograms with height corresponding to the classification log-likelihood for model-based hierarchical clustering of the EuroUnemployment data with the EII (a) and VVV (b) models. The log-likelihood is undefined at the other levels of the trees due to the presence of singletons.\n\n\n\n\n3.6.1 Agglomerative Clustering for Large Datasets\nModel-based hierarchical agglomerative clustering is computationally demanding on large datasets, both in terms of time and memory. However, it is possible to initialize the modeling from a partition containing fewer groups than the number of observations. For example, Posse (2001) proposed using a subgraph of the minimum spanning tree associated with the data to derive an initial partition for model-based hierarchical clustering. He illustrated the method by applying it to a multiband MRI image of the human brain and to data on global precipitation climatology.\nThe optional argument partition available in the function hc() allows us to specify an initial partition of observations from which to start the agglomeration process.\n\nExample 3.10   Model-based hierarchical clustering on star data\nAs a simple illustration, consider the HRstars dataset from the GDAdata package (Unwin 2015), which contains 6,200 measurements of star data on four variables. We first use \\(k\\)-means to divide the data into 100 groups, and then apply MBAHC to the data with this initial partition.\n\ndata(\"HRstars\", package = \"GDAdata\")\nset.seed(0)\ninitial = kmeans(HRstars[, -1], centers = 100, nstart=10)$cluster\nHC_VVV = hc(HRstars[, -1], modelName = \"VVV\", \n            partition = initial, use = \"VARS\")\nHC_VVV\n## Call:\n## hc(data = HRstars[, -1], modelName = \"VVV\", use = \"VARS\", partition =\n## initial) \n## \n## Model-Based Agglomerative Hierarchical Clustering \n## Model name        = VVV \n## Use               = VARS \n## Number of objects = 6220",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#sec-mclust_init",
    "href": "chapters/03_cluster.html#sec-mclust_init",
    "title": "3  Model-Based Clustering",
    "section": "\n3.7 Initialization in mclust",
    "text": "3.7 Initialization in mclust\nAs discussed in Section 2.2.3, initialization of the EM algorithm in model-based clustering is often crucial, because the likelihood surfaces for models of interest tend to have many local maxima and may even be unbounded, not to mention having regions where progress is slow. In mclust, the EM algorithm for multivariate data is initialized with the partitions obtained from model-based agglomerative hierarchical clustering (MBAHC), by default using the unconstrained (VVV) model. Efficient numerical algorithms for agglomerative hierarchical clustering with multivariate normal models have been discussed by Fraley (1998) and briefly reviewed in Section 3.6.  In this approach, the two clusters are merged that yield the minimum regularized merge criterion (derived from the classification likelihood) over all possible merges at the current stage of the procedure.\nUsing unconstrained MBAHC is particularly convenient for GMMs because the underlying probability model (VVV) is shared to some extent by both the initialization step and the model fitting step. This strategy also has the advantage in that it provides the basis for initializing EM for any number of mixture components and component-covariance parameterizations. Although there is no guarantee that EM when so initialized will converge to a finite local optimum, it often provides reasonable starting values.\nAn issue with hierarchical clustering methods is that the resolution of ties in the merge criterion can have a significant effect on the downstream clustering outcome. Such ties are not uncommon when the data are discrete or when continuous data are rounded. Moreover, as the merges progress, these ties may not be exact for numerical reasons, and as a consequence results may be sensitive to relatively small changes in the data, including the order of the variables.\nOne way of detecting sensitivity in the results of a clustering method is to apply it to perturbed data. Such perturbations can be implemented in a number of ways, such as jittering the data, changing the order of the variables, or comparing results on different subsets of the data when there are a sufficient number of observations. Iterative methods, such as EM, can be initialized with different starting values.\n\nExample 3.11   Clustering flea beatle data\nAs an illustration of the issues related to the initialization of the EM algorithm, consider the flea beatle data available in package tourr (Wickham and Cook 2022). This dataset provides six physical measurements for a sample of 74 fleas from three species as shown in the table below.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nspecies\nSpecies of flea beetle from the genus Chaetocnema: concinna, heptapotamica, heikertingeri.\n\n\ntars1\nWidth of the first joint of the first tarsus in microns (the sum of measurements for both tarsi)\n\n\ntars2\nWidth of the second joint of the first tarsus in microns (the sum of measurements for both tarsi)\n\n\nhead\nMaximal width of the head between the external edges of the eyes (in \\(0.01\\) mm).\n\n\nade1\nMaximal width of the aedeagus in the fore-part (in microns).\n\n\nade2\nFront angle of the aedeagus (1 unit = \\(7.5\\) degrees)\n\n\nade3\nAedeagus width from the side (in microns).\n\n\n\n\ndata(\"flea\", package = \"tourr\")\nX = data.matrix(flea[, 1:6])\nClass = factor(flea$species, \n               labels = c(\"Concinna\", \"Heikertingeri\", \"Heptapotamica\")) \ntable(Class)\n## Class\n##      Concinna Heikertingeri Heptapotamica \n##            21            31            22\n\nFigure 3.28, obtained with the code below, shows the scatterplot matrix of variables in the flea dataset with points marked according to the flea species. Since the observed values are rounded (to the nearest integer presumably), there is a considerable overplotting of points. For this reason, we added transparency (or opacity) to cluster colors via the alpha channel in the following plot:\n\ncol = mclust.options(\"classPlotColors\")\nclp = clPairs(X, Class, lower.panel = NULL, gap = 0,\n              symbols = c(16, 15, 17), \n              colors = adjustcolor(col, alpha.f = 0.5))\nclPairsLegend(x = 0.1, y = 0.3, class = clp$class, \n              col = col, pch = clp$pch,\n              title = \"Flea beatle species\")\n\n\n\n\n\n\nFigure 3.28: Scatterplot matrix for the flea dataset with points marked according to the true classes.\n\n\n\n\nLuca Scrucca et al. (2016) discussed model-based clustering for this dataset and showed that using the original variables for the MBAHC initialization step leads to sub-optimal results in Mclust().\n\n# set the default for the current session\nmclust.options(\"hcUse\" = \"VARS\")\nmod1 = Mclust(X)\n# or specify the initialization method only for this model\n# mod1 = Mclust(X, initialization = list(hcPairs = hc(X, use = \"VARS\")))\nsummary(mod1)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEE (ellipsoidal, equal volume, shape and orientation) model\n## with 3 components: \n## \n##  log-likelihood  n df     BIC    ICL\n##         -395.01 74 41 -966.49 -966.5\n## \n## Clustering table:\n##  1  2  3 \n## 21 22 31\ntable(Class, mod1$classification)\n##                \n## Class            1  2  3\n##   Concinna      21  0  0\n##   Heikertingeri  0  0 31\n##   Heptapotamica  0 22  0\nadjustedRandIndex(Class, mod1$classification)\n## [1] 1\n\nIf we reverse the order of the variables in the fit, MBAHC yields a different set of initial partitions for EM, a consequence of the discrete nature of the data. With this initialization, Mclust still chooses the EEE model with 5 components, but with a different clustering partition.\n\nmod2 = Mclust(X[, 6:1])\nsummary(mod2)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEE (ellipsoidal, equal volume, shape and orientation) model\n## with 3 components: \n## \n##  log-likelihood  n df     BIC    ICL\n##         -395.01 74 41 -966.49 -966.5\n## \n## Clustering table:\n##  1  2  3 \n## 21 22 31\ntable(Class, mod2$classification)\n##                \n## Class            1  2  3\n##   Concinna      21  0  0\n##   Heikertingeri  0  0 31\n##   Heptapotamica  0 22  0\nadjustedRandIndex(Class, mod2$classification)\n## [1] 1\n\nThis second model has a higher BIC and better clustering accuracy.\nIn order to assess the stability of the results, we randomly start the EM algorithm using the function hcRandomPairs() to obtain a random hierarchical partitioning of the data for initialization:\n\nmod3 = Mclust(X, initialization = list(hcPairs = hcRandomPairs(X, seed = 1)))\nsummary(mod3)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VEE (ellipsoidal, equal shape and orientation) model with 4\n## components: \n## \n##  log-likelihood  n df     BIC     ICL\n##         -385.73 74 51 -990.98 -991.14\n## \n## Clustering table:\n##  1  2  3  4 \n## 28 22  3 21\ntable(Class, mod3$classification)\n##                \n## Class            1  2  3  4\n##   Concinna       0  0  0 21\n##   Heikertingeri 28  0  3  0\n##   Heptapotamica  0 22  0  0\nadjustedRandIndex(Class, mod3$classification)\n## [1] 0.9286\n\nIn this case, we obtain the VEE model with 4 components, which has a lower BIC but a higher ARI compared to the previous model. The function mclustBICupdate()  can be used to retain the best models for each combination of number of components and cluster-covariance parameterization across several replications:\n\nBIC = NULL\nfor (i in 1:50)\n{\n  # get BIC table from initial random start\n  BIC0 = mclustBIC(X, verbose = FALSE,\n                   initialization = list(hcPairs = hcRandomPairs(X)))\n  # update BIC table by merging best BIC values for each\n  # G and modelNames\n  BIC  = mclustBICupdate(BIC, BIC0)\n}\nsummary(BIC, k = 5)\n## Best BIC values:\n##            EEE,3     VEE,3    EEE,4    VVE,4   VEE,4\n## BIC      -966.49 -970.3464 -977.888 -986.025 -991.67\n## BIC diff    0.00   -3.8529  -11.394  -19.531  -25.18\n\nWe can then recover the best model object by a subsequent call to Mclust:\n\nmod4 = Mclust(X, x = BIC)\nsummary(mod4)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEE (ellipsoidal, equal volume, shape and orientation) model\n## with 3 components: \n## \n##  log-likelihood  n df     BIC    ICL\n##         -395.01 74 41 -966.49 -966.5\n## \n## Clustering table:\n##  1  2  3 \n## 31 22 21\ntable(Class, mod4$classification)\n##                \n## Class            1  2  3\n##   Concinna       0  0 21\n##   Heikertingeri 31  0  0\n##   Heptapotamica  0 22  0\nadjustedRandIndex(Class, mod4$classification)\n## [1] 1\n\n\nThe above procedure is able to identify the true clustering structure, but it is a time-consuming process which for large datasets may not be feasible. Moreover, even when using multiple random starts, there is no guarantee that the best solution found is the best that can be achieved.\nL. Scrucca and Raftery (2015) discussed several approaches to improve the hierarchical clustering initialization for model-based clustering. The main idea is to apply a transformation to the data in an effort to enhance separation among clusters before applying the MBAHC for the initialization step. The EM algorithm is then run using the data on the original scale. Among the studied transformations, one that often worked reasonably well (and is used as the default in mclust) is the scaled SVD transformation. \nLet \\(\\boldsymbol{X}\\) be the \\((n \\times p)\\) data matrix, and \\(\\mathbb{X}= (\\boldsymbol{X}- \\boldsymbol{1}_n\\bar{\\boldsymbol{x}}{}^{\\!\\top})\\boldsymbol{S}^{-1/2}\\) be the corresponding centered and scaled matrix, where \\(\\bar{\\boldsymbol{x}}\\) is the vector of sample means, \\(\\boldsymbol{1}_n\\) is the unit vector of length \\(n\\), and \\(\\boldsymbol{S}= \\diag(s^2_1, \\dots, s^2_d)\\) is the diagonal matrix of sample variances. Consider the following singular value decomposition (SVD): \\[\n\\mathbb{X}= \\boldsymbol{U}\\boldsymbol{\\Omega}\\boldsymbol{V}{}^{\\!\\top}= \\sum_{i=1}^r \\omega_i\\boldsymbol{u}_i\\boldsymbol{v}{}^{\\!\\top}_i,\n\\] where \\(\\boldsymbol{u}_i\\) are the left singular vectors, \\(\\boldsymbol{v}_i\\) the right singular vectors, \\(\\omega_1 \\ge \\omega_2 \\ge \\dots \\ge \\omega_r &gt; 0\\) the corresponding singular values, and \\(r \\le \\min(n,d)\\) the rank of matrix \\(\\mathbb{X}\\), with equality when there are no singularities. The scaled SVD transformation is computed as: \\[\n\\boldsymbol{\\mathscr{X}}= \\mathbb{X}\\boldsymbol{S}^{-1/2} \\boldsymbol{V}\\boldsymbol{\\Omega}^{-1/2} = \\boldsymbol{U}\\boldsymbol{\\Omega}^{1/2},\n\\] for which \\(\\Exp(\\boldsymbol{\\mathscr{X}}) = \\boldsymbol{0}\\) and \\(\\Var(\\boldsymbol{\\mathscr{X}}) = \\boldsymbol{\\Omega}/n = \\diag(\\omega_i)/n\\). Thus, in the transformed scale the features are centered, uncorrelated, and with decreasing variances equal to the square root of the eigenvalues of the marginal sample correlation matrix.\n\nExample 3.12   Clustering flea beatle data (continued)\nA GMM estimation initialized using MBAHC with the scaled SVD transformation of the data described above is obtained with the following code:\n\nmclust.options(\"hcUse\" = \"SVD\")  # restore the default\nmod5 = Mclust(X) # X is the unscaled flea data\n# or specify only for this model fit\n# mod5 = Mclust(X, initialization = list(hcPairs = hc(X, use = \"SVD\")))\nsummary(mod5)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEE (ellipsoidal, equal volume, shape and orientation) model\n## with 3 components: \n## \n##  log-likelihood  n df     BIC    ICL\n##         -395.01 74 41 -966.49 -966.5\n## \n## Clustering table:\n##  1  2  3 \n## 21 31 22\ntable(Class, mod5$classification)\n##                \n## Class            1  2  3\n##   Concinna      21  0  0\n##   Heikertingeri  0 31  0\n##   Heptapotamica  0  0 22\nadjustedRandIndex(Class, mod5$classification)  \n## [1] 1\n\nIn this case, a single run with the scaled SVD initialization strategy (the default strategy for the Mclust() function) yields the highest BIC, and a perfect classification of the fleas into the actual species. However, as the true clustering would not usually be known, analysis with perturbations and/or multiple initialization strategies is always advisable.\n\nWith EM, it is possible to do model-based clustering starting with parameter estimates, conditional probabilities, or classifications other than those produced by model-based agglomerative hierarchical clustering. The next section provides some further details on how to run an EM algorithm for Gaussian mixtures in mclust.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#sec-mclust_em",
    "href": "chapters/03_cluster.html#sec-mclust_em",
    "title": "3  Model-Based Clustering",
    "section": "\n3.8 EM Algorithm in mclust",
    "text": "3.8 EM Algorithm in mclust\nAs described in Section 2.2.2, an iteration of EM consists of an E-step and an M-step. The E-step computes a matrix \\(\\boldsymbol{Z}=\\{z_{ik}\\}\\), where \\(z_{ik}\\) is an estimate of the conditional probability that observation \\(i\\) belongs to cluster \\(k\\) given the current parameter estimates. The M-step computes parameter estimates given \\(\\boldsymbol{Z}\\).\nmclust provides functions em() and me() implementing the EM algorithm for maximum likelihood estimation in Gaussian mixture models for all 14 of the covariance parameterizations based on eigen-decomposition. The em()  function starts with the E-step; besides the data and model specification, the model parameters (means, covariances, and mixing proportions) must be provided. The me()  function, on the other hand, starts with the M-step; besides the data and model specification, the matrix of conditional probabilities must be provided. The output for both are the maximum likelihood estimates of the model parameters and the matrix of conditional probabilities.\nFunctions estep()  and mstep()  implement the individual E- and M- steps, respectively, of an EM iteration. Conditional probabilities and the log-likelihood can be recovered from parameters via estep(), while parameters can be recovered from conditional probabilities using mstep().\n\nExample 3.13   Single M- and E- steps using the iris data\nConsider the well-known iris dataset (Anderson 1935, Fisher:1936) which contains the measurements (in cm) of sepal length and width, and petal length and width, for 50 flowers from each of 3 species of Iris, namely setosa, versicolor, and virginica.\nThe following code shows how single M- and E- steps can be performed using functions mstep() and estep():\n\ndata(\"iris\", package = \"datasets\")\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n## $ Species : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1\n##    1 1 1 ...\nms = mstep(iris[, 1:4], modelName = \"VVV\",\n            z = unmap(iris$Species))\nstr(ms, 1)\n## List of 7\n## $ modelName : chr \"VVV\"\n## $ prior : NULL\n## $ n : int 150\n## $ d : int 4\n## $ G : int 3\n## $ z : num [1:150, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n## ..- attr(*, \"dimnames\")=List of 2\n## $ parameters:List of 3\n## - attr(*, \"returnCode\")= num 0\nes = estep(iris[, 1:4], modelName = \"VVV\", \n            parameters = ms$parameters)\nstr(es, 1)\n## List of 7\n## $ modelName : chr \"VVV\"\n## $ n : int 150\n## $ d : int 4\n## $ G : int 3\n## $ z : num [1:150, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n## ..- attr(*, \"dimnames\")=List of 2\n## $ parameters:List of 3\n## $ loglik : num -183\n## - attr(*, \"returnCode\")= num 0\n\nIn this example, the initial estimate of z for the M-step is a matrix of indicator variables corresponding to a discrete classification taken from the true classes contained in iris$Species. Here, we used the function unmap()  to convert the classification into the corresponding matrix of indicator variables. The inverse function, called map(),  converts a matrix in which each row sums to 1 into an integer vector specifying for each row the column index of the maximum. This last operation is basically the MAP when applied to the matrix of posterior conditional probabilities (see Section 2.2.4).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/03_cluster.html#further-considerations-in-cluster-analysis-via-mixture-modeling",
    "href": "chapters/03_cluster.html#further-considerations-in-cluster-analysis-via-mixture-modeling",
    "title": "3  Model-Based Clustering",
    "section": "\n3.9 Further Considerations in Cluster Analysis via Mixture Modeling",
    "text": "3.9 Further Considerations in Cluster Analysis via Mixture Modeling\nClustering can be affected by control parameter settings, such as convergence tolerances within the clustering functions, although the defaults are often adequate.\nControl parameters used by the EM functions in mclust are set and retrieved using the function emControl(),  These include:\n\neps: A tolerance value for terminating iterations due to ill-conditioning, such as near singularity in covariance matrices. By default this is set to .Machine$double.eps, the relative machine precision, which has the value 2.220446e-16 on IEEE-compliant machines.\ntol: A vector of length 2 giving iteration convergence tolerances. By default this is set to c(1.0e-5, sqrt(.Machine$double.eps)). The first value is the tolerance for the relative convergence of the log-likelihood in the EM algorithm, and the second value is the relative convergence tolerance for the M-step of those models that have an iterative M-step, namely the models VEI, EVE, VEE, VVE, and VEV (see Table 2.2).\nitmax : An integer vector of length two giving integer limits on the number of EM iterations and on the number of iterations in the inner loop for models with iterative M-step (see above). By default this is set to c(.Machine$integer.max, .Machine$integer.max), allowing termination to be completely governed by the control parameter tol. A warning is issued if this limit is reached before the convergence criterion is satisfied.\n\nAlthough these control settings are in a sense hidden by the defaults, they may have a significant effect on results in some instances and should be taken into consideration in any analysis.\nThe mclust implementation includes various warning messages in cases of potential problems. These are issued if mclust.options(\"warn\") is set to TRUE or specified directly in function calls. Note, however, that by default mclust.options(\"warn\") = FALSE, so that these warning messages are suppressed.\nFinally, it is important to take into account numerical issues in model-based cluster analysis, and more generally in GMM estimation. The computations for estimating the model parameters break down when the covariance corresponding to one or more components becomes ill-conditioned (singular or nearly singular), and cannot proceed if clusters contain only a few observations or if the observations they contain are nearly colinear. Estimation may also fail when one or more mixing proportions shrink to negligible values. Including a prior is often helpful in such situations (see Section 7.2).\n\n\n\n\nAnderson, E. 1935. “The Irises of the Gaspé Peninsula.” Bulletin of the American Iris Society 59: 2–5.\n\n\nBanfield, J., and Adrian E. Raftery. 1993. “Model-Based Gaussian and Non-Gaussian Clustering.” Biometrics 49: 803–21.\n\n\nBasford, K E, D R Greenway, G J McLachlan, and D Peel. 1997. “Standard Errors of Fitted Component Means of Normal Mixtures.” Computational Statistics 12 (1): 1–18.\n\n\nCeleux, G., and G. Govaert. 1995. “Gaussian Parsimonious Clustering Models.” Pattern Recognition 28: 781–93.\n\n\nCoomans, D, and I Broeckaert. 1986. Potential Pattern Recognition in Chemical and Medical Decision Making. Letchworth, England: Research Studies Press.\n\n\nDua, Dheeru, and Casey Graff. 2017. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. http://archive.ics.uci.edu/ml.\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” Annals of Statistics 7: 1–26.\n\n\nForina, M., C. Armanino, M. Castino, and M. Ubigli. 1986. “Multivariate Data Analysis as a Discriminating Method of the Origin of Wines.” Vitis 25: 189–201. ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine.\n\n\nFraley, Chris. 1998. “Algorithms for Model-Based Gaussian Hierarchical Clustering.” SIAM Journal on Scientific Computing 20 (1): 270–81.\n\n\nFraley, Chris, and Adrian E Raftery. 2007. “Bayesian Regularization for Normal Mixture Estimation and Model-Based Clustering.” Journal of Classification 24 (2): 155–81.\n\n\nFriedman, Herman P, and Jerrold Rubin. 1967. “On Some Invariant Criteria for Grouping Data.” Journal of the American Statistical Association 62 (320): 1159–78.\n\n\nHabbema, J D F, J. Hermans, and K van den Broek. 1974. “A Stepwise Discriminant Analysis Program Using Density Estimation.” In Proceedings in Computational Statistics, 101–10. Vienna: Physica-Verlag: COMPSTAT.\n\n\nHärdle, Wolfgang Karl. 1991. Smoothing Techniques: With Implementation in S. Springer Science & Business Media.\n\n\nHastie, T., R. Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer-Verlag. http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/.\n\n\nHubert, L., and P. Arabie. 1985. “Comparing Partitions.” Journal of Classification 2: 193–218.\n\n\nHurley, Catherine. 2019. gclus: Clustering Graphics. https://CRAN.R-project.org/package=gclus.\n\n\nMcNeil, D. R. 1977. Interactive Data Analysis. New York: Wiley.\n\n\nPosse, C. 2001. “Hierarchical Model-Based Clustering for Large Datasets.” Journal of Computational and Graphical Statistics 10 (3): 464–86.\n\n\nReaven, GM, and RG Miller. 1979. “An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis.” Diabetologia 16 (1): 17–24.\n\n\nScott, AJ, and Michael J Symons. 1971. “Clustering Methods Based on Likelihood Ratio Criteria.” Biometrics 27 (2): 387–97.\n\n\nScrucca, L., and A. E. Raftery. 2015. “Improved Initialisation of Model-Based Clustering Using Gaussian Hierarchical Partitions.” Advances in Data Analysis and Classification 4 (9): 447–60. https://doi.org/10.1007/s11634-015-0220-z.\n\n\nScrucca, Luca, Michael Fop, Thomas Brendan Murphy, and Adrian E. Raftery. 2016. “mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.” The R Journal 8 (1): 205–33. https://journal.r-project.org/archive/2016-1/scrucca-fop-murphy-etal.pdf.\n\n\nTodorov, Valentin. 2022. rrcov: Scalable Robust Estimators with High Breakdown Point. https://CRAN.R-project.org/package=rrcov.\n\n\nUnwin, Antony. 2015. GDAdata: Datasets for the Book Graphical Data Analysis with R. https://CRAN.R-project.org/package=GDAdata.\n\n\nWard, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective Function.” Journal of the American Statistical Association 58 (301): 236–44.\n\n\nWickham, Hadley, and Dianne Cook. 2022. tourr: Implement Tour Methods in r Code. https://CRAN.R-project.org/package=tourr.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model-Based Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html",
    "href": "chapters/04_classification.html",
    "title": "4  Mixture-Based Classification",
    "section": "",
    "text": "4.1 Classification as Supervised Learning\nChapter 3 discussed methods for clustering, an instance of unsupervised learning. There the main goal was to identify the presence of groups of homogeneous observations based on the measurements available for a set of variables or features. This chapter deals with the supervised learning  problem, where the classification of each observation is known. In this case, the main objective is to build a classifier (or decision rule) for classifying future observations from the available data (Bishop 2006; T. Hastie, Tibshirani, and Friedman 2009; Alpaydin 2014). This task is also known by various other names, such as statistical pattern recognition or discriminant analysis (G. McLachlan 2004).\nIn the probabilistic approach to classification, a statistical model is estimated to predict the class \\(C_k\\) for \\(k=1, \\dots, K\\) of a given observation with feature vector \\(\\boldsymbol{x}\\). This model provides a posterior class probability  \\(\\Pr(C_k|\\boldsymbol{x})\\) for each class, which can then be used to determine the class membership for new observations. Some modeling methods directly estimate posterior probabilities by constructing, or learning, a discriminant function \\(\\eta_k(\\boldsymbol{x}) = \\Pr(C_k|\\boldsymbol{x})\\) that maps the features \\(\\boldsymbol{x}\\) directly onto a class \\(C_k\\). These are called discriminative models,  of which a popular instance is the logistic regression model for binary-class problems.\nOther approaches try to explicitly or implicitly model the distribution of features as well as classes, and then obtain the posterior probabilities using Bayes’ theorem. Thus, by learning the class-conditional densities \\(f(\\boldsymbol{x}|C_k)\\) and the prior class probabilities \\(\\Pr(C_k)\\) for each class \\(C_k\\) (\\(k=1, \\dots, K\\)), the posterior class probabilities are given by% can be found as \\[\n\\Pr(C_k|\\boldsymbol{x}) = \\frac{f(\\boldsymbol{x}|C_k) \\Pr(C_k)}{\\displaystyle\\sum_{g=1}^K f(\\boldsymbol{x}|C_g) \\Pr(C_g)} .\n\\] Methods that follow this approach, such as those based on finite mixture modeling, are called generative models.\nTypically, classification models are estimated using the information from a training set, meaning a dataset used for learning or fitting the model in order to obtain parameter estimates. The same dataset, if used also for model tuning such as hyperparameter estimation or feature selection and for evaluating the classifier, tends to produce an optimistic assessment of performance. This phenomenon is called overfitting,  meaning that there is a risk of fitting a model that too closely corresponds to a particular set of data, and therefore may fail to fit additional data or predict future observations well. For these reasons, it is advisable to perform model tuning using a validation set, a dataset designed for this purpose and usually set aside from the original dataset. An alternative is to repeatedly split into a training set and a validation set using resampling approaches, such as cross-validation, to be discussed in Section 4.4.3. Another subset of the original dataset is also set aside in advance as a test set for the final evaluation of the classifier.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#sec-gaussmixmodclas",
    "href": "chapters/04_classification.html#sec-gaussmixmodclas",
    "title": "4  Mixture-Based Classification",
    "section": "\n4.2 Gaussian Mixture Models for Classification",
    "text": "4.2 Gaussian Mixture Models for Classification\nConsider a training dataset \\(\\mathcal{D}_\\text{train}= \\{(\\boldsymbol{x}_1, y_1), \\dots, (\\boldsymbol{x}_n, y_n)\\}\\) for which both the feature vectors \\(\\boldsymbol{x}_i\\) and the true classes \\(y_i \\in \\{C_1, \\dots, C_K\\}\\) are known. Each observation has an associated class label \\(C_k\\).\nMixture-based classification models typically assume that the density within each class follows a Gaussian mixture distribution: \\[\nf(\\boldsymbol{x}|C_k) = \\sum_{g=1}^{G_k} \\pi_{g,k} \\phi(\\boldsymbol{x}; \\boldsymbol{\\mu}_{g,k}, \\boldsymbol{\\Sigma}_{g,k}),\n\\tag{4.1}\\] where \\(G_k\\) is the number of components within class \\(k\\), \\(\\pi_{g,k}\\) are the mixing probabilities for class \\(k\\) (\\(\\pi_{g,k} &gt; 0\\) and \\(\\sum_{g=1}^{G_k}\\pi_{g,k}=1\\)), and \\(\\boldsymbol{\\mu}_{g,k}\\) and \\(\\boldsymbol{\\Sigma}_{g,k}\\) are, respectively, the mean vectors and the covariance matrices for component \\(g\\) within class \\(k\\).\nTrevor Hastie and Tibshirani (1996) proposed the Mixture Discriminant Analysis (MDA)  model where it is assumed that the covariance matrix is the same for all classes but is otherwise unconstrained (\\(\\boldsymbol{\\Sigma}_{gk} = \\boldsymbol{\\Sigma}\\) for all \\(g=1,\\dots,G_k\\) and \\(k=1,\\dots,K\\) in Equation 4.1). Moreover, the number of mixture components is the same for each class and assumed known.\nBensmail and Celeux (1996) proposed the Eigenvalue Decomposition Discriminant Analysis (EDDA)  model which assumes that the density for each class can be described by a single Gaussian component (\\(G_k=1\\) for all \\(k\\) in Equation 4.1), with the class covariance structure factorized as \\[\n\\boldsymbol{\\Sigma}_{k} = \\lambda_k\\boldsymbol{U}_k\\boldsymbol{\\Delta}_k\\boldsymbol{U}{}^{\\!\\top}_k.\n\\]\nAs for GMM clustering, several classification models can be obtained from the above decomposition. If each component has the same covariance matrix (\\(\\boldsymbol{\\Sigma}_{k} = \\lambda\\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\) — model EEE in Table 2.1), then EDDA is equivalent to the classical Linear Discriminant Analysis (LDA) model. If the component covariance matrices are unconstrained and vary between components (\\(\\boldsymbol{\\Sigma}_{k} = \\lambda_k\\boldsymbol{U}_k\\boldsymbol{\\Delta}_k\\boldsymbol{U}{}^{\\!\\top}_k\\) — model VVV in Table 2.1), then EDDA is equivalent to the Quadratic Discriminant Analysis (QDA) model. Finally, by assuming conditional independence of features within each class (models with coordinate axes orientation, denoted by **I in Table 2.1), the Naïve-Bayes models are obtained.\nThe most general model from Equation 4.1 is the MclustDA  model proposed by Fraley and Raftery (2002), which uses a finite mixture of Gaussian distributions within each class, in which the number of components and covariance matrix (parameterized by the eigen-decomposition described in Section 2.2.1) may differ among classes.\n\n4.2.1 Prediction\nLet \\(\\tau_k\\) be the class prior probability that an observation \\(\\boldsymbol{x}\\) comes from class \\(C_k\\) (\\(k=1,\\dots,K\\)). By Bayes’ theorem we can compute the posterior probability that an observation \\(\\boldsymbol{x}\\) belongs to class \\(C_k\\) as \\[\n\\Pr(C_k | \\boldsymbol{x}) = \\frac{\\tau_k f(\\boldsymbol{x}| C_k)}{\\displaystyle\\sum_{j=1}^K \\tau_{j} f(\\boldsymbol{x}| C_{j})},\n\\tag{4.2}\\] where \\(f(\\boldsymbol{x}|C_k)\\) is the probability density function in Equation 4.1 specific to class \\(C_k\\). As discussed earlier, this density depends on the assumed model for within-class distributions.\nThus an observation \\(\\boldsymbol{x}\\) can be classified according to the maximum a posteriori (MAP) rule to the class which has the highest posterior probability: \\[\ny = \\{ C_{\\widehat{k}} \\}\n\\qquad\\text{where}\\quad\n\\widehat{k} = \\argmax_{k} \\Pr(C_k | \\boldsymbol{x}) \\propto \\tau_k f(\\boldsymbol{x}| C_k),\n\\tag{4.3}\\] where the right-hand side follows by noting that the denominator in Equation 4.2 is just a constant of normalization. This rule minimizes the expected misclassification rate and is known as the Bayes classifier. \n\n4.2.2 Estimation\nThe parameters of the model in Equation 4.1 can be estimated from the training dataset by maximum likelihood. In particular, for the EDDA model the parameters can be obtained with a single M-step from the EM algorithm for Gaussian mixtures described in Section 2.2.2, with \\(z_{ik}\\) set to \\(1\\) if observation \\(i\\) belongs to class \\(k\\) and \\(0\\) otherwise. For the general MclustDA model, as well as for MDA, a Gaussian mixture model can be estimated separately for each class using the EM algorithm, and parameters cannot be constrained across classes.\nFor the class prior probabilities, if the training data have been obtained by random sampling from the underlying population, the mixing proportions \\(\\tau_k\\) can be simply estimated by the sample proportions \\(n_k/n\\), where \\(n_k\\) denotes the number of observations known to belong to class \\(k\\) and \\(n = \\sum_{k=1}^K n_k\\) is the number of observations in the training set. However, there are instances in which different values have to be assigned to the prior probabilities for the classes.\nThis includes cases where the cost of misclassification may differ depending on the affected classes, and cases where classes have very different numbers of members. These issues are further discussed in Section 4.6 and Section 4.5, respectively.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#sec-mclustclass",
    "href": "chapters/04_classification.html#sec-mclustclass",
    "title": "4  Mixture-Based Classification",
    "section": "\n4.3 Classification in mclust",
    "text": "4.3 Classification in mclust\nThe main function available in mclust for classification tasks is MclustDA(),  which requires a data frame or a matrix for the training data (data) and the corresponding vector of class labels (class). The type of mixture model to be fitted is specified by the argument modelType, a string that can take the values \"MclustDA\" (default) or \"EDDA\".\n\nExample 4.1   Classification of Wisconsin diagnostic breast cancer data\nConsider a dataset of measurements for 569 patients on 30 features of the cell nuclei obtained from a digitized image of a fine needle aspirate (FNA) of a breast mass [Street, Wolberg, and Mangasarian (1993); Mangasarian:etal:1995], available as one of several contributions to the “Breast Cancer Wisconsin (Diagnostic) Data Set” of the UCI Machine Learning Repository (Dua and Graff 2017). For each patient, the mass was diagnosed as either malignant or benign. This data can be obtained in mclust via the data command under the name wdbc. Following Mangasarian, Street, and Wolberg (1995) and Fraley and Raftery (2002),we consider only three attributes in the following analysis: extreme area, extreme smoothness, and mean texture.\n\ndata(\"wdbc\", package = \"mclust\")\nX = wdbc[, c(\"Texture_mean\", \"Area_extreme\", \"Smoothness_extreme\")]\nClass = wdbc[, \"Diagnosis\"]\n\nWe randomly assign approximately two-thirds of the observations to the training set, and the remaining ones to the test set, as follows:\n\nset.seed(123)\ntrain = sample(1:nrow(X), size = round(nrow(X)*2/3), replace = FALSE)\nX_train = X[train, ]\nClass_train = Class[train]\ntab = table(Class_train)\ncbind(Counts = tab, \"%\" = prop.table(tab)*100)\n##   Counts      %\n## B    251 66.227\n## M    128 33.773\nX_test = X[-train, ]\nClass_test = Class[-train]\ntab = table(Class_test)\ncbind(Counts = tab, \"%\" = prop.table(tab)*100)\n##   Counts      %\n## B    106 55.789\n## M     84 44.211\n\nThe distribution of the features with training data points marked according to cancer diagnosis is shown in Figure 4.1:\n\nclp = clPairs(X_train, Class_train, lower.panel = NULL)\nclPairsLegend(0.1, 0.3, col = clp$col, pch = clp$pch, \n              class = ifelse(clp$class == \"B\", \"Benign\", \"Malign\"),\n              title = \"Breast cancer diagnosis:\")\n\n\n\n\n\n\nFigure 4.1: Pairwise scatterplot matrix of selected features for the breast cancer data with points distinguished by tumor diagnosis.\n\n\n\n\nThe function MclustDA() provides fitting capabilities for the EDDA model by specifying the optional argument modelType = \"EDDA\". The corresponding function call is as follows:\n\nmod1 = MclustDA(X_train, Class_train, modelType = \"EDDA\")\nsummary(mod1)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## EDDA model summary: \n## \n##  log-likelihood   n df     BIC\n##         -2934.1 379 13 -5945.4\n##        \n## Classes   n     % Model G\n##       B 251 66.23   VVI 1\n##       M 128 33.77   VVI 1\n## \n## Training confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 249   2\n##     M  17 111\n## Classification error = 0.0501 \n## Brier score          = 0.0374\n\nThe estimated EDDA mixture model with the largest BIC is the VVI model, in which each group is described by a single Gaussian component with varying volume and shape, and orientation aligned with the coordinate axes. As mentioned earlier, this model is a member of the Naïve-Bayes family. By default, the summary()  function also returns the confusion matrix obtained by cross-tabulation of the input and predicted classes, followed by two measures of accuracy to be discussed in Section 4.4.\nEstimated parameters can be shown with the summary() function by setting the optional argument parameters as follows:\n\nsummary(mod1, parameters = TRUE)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## EDDA model summary: \n## \n##  log-likelihood   n df     BIC\n##         -2934.1 379 13 -5945.4\n##        \n## Classes   n     % Model G\n##       B 251 66.23   VVI 1\n##       M 128 33.77   VVI 1\n## \n## Class prior probabilities:\n##       B       M \n## 0.66227 0.33773 \n## \n## Class = B\n## \n## Means:\n##                         [,1]\n## Texture_mean        17.95530\n## Area_extreme       562.71673\n## Smoothness_extreme   0.12486\n## \n## Variances:\n## [,,1]\n##                    Texture_mean Area_extreme Smoothness_extreme\n## Texture_mean             15.312            0         0.00000000\n## Area_extreme              0.000        26588         0.00000000\n## Smoothness_extreme        0.000            0         0.00040151\n## \n## Class = M\n## \n## Means:\n##                          [,1]\n## Texture_mean         21.80203\n## Area_extreme       1343.71094\n## Smoothness_extreme    0.14478\n## \n## Variances:\n## [,,1]\n##                    Texture_mean Area_extreme Smoothness_extreme\n## Texture_mean             12.408            0         0.00000000\n## Area_extreme              0.000       288727         0.00000000\n## Smoothness_extreme        0.000            0         0.00060343\n## \n## Training confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 249   2\n##     M  17 111\n## Classification error = 0.0501 \n## Brier score          = 0.0374\n\nThe confusion matrix and evaluation metrics for a new test set can be obtained by providing the data matrix of features (newdata) and the corresponding classes (newclass):\n\nsummary(mod1, newdata = X_test, newclass = Class_test)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## EDDA model summary: \n## \n##  log-likelihood   n df     BIC\n##         -2934.1 379 13 -5945.4\n##        \n## Classes   n     % Model G\n##       B 251 66.23   VVI 1\n##       M 128 33.77   VVI 1\n## \n## Training confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 249   2\n##     M  17 111\n## Classification error = 0.0501 \n## Brier score          = 0.0374 \n## \n## Test confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 103   3\n##     M   5  79\n## Classification error = 0.0421 \n## Brier score          = 0.0357\n\nNote that, for this model, the performance metrics on the test set are no worse than those for the training set, and in fact are even slightly better. This indicates that the estimated model is not overfitting the data, which is likely due to the parsimonious covariance model adopted.\nObjects returned by MclustDA() can be visualized in a variety of ways through the associated plot  method. For instance, the pairwise scatterplot matrix between the features, showing both the known classes and the estimated mixture components, is displayed in Figure 4.2 and obtained with the code:\n\nplot(mod1, what = \"scatterplot\")\n\n\n\n\n\n\nFigure 4.2: Pairwise scatterplot matrix of selected features for the breast cancer training data with points distinguished by observed classes and ellipses representing the Gaussian distribution estimated for each class by EDDA.\n\n\n\n\nFigure 4.3 displays the pairwise scatterplots showing the misclassified training data points obtained with the following code:\n\nplot(mod1, what = \"error\")\n\n\n\n\n\n\nFigure 4.3: Pairwise scatterplot matrix of selected features for the breast cancer training data with points distinguished by observed classes and filled black points representing those cases misclassified by the fitted EDDA model.\n\n\n\n\nEDDA imposes a single mixture component for each group. However, in certain circumstances, a more flexible model may result in a better classifier. As mentioned in Section 4.2, a more general approach, called MclustDA, is available, in which a finite mixture of Gaussian distributions is used within each class, with both the number of components and covariance matrix structures (expressed following the usual eigen-decomposition in Equation 2.4 allowed to differ among classes. This is the model estimated by default (or by setting modelType = \"MclustDA\"):\n\nmod2 = MclustDA(X_train, Class_train)\nsummary(mod2, newdata = X_test, newclass = Class_test)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## MclustDA model summary: \n## \n##  log-likelihood   n df     BIC\n##         -2893.6 379 27 -5947.5\n##        \n## Classes   n     % Model G\n##       B 251 66.23   EEI 3\n##       M 128 33.77   EVI 2\n## \n## Training confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 248   3\n##     M   8 120\n## Classification error = 0.029 \n## Brier score          = 0.0262 \n## \n## Test confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 103   3\n##     M   5  79\n## Classification error = 0.0421 \n## Brier score          = 0.0273\n\nMclustDA fits a three-component EEI mixture to benign cases, and a two-component EVI mixture to the malignant cases. Note that diagonal covariance structures are used within each class. The training classification error rate is smaller for this model than for the EDDA model. However, the test misclassification rate is the same for the two types of models. This is an effect of overfitting induced by the increased complexity of the MclustDA model, which has 26 parameters to estimate, more than twice the number required by the EDDA model.\nFigure 4.4 displays a matrix of pairwise scatterplots between the features showing both the known classes and the estimated mixture components drawn with the code:\n\nplot(mod2, what = \"scatterplot\")\n\n\n\n\n\n\nFigure 4.4: Scatterplots of selected features for the breast cancer training data with points distinguished by observed classes and ellipses representing the Gaussian distribution estimated for each class by MclustDA.\n\n\n\n\nSpecific marginals can be obtained by using the optional argument dimens. For instance, the following code produces the scatterplot for the first two features:\n\nplot(mod2, what = \"scatterplot\", dimens = c(1, 2))\n\nFinally, note that the MDA model of Trevor Hastie and Tibshirani (1996) is equivalent to MclustDA with \\(\\boldsymbol{\\Sigma}_{k} = \\lambda\\boldsymbol{U}\\boldsymbol{\\Delta}\\boldsymbol{U}{}^{\\!\\top}\\) (model EEE) and fixed \\(G_k \\ge 1\\) for each class. For instance, an MDA model with two mixture components for each class can be fitted using the code:\n\nmod3 = MclustDA(X_train, Class_train, G = 2, modelNames = \"EEE\")\nsummary(mod3, newdata = X_test, newclass = Class_test)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## MclustDA model summary: \n## \n##  log-likelihood   n df     BIC\n##         -2910.8 379 27 -5981.9\n##        \n## Classes   n     % Model G\n##       B 251 66.23   EEE 2\n##       M 128 33.77   EEE 2\n## \n## Training confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 247   4\n##     M  10 118\n## Classification error = 0.0369 \n## Brier score          = 0.0297 \n## \n## Test confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 104   2\n##     M   4  80\n## Classification error = 0.0316 \n## Brier score          = 0.0248",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#sec-evalclassifier",
    "href": "chapters/04_classification.html#sec-evalclassifier",
    "title": "4  Mixture-Based Classification",
    "section": "\n4.4 Evaluating Classifier Performance",
    "text": "4.4 Evaluating Classifier Performance\nEvaluating the performance of a classifier is an essential part of any classification task. Mixture models used for classification are able to generate two types of predictions: the most likely class label according to the MAP rule in Equation 4.3, and the posterior probabilities of class membership from Equation 4.2. mclust automatically computes one performance measure for each type of prediction, namely the misclassification error and the Brier score, to be described in the following sections. Note, however, that several other measures exist, such as the Receiving Operating Characteristic (ROC) curve and the Area Under the [ROC] Curve (AUC) for two-class cases, and it is possible to compute them using the estimates provided by the predict() method associated with MclustDA objects.\n\n4.4.1 Evaluating Predicted Classes: Classification Error\nThe simplest measure available is the misclassification error rate,  or simply the classification error, which is the proportion of wrong predictions made by the classifier: \\[\n\\CE = \\frac{1}{n} \\sum_{i=1}^n \\mathnormal{I}(\\widehat{y}_i \\ne y_i) ,\n\\tag{4.4}\\] where \\(y_i = \\{C_k\\}\\) is the known class for the \\(i\\)th observation, \\(\\widehat{y}_i = \\{C_{\\widehat{k}}\\}\\) is the predicted class label, and \\(\\mathnormal{I}(\\widehat{y}_i \\ne y_i)\\) is an indicator function that equals \\(1\\) if \\(\\widehat{y}_i \\ne y_i\\) and \\(0\\) otherwise. A good classifier should have a small error rate, preferably close to zero. Equivalently, the accuracy of a classifier is defined as the proportion of correct predictions, \\(1 - \\CE\\). Note, however, that when classes are unbalanced (not represented more or less equally), the error rate or accuracy may not be meaningful. A classifier could have a high overall accuracy yet not be able to accurately detect members of small classes.\n\n4.4.2 Evaluating Class Probabilities: Brier Score\nThe Brier score is a measure of the predictive accuracy for probabilistic predictions. It is computed as the mean squared difference between the true class indicators and the predicted probabilities.\nBased on the original multi-class definition by Brier (1950), the following formula provides the normalized Brier score:  \\[\n\\BS = \\frac{1}{2n} \\sum_{i=1}^n \\sum_{k=1}^K (C_{ik} - \\widehat{p}_{ik})^2 ,\n\\] where \\(n\\) is the number of observations, \\(K\\) is the number of classes, \\(C_{ik} = 1\\) if observation \\(i\\) is from class \\(k\\) and 0 otherwise, and \\(\\widehat{p}_{ik}\\) is the predicted probability that observation \\(i\\) belongs to class \\(k\\). In this formula, the inclusion of the constant 2 in the denominator ensures that the index takes values in the range \\([0,1]\\) (Kruppa et al. 2014, Kruppa:etal:2014b).\nThe Brier score is a strictly proper score (Gneiting and Raftery 2007), which implies that it takes its minimal value only when the predicted probabilities match the empirical probabilities. Thus, small values of the Brier score indicate high prediction accuracy, with \\(\\BS = 0\\) when the observations are all correctly classified with probability one.\n\n4.4.3 Estimating Classifier Performance: Test Set and Resampling-Based Validation\nAny performance measure computed using the training set will tend to provide an optimistic performance estimate. For instance, the training classification error rate \\(\\CE_{\\text{train}}\\) is obtained by applying Equation 4.4 to the training observations. This measure of the accuracy of a classifier is optimistic because the same set of observations is used for both model estimation and for its assessment. A more realistic estimate can be obtained by computing the test misclassification error rate, which is the error rate computed on a fresh test set of \\(m\\) observations \\(\\mathcal{D}_\\text{test}= \\{(\\boldsymbol{x}^*_1,y^*_1), \\dots, (\\boldsymbol{x}^*_m,y^*_m)\\}\\): \\[\n\\CE_{\\text{test}} = \\frac{1}{m} \\sum_{i=1}^{m} \\mathnormal{I}(y^*_i \\ne \\widehat{y}^*_i),\n\\] where \\(\\widehat{y}^*_i\\) is the predicted class label that results from applying the classifier with feature vector \\(\\boldsymbol{x}^*\\).\nThis seems an obvious choice, “but, to get reasonable precision of the performance values, the size of the test set may need to be large” (Kuhn and Johnson 2013, 66). The same considerations also apply to the Brier score.\nIn cases where a test set is not available, or its size is not sufficient to guarantee reliable estimates, an assessment of a model’s performance can be obtained by resampling methods. Different resampling schemes are available, but all rely on modeling repeated samples drawn from a training set.\nCross-validation  is a simple and intuitive way to obtain a realistic performance measure. A standard resampling scheme is the \\(V\\)-fold cross-validation approach, which randomly splits the set of training observations into \\(V\\) parts or folds. At each step of the procedure, data from \\(V-1\\) folds are used for model fitting, and the held-out fold is used as a validation set. Figure 4.5 provides a schematic view of 10-fold cross-validation.\nConsider a generic loss function of the prediction error, say \\(L(y,\\widehat{y})\\), that we would like to minimize. For instance, by setting \\(L(y,\\widehat{y}) = \\mathnormal{I}(y \\ne \\widehat{y})\\), the \\(0-1\\) loss, we obtain the misclassification error rate, whereas by setting \\(L(y,\\widehat{y}) = (C_k - \\widehat{p})^2\\), the squared error with respect to the estimated probability \\(\\widehat{p}\\), we obtain the Brier score. The \\(V\\)-fold cross-validation steps are the following:\n\nSplit the training set into \\(V\\) folds of roughly equal size (and stratified1), say \\(F_1, \\dots, F_V\\).\n\n\n\nFor \\(v=1, \\dots, V\\):\n\nfit the model using \\(\\{(\\boldsymbol{x}_i,y_i): i \\notin F_v\\}\\) as training set;\nevaluate the model using \\(\\{(\\boldsymbol{x}_i,y_i): i \\in F_v\\}\\) as validating set by computing \\[\nL_v = \\frac{1}{n_v} \\sum_{i \\in F_v} L(y_i, \\widehat{y}_i),\n\\] where \\(n_v\\) is the number of observations in fold \\(F_v\\).\n\n\nAverage the loss function over the folds by computing \\[\nL_{\\CV} = \\sum_{v = 1}^V \\frac{n_v}{n} L_v\n=   \\frac{1}{n} \\sum_{v = 1}^V \\sum_{i \\in F_v} L(y_i, \\widehat{y}_i).\n\\]\n\n\n\n\n\n\n\nFigure 4.5: Schematic representation of the 10-fold cross-validation resampling technique.\n\n\nThere is no general rule for choosing an optimal value for \\(V\\). If \\(V = n\\), the procedure is called leave-one-out cross-validation (LOOCV), because one data point is held out at a time. Large values of \\(V\\) reduce the bias of the estimator but increase its variance, while small values of \\(V\\) increase the bias but decrease the variance. Furthermore, for large values of \\(V\\), the computational burden may be quite high. For these reasons, it is often suggested to set \\(V\\) equal to 5 or 10 (T. Hastie, Tibshirani, and Friedman 2009, sec. 7.10).\nAn advantage of using a cross-validation approach is that it provides an estimate of the standard error of the procedure. This can be computed as \\[\n\\se(L_{\\CV}) = \\frac{\\sd(L)}{\\sqrt{V}},\n\\] where \\[\n\\sd(L) = \\sqrt{ \\frac{\\sum_{v=1}^V (L_v - L_{\\CV})^2 n_v}{n(V-1)/V} }.\n\\] The estimate \\(\\se(L_{\\CV})\\) is often used for implementing the one-standard error rule: when models of different complexity are compared, select the simplest model whose performance is within one standard error of the best value (Breiman et al. 1984, sec. 8.1 and 14.1). For an in-depth investigation of the behavior of cross-validation for some commonly used statistical models, see Bates, Hastie, and Tibshirani (2021).\n\n4.4.4 Cross-Validation in mclust\nThe function cvMclustDA()  is available in mclust to carry out \\(V\\)-fold cross-validation as discussed above. It requires an object as returned by MclustDA() and, among the optional arguments, nfold can be used to set the number of folds (by default set to 10).\n\nExample 4.2   Evaluation of classification models using cross-validation for the Wisconsin diagnostic breast cancer data\nConsider the classification models estimated in Example 4.1. The following code computes the 10-fold CV for the selected EDDA and MclustDA models:\n\ncv1 = cvMclustDA(mod1)\nstr(cv1)\n## List of 6\n## $ classification: Factor w/ 2 levels \"B\",\"M\": 2 1 1 1 1 2 1 1 1 1 ...\n## $ z : num [1:379, 1:2] 0.191 0.744 0.972 0.927 0.705 ...\n## ..- attr(*, \"dimnames\")=List of 2\n## .. ..$ : NULL\n## .. ..$ : chr [1:2] \"B\" \"M\"\n## $ ce : num 0.0528\n## $ se.ce : num 0.0125\n## $ brier : num 0.0399\n## $ se.brier : num 0.00734\ncv2 = cvMclustDA(mod2)\nstr(cv2)\n## List of 6\n## $ classification: Factor w/ 2 levels \"B\",\"M\": 2 1 1 1 2 2 1 1 1 2 ...\n## $ z : num [1:379, 1:2] 0.304 0.891 0.916 0.994 0.471 ...\n## ..- attr(*, \"dimnames\")=List of 2\n## .. ..$ : NULL\n## .. ..$ : chr [1:2] \"B\" \"M\"\n## $ ce : num 0.0317\n## $ se.ce : num 0.00765\n## $ brier : num 0.0298\n## $ se.brier : num 0.00462\n\nThe list of values returned by cvMclustDA() contains the cross-validated predicted classes (classification), the posterior class conditional probabilities (z), followed by the cross-validated metrics (the misclassification error rate and the Brier score) and their standard errors. The latter can be extracted using:\n\nunlist(cv1[c(\"ce\", \"se.ce\", \"brier\", \"se.brier\")])\n##        ce     se.ce     brier  se.brier \n## 0.0527704 0.0124978 0.0399285 0.0073402\nunlist(cv2[c(\"ce\", \"se.ce\", \"brier\", \"se.brier\")])\n##        ce     se.ce     brier  se.brier \n## 0.0316623 0.0076524 0.0297516 0.0046195\n\nTraining and resampling metrics for all of the EDDA models and the MclustDA model can be obtained using the following code:\n\nmodels = mclust.options(\"emModelNames\")\ntab_CE = tab_Brier = \n  matrix(as.double(NA), nrow = length(models)+1, ncol = 5)\nrownames(tab_CE) = rownames(tab_Brier) = \n  c(paste0(\"EDDA[\", models, \"]\"), \"MCLUSTDA\")\ncolnames(tab_CE) = colnames(tab_Brier) = \n  c(\"Train\", \"10-fold CV\", \"se(CV)\", \"lower\", \"upper\")\nfor (i in seq(models))\n{\n  mod = MclustDA(X, Class, modelType = \"EDDA\", \n                  modelNames = models[i], verbose = FALSE)\n  pred = predict(mod, X)\n  cv = cvMclustDA(mod, nfold = 10, verbose = FALSE)\n  #\n  tab_CE[i, 1] = classError(pred$classification, Class)$errorRate\n  tab_CE[i, 2] = cv$ce\n  tab_CE[i, 3] = cv$se.ce\n  tab_CE[i, 4] = cv$ce - cv$se.ce\n  tab_CE[i, 5] = cv$ce + cv$se.ce\n  #\n  tab_Brier[i, 1] = BrierScore(pred$z, Class)\n  tab_Brier[i, 2] = cv$brier\n  tab_Brier[i, 3] = cv$se.brier\n  tab_Brier[i, 4] = cv$brier - cv$se.brier\n  tab_Brier[i, 5] = cv$brier + cv$se.brier\n}\ni = length(models)+1\nmod = MclustDA(X, Class, modelType = \"MclustDA\", verbose = FALSE)\npred = predict(mod, X)\ncv = cvMclustDA(mod, nfold = 10, verbose = FALSE)\n#\ntab_CE[i, 1] = classError(pred$classification, Class)$errorRate\ntab_CE[i, 2] = cv$ce\ntab_CE[i, 3] = cv$se.ce\ntab_CE[i, 4] = cv$ce - cv$se.ce\ntab_CE[i, 5] = cv$ce + cv$se.ce\n#\ntab_Brier[i, 1] = BrierScore(pred$z, Class)\ntab_Brier[i, 2] = cv$brier\ntab_Brier[i, 3] = cv$se.brier\ntab_Brier[i, 4] = cv$brier - cv$se.brier\ntab_Brier[i, 5] = cv$brier + cv$se.brier\n\nThe following table gives the training error, the 10-fold CV error with its standard error, and the lower and upper bounds computed as \\(\\pm\\) one standard error from the CV estimate:\n\ntab_CE\n##              Train 10-fold CV    se(CV)    lower    upper\n## EDDA[EII] 0.112478   0.117750 0.0168034 0.100947 0.134554\n## EDDA[VII] 0.079086   0.079086 0.0120644 0.067022 0.091150\n## EDDA[EEI] 0.087873   0.087873 0.0089230 0.078950 0.096797\n## EDDA[VEI] 0.091388   0.093146 0.0128794 0.080266 0.106025\n## EDDA[EVI] 0.066784   0.072056 0.0083953 0.063661 0.080452\n## EDDA[VVI] 0.043937   0.047452 0.0064740 0.040978 0.053926\n## EDDA[EEE] 0.066784   0.068541 0.0076137 0.060928 0.076155\n## EDDA[VEE] 0.072056   0.073814 0.0159524 0.057861 0.089766\n## EDDA[EVE] 0.059754   0.065026 0.0073839 0.057642 0.072410\n## EDDA[VVE] 0.042179   0.043937 0.0088144 0.035122 0.052751\n## EDDA[EEV] 0.047452   0.050967 0.0075542 0.043412 0.058521\n## EDDA[VEV] 0.052724   0.056239 0.0108316 0.045407 0.067071\n## EDDA[EVV] 0.045694   0.047452 0.0074344 0.040017 0.054886\n## EDDA[VVV] 0.036907   0.038664 0.0090412 0.029623 0.047705\n## MCLUSTDA  0.022847   0.036907 0.0105586 0.026348 0.047465\n\nThe same information is also shown graphically in Figure 4.6 using:\n\nlibrary(\"ggplot2\")\ndf = data.frame(rownames(tab_CE), tab_CE)\ncolnames(df) = c(\"model\", \"train\", \"cv\", \"se\", \"lower\", \"upper\")\ndf$model = factor(df$model, levels = rev(df$model))\nggplot(df, aes(x = model, y = cv, ymin = lower, ymax = upper)) +\n  geom_point(aes(shape = \"s1\", color = \"c1\")) + \n  geom_errorbar(width = 0.5, col = \"dodgerblue3\") + \n  geom_point(aes(y = train, shape = \"s2\", color = \"c2\")) +\n  scale_y_continuous(breaks = seq(0, 0.2, by = 0.01), lim = c(0, NA)) +\n  scale_color_manual(name = \"\", \n                     breaks = c(\"c1\", \"c2\"),\n                     values = c(\"dodgerblue3\", \"black\"),\n                     labels = c(\"CV\", \"Train\")) +\n  scale_shape_manual(name = \"\", \n                     breaks = c(\"s1\", \"s2\"),\n                     values = c(19, 0),\n                     labels = c(\"CV\", \"Train\")) +\n  ylab(\"Classification error\") + xlab(\"\") + coord_flip() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nFigure 4.6: Training and cross-validated misclassification error rates of Gaussian mixture classification models for the breast cancer data.\n\n\n\n\nThe analogous table for the Brier score is:\n\ntab_Brier\n##              Train 10-fold CV    se(CV)    lower    upper\n## EDDA[EII] 0.095691   0.096282 0.0141399 0.082142 0.110422\n## EDDA[VII] 0.072572   0.072936 0.0099342 0.063001 0.082870\n## EDDA[EEI] 0.055086   0.055573 0.0055047 0.050068 0.061078\n## EDDA[VEI] 0.060386   0.062123 0.0094198 0.052703 0.071543\n## EDDA[EVI] 0.047277   0.049384 0.0067815 0.042603 0.056166\n## EDDA[VVI] 0.036197   0.037103 0.0044475 0.032655 0.041550\n## EDDA[EEE] 0.051470   0.052056 0.0040711 0.047984 0.056127\n## EDDA[VEE] 0.052813   0.054901 0.0085825 0.046318 0.063483\n## EDDA[EVE] 0.043315   0.046130 0.0053728 0.040757 0.051502\n## EDDA[VVE] 0.035243   0.035845 0.0065776 0.029268 0.042423\n## EDDA[EEV] 0.039127   0.040170 0.0040287 0.036141 0.044199\n## EDDA[VEV] 0.041625   0.043191 0.0074828 0.035708 0.050674\n## EDDA[EVV] 0.036139   0.037555 0.0050249 0.032530 0.042580\n## EDDA[VVV] 0.028803   0.030183 0.0063246 0.023859 0.036508\n## MCLUSTDA  0.022999   0.026530 0.0064555 0.020075 0.032986\n\nwith the corresponding plot in Figure 4.7:\n\ndf = data.frame(rownames(tab_Brier), tab_Brier)\ncolnames(df) = c(\"model\", \"train\", \"cv\", \"se\", \"lower\", \"upper\")\ndf$model = factor(df$model, levels = rev(df$model))\nggplot(df, aes(x = model, y = cv, ymin = lower, ymax = upper)) +\n  geom_point(aes(shape = \"s1\", color = \"c1\")) + \n  geom_errorbar(width = 0.5, col = \"dodgerblue3\") + \n  geom_point(aes(y = train, shape = \"s2\", color = \"c2\")) +\n  scale_y_continuous(breaks = seq(0, 0.2, by = 0.01), lim = c(0, NA)) +\n  scale_color_manual(name = \"\", \n                     breaks = c(\"c1\", \"c2\"),\n                     values = c(\"dodgerblue3\", \"black\"),\n                     labels = c(\"CV\", \"Train\")) +\n  scale_shape_manual(name = \"\", \n                     breaks = c(\"s1\", \"s2\"),\n                     values = c(19, 0),\n                     labels = c(\"CV\", \"Train\")) +\n  ylab(\"Brier score\") + xlab(\"\") + coord_flip() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nFigure 4.7: Training and cross-validated Brier scores of Gaussian mixture classification models for the breast cancer data.\n\n\n\n\nThe plots in Figure 4.6 and Figure 4.7 show that, by increasing the complexity of the model, it is sometimes possible to improve the accuracy of the predictions. In particular, the MclustDA model had better performance than the EDDA models, with the exception of the EDDA model with unconstrained covariances (VVV), which had both misclassification error rate and Brier score within one standard error from the best.\n\nThe information returned by cvMclustDA() can also be used for computing other cross-validation metrics. For instance, in the binary class case two popular measures are sensitivity and specificity.  The sensitivity, or true positive rate, is given by the ratio of the number observations that are classified in the positive class to the total number of positive cases. The specificity, or true negative rate, is given by the ratio of observations that are classified in the negative class to the total number of negative cases. Both metrics are easily computed from the confusion matrix, obtained by cross-tabulating the true classes and the classes predicted by a classifier.\n\nExample 4.3   ROC-AUC analysis of classification models for the Wisconsin diagnostic breast cancer data\nReturning to the Wisconsin diagnostic breast cancer data in Example 4.1, we are mainly interested in the identification of patients with malignant diagnosis, so the positive class can be taken to be class M, while benign cases (B) are assigned to the negative class.\n\n# confusion matrix\n(tab = table(Predict = cv1$classification, Class = Class_train))\n##        Class\n## Predict   B   M\n##       B 248  17\n##       M   3 111\ntab[2, 2]/sum(tab[, 2])  # sensitivity\n## [1] 0.86719\ntab[1, 1]/sum(tab[, 1])  # specificity\n## [1] 0.98805\n\nThe code above uses the cross-validated classifications obtained using the MAP approach, which for the binary class case is equivalent to setting the classification probability threshold at \\(0.5\\). However, the posterior probabilities returned by cvMclustDA() can be used to get classifications at different threshold values. The following code computes the sensitivity and specificity over a fine grid of threshold values:\n\nthreshold = seq(0, 1, by = 0.01)\nsensitivity = specificity = rep(NA, length(threshold))\nfor(i in 1:length(threshold))\n{\n  pred = factor(ifelse(cv1$z[, \"M\"] &gt; threshold[i], \"M\", \"B\"),\n                 levels = c(\"B\", \"M\"))\n  tab = table(pred, Class_train)\n  sensitivity[i] = tab[2, 2]/sum(tab[, 2])\n  specificity[i] = tab[1, 1]/sum(tab[, 1])\n}\n\nThe metrics computed above for varying thresholds in binary decisions can be represented graphically using the Receiver Operating Characteristic (ROC) curve,  which plots the sensitivity (true positive rate) vs. one minus the specificity (false positive rate). The resulting display is shown in Figure 4.8 (a) and obtained with the following code:\n\nplot(1-specificity, sensitivity, type = \"l\", lwd = 2)  # ROC curve\nabline(h = c(0, 1), v = c(0, 1), lty = 3)  # limits of [0,1]x[0,1] region\nabline(a = 0, b = 1, lty = 2)  # line of random classification\n\nNotice that the optimal ROC curve would pass through the upper left corner, which corresponds to both sensitivity and specificity equal to 1.\nA summary of the ROC curve which is used for evaluating the overall performance of a classifier is the Area Under the Curve (AUC).  This is equal to 1 for a perfect classifier, and 0.5 for a random classification; values larger than 0.8 are considered to be good (Lantz 2019, 333). Provided that the threshold grid is fine enough, a simple approximation of the AUC is obtained using the following function:\n\nauc_approx = function(tpr, fpr)\n{\n  x = 1 - fpr\n  y = tpr\n  dx = c(diff(x), 0)\n  dy = c(diff(y), 0)\n  sum(y * dx) + sum(dy * dx)/2\n}\nauc_approx(tpr = sensitivity, fpr = 1 - specificity)\n## [1] 0.98129\n\nThe value of AUC indicates a classifier with a very good classification performance.\nThe same ROC-AUC analysis can also be replicated for the selected MclustDA model (in object mod2), producing the ROC curve shown in Figure 4.8 (b). The corresponding value of the AUC is 0.98506.\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 4.8: ROC curves from the cross-validated predictions of selected (a) EDDA and (b) MclustDA models for the breast cancer data.\n\n\nFinally, the ROC curve can also be used to select an optimal threshold value. This can be set at the value where the true positive rate is high and the false positive rate is low, which is equivalent to maximizing Youden’s index:  \\[\nJ = \\text{Sensitivity} - (1 - \\text{Specificity}) = \\text{Sensitivity} + \\text{Specificity} - 1.\n\\] Note that \\(J\\) corresponds to the vertical distance between the ROC curve and the random classification line. The following code computes Youden’s index and the optimal threshold for the selected EDDA model:\n\nJ = sensitivity + specificity - 1 \nthreshold[which.max(J)]     # optimal threshold\n## [1] 0.28\nsensitivity[which.max(J)]   # sensitivity at optimal threshold\n## [1] 0.94531\nspecificity[which.max(J)]   # specificity at optimal threshold\n## [1] 0.96813\n\n\nROC curves are a suitable measure of performance when the distribution of the two classes is approximately equal. Otherwise, Precision-Recall (PR) curves are a better alternative. Both measures require a sufficient number of thresholds to obtain an accurate estimate of the corresponding area under the curve. A discussion of the relationship between ROC and PR curves can be found in Davis and Goadrich (2006). R implementations include CRAN package PRROC (Keilwagen, Grosse, and Grau 2014; Grau, Grosse, and Keilwagen 2015) and ROCR (Sing et al. 2005).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#sec-misclascosts",
    "href": "chapters/04_classification.html#sec-misclascosts",
    "title": "4  Mixture-Based Classification",
    "section": "\n4.5 Classification with Unequal Costs of Misclassification",
    "text": "4.5 Classification with Unequal Costs of Misclassification\nIn many practical applications, different costs are associated with different types of classification error. Thus it can be argued that the decision rule should be based on the principle that the total cost of misclassification should be minimized. Costs can be incorporated into a decision rule either at the learning stage or at the prediction stage.\nWe now describe a strategy for Gaussian mixtures that takes costs into account only at the final prediction stage. Let \\(c(k|j)\\) be the cost of allocating an observation from class \\(j\\) to class \\(k \\ne j\\), with \\(c(j|j) = 0\\). Let \\(p(k|j) = \\Pr(C_k | \\boldsymbol{x}\\in C_j)\\) be the probability of allocating an observation coming from class \\(j\\) to class \\(k\\). The expected cost of misclassification (ECM)  for class \\(j\\) is given by \\[\n\\ECM(j) = \\sum_{k \\ne j}^K c(k|j) p(k|j) \\propto \\sum_{k \\ne j}^K c(k|j) \\tau_k f(\\boldsymbol{x}| C_k),\n\\] and the overall ECM is thus \\[\n\\ECM = \\sum_{j=1}^K \\ECM(j) = \\sum_{j=1}^K \\sum_{k \\ne j}^K c(k|j) p(k|j).\n\\] According to this criterion, a new observation \\(\\boldsymbol{x}\\) should be allocated by minimizing the expected cost of misclassification.\nIn the case of equal costs of misclassification (\\(c(k|j) = 1\\) if \\(k \\ne j\\)), we obtain \\[\n\\ECM(j) = \\sum_{k \\ne j}^K p(k|j) \\propto \\sum_{k \\ne j}^K \\tau_k f(\\boldsymbol{x}| C_k),\n\\] and we allocate an observation \\(\\boldsymbol{x}\\) to the class \\(C_k\\) that minimizes \\(\\ECM(j)\\), or, equivalently, that maximizes \\(\\tau_k f(\\boldsymbol{x}| C_k)\\). This rule is the same as the standard MAP rule which uses the posterior probability from Equation 4.2.\nIn the case of unequal costs of misclassification, consider the \\(K \\times K\\) matrix of costs having the form: \\[\nC = \\{ c(k|j) \\} =\n\\begin{bmatrix}\n0 & c(2|1) & c(3|1) & \\dots & c(K|1) \\\\\nc(1|2) & 0 & c(3|2) & \\dots & c(K|2) \\\\\nc(1|3) & c(2|3) & 0 & \\dots & c(K|3) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\nc(1|K) & c(2|K) & c(3|K) & \\dots & 0 \\\\\n\\end{bmatrix}\n.\n\\] A simple solution can be obtained if we assume a constant cost of misclassifying an observation from class \\(j\\), irrespective of the class predicted. Following (Breiman et al. 1984, 112–15), we can compute the per-class cost \\(c(k|j) = c(j)\\) for \\(j \\neq k\\), and then get the predictions as in the unit-cost case with adjusted prior probabilities for the classes: \\[\n\\tau^*_j = \\frac{c(j)\\tau_j}{\\displaystyle\\sum_{j=1}^K c(j)\\tau_j}.\n\\] Note that for two-class problems the use of the per-class cost vector is equivalent to using the original cost matrix.\n\nExample 4.4   Bankruptcy prediction based on financial ratios of corporations\nConsider the data on financial ratios from Altman (1968), and available in the R package MixGHD (Tortora et al. 2022), which provides the ratio of retained earnings (RE) to total assets, and the ratio of earnings before interests and taxes (EBIT) to total assets, for 66 American corporations, of which half had filed for bankruptcy.\nThe following code loads the data and plots the financial ratios conditional on the class (see Figure 4.9):\n\ndata(\"bankruptcy\", package = \"MixGHD\")\nX = bankruptcy[, -1]\nClass = factor(bankruptcy$Y, levels = c(1:0), \n               labels = c(\"solvent\", \"bankrupt\"))\ncl = clPairs(X, Class)\nlegend(\"bottomright\", legend = cl$class, \n       pch = cl$pch, col = cl $col, inset = 0.02)\n\n\n\n\n\n\nFigure 4.9: Scatterplot of financial ratios with points distinguished by observed classes.\n\n\n\n\nAlthough the within-class distribution is clearly not Gaussian, in particular for the companies that have declared bankruptcy, we fit an EDDA classification model:\n\nmod = MclustDA(X, Class, modelType = \"EDDA\")\nsummary(mod)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## EDDA model summary: \n## \n##  log-likelihood  n df     BIC\n##         -661.27 66  9 -1360.3\n##           \n## Classes     n  % Model G\n##   solvent  33 50   VEE 1\n##   bankrupt 33 50   VEE 1\n## \n## Training confusion matrix:\n##           Predicted\n## Class      solvent bankrupt\n##   solvent       33        0\n##   bankrupt       2       31\n## Classification error = 0.0303 \n## Brier score          = 0.0295\n\nThe confusion matrix indicates that two training data points are misclassified. Both are bankrupt firms which have been classified as solvent. The following plots show the distribution of financial ratios with (a) Gaussian ellipses implied by the estimated model, and (b) black points corresponding to the misclassified observations (see Figure 4.10):\nplot(mod, what = \"scatterplot\")\nplot(mod, what = \"error\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 4.10: Scatterplots of financial ratios with points distinguished by observed classes. Panel (a) shows the ellipses implied by the estimated model. Panel (b) includes black points corresponding to the misclassified observations.\n\n\nNow consider the error of misclassifying a bankrupt firm as solvent to be more serious than the opposite. We quantify these different costs of misclassification in a matrix \\(C\\)\n\n(C = matrix(c(0, 1, 10, 0), nrow = 2, ncol = 2, byrow = TRUE))\n##      [,1] [,2]\n## [1,]    0    1\n## [2,]   10    0\n\nand obtain the per-class cost vector:\n\nrowSums(C)\n## [1]  1 10\n\nThe total cost of misclassification for the MAP predictions is\n\npred = predict(mod)\n(tab = table(Class, Predicted = pred$classification))\n##           Predicted\n## Class      solvent bankrupt\n##   solvent       33        0\n##   bankrupt       2       31\nsum(tab * C)\n## [1] 20\n\nUnequal costs of misclassification can be included in the prediction as follows:\n\npred = predict(mod, prop = mod$prop*rowSums(C))\n(tab = table(Class, Predicted = pred$classification))\n##           Predicted\n## Class      solvent bankrupt\n##   solvent       28        5\n##   bankrupt       0       33\nsum(tab * C)\n## [1] 5\n\nThe last command shows that we have been able to reduce the total cost by zeroing out the errors for bankrupt firms, at the same time increasing the errors for solvent corporations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#sec-imbalclass",
    "href": "chapters/04_classification.html#sec-imbalclass",
    "title": "4  Mixture-Based Classification",
    "section": "\n4.6 Classification with Unbalanced Classes",
    "text": "4.6 Classification with Unbalanced Classes\nMost classification datasets are not balanced;  that is, classes have unequal numbers of instances. Small differences between the number of instances in different classes can usually be ignored. In some cases, however, the imbalance in class proportions can be dramatic, and the class of interest is sometimes the class with the smallest number of cases. For instance, in studies aimed at identifying fraudulent transactions, classes are typically unbalanced, with the vast majority of the transactions not being fraudulent. In medical studies aimed at characterizing rare diseases, the class of individuals with the disease is only a small fraction of the total population (the prevalence).\nIn such situations, a case-control sampling scheme can be adopted by sampling approximately 505 of the data from the cases (fraudulent transactions, individuals suffering from a disease) and 50% from the controls. Balanced datasets can also be obtained in observational studies by undersampling, or downsizing the majority class by removing observations at random until the dataset is balanced. In both cases the class prior probabilities estimated from the training set do not reflect the “true” a priori probabilities. As a result, the predicted posterior class probabilities are not well estimated, resulting in a loss of classification accuracy compared to a classifier based on the true prior probabilities for the classes.\n\nExample 4.5   Classification of synthetic unbalanced two-class data\nAs an example, consider a simulated binary classification task, with the majority class having distribution \\(x | (y = 0) \\sim N(0, 1)\\), whereas the distribution of the minority class is \\(x | (y = 1) \\sim N(3, 1)\\), and in the population the latter accounts for 10% of cases. Suppose that a training sample is obtained using case-control sampling, so that the two groups have about the same proportion of cases.\nA synthetic dataset from this specification can be simulated with the following code and shown graphically in Figure 4.11:\n# generate training data from a balanced case-control sample\nn_train = 1000\nclass_train = factor(sample(0:1, size = n_train, prob = c(0.5, 0.5), \n                             replace = TRUE))\nx_train = ifelse(class_train == 1, rnorm(n_train, mean = 3, sd = 1), \n                                    rnorm(n_train, mean = 0, sd = 1))\n\nhist(x_train[class_train == 0], breaks = 11, xlim = range(x_train), \n     main = \"\", xlab = \"x\", \n     col = adjustcolor(\"dodgerblue2\", alpha.f = 0.5), border = \"white\")\nhist(x_train[class_train == 1], breaks = 11, add = TRUE,\n     col = adjustcolor(\"red3\", alpha.f = 0.5), border = \"white\")\nbox()\n\n# generate test data from mixture f(x) = 0.9 * N(0,1) + 0.1 * N(3,1)\nn = 10000\nmixpro = c(0.9, 0.1)\nclass_test = factor(sample(0:1, size = n, prob = mixpro, \n                            replace = TRUE))\nx_test = ifelse(class_test == 1, rnorm(n, mean = 3, sd = 1), \n                                  rnorm(n, mean = 0, sd = 1))\nhist(x_test[class_test == 0], breaks = 15, xlim = range(x_test), \n     main = \"\", xlab = \"x\", \n     col = adjustcolor(\"dodgerblue2\", alpha.f = 0.5), border = \"white\")\nhist(x_test[class_test == 1], breaks = 11, add = TRUE,\n     col = adjustcolor(\"red3\", alpha.f = 0.5), border = \"white\")\nbox()\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 4.11: Histograms for synthetic datasets with observations sampled from two different Gaussian distributions. In the training data set, the cases (\\(y=1\\)) and the controls (\\(y=0\\)) are sampled in about the same proportions (a), whereas the cases (\\(y=1\\)) account for 10% of the observations in the whole population (b).\n\n\nUsing the training sample we can estimate a classification Gaussian mixture model:\n\nmod = MclustDA(x_train, class_train)\nsummary(mod, parameters = TRUE)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## MclustDA model summary: \n## \n##  log-likelihood    n df     BIC\n##         -1947.1 1000  5 -3928.8\n##        \n## Classes   n    % Model G\n##       0 505 50.5     X 1\n##       1 495 49.5     X 1\n## \n## Class prior probabilities:\n##     0     1 \n## 0.505 0.495 \n## \n## Class = 0\n## \n## Mixing probabilities: 1 \n## \n## Means:\n## [1] 0.043067\n## \n## Variances:\n## [1] 0.93808\n## \n## Class = 1\n## \n## Mixing probabilities: 1 \n## \n## Means:\n## [1] 3.0959\n## \n## Variances:\n## [1] 0.96652\n## \n## Training confusion matrix:\n##      Predicted\n## Class   0   1\n##     0 479  26\n##     1  24 471\n## Classification error = 0.05 \n## Brier score          = 0.0402\n\nThe estimated parameters for the class conditional distributions are close to the true values, but the prior class probabilities are highly biased due to the sampling scheme adopted. Performance measures can be computed for the test set:\n\npred = predict(mod, newdata = x_test)\nclassError(pred$classification, class_test)$error\n## [1] 0.0592\nBrierScore(pred$z, class_test)\n## [1] 0.045927\n\nshowing that they are slightly worse than those for the training set, as is often the case.\nFor such simulated data we know the true classes, so we can compute the performance measures over a grid of values of the prior probability of the minority class:\n\npriorProp = seq(0.01, 0.99, by = 0.01)\nCE = BS = rep(as.double(NA), length(priorProp))\nfor (i in seq(priorProp))\n{\n  pred = predict(mod, newdata = x_test, \n                  prop = c(1-priorProp[i], priorProp[i]))\n  CE[i] = classError(pred$classification, class = class_test)$error\n  BS[i] = BrierScore(pred$z, class_test)\n}\n\nThe following code produces Figure 4.12, which shows the classification error and the Brier score as functions of the prior probability for the minority class.\n\nmatplot(priorProp, cbind(CE, BS), type = \"l\", lty = 1, lwd = 2, xaxt = \"n\",\n        xlab = \"Class prior probability\", ylab = \"\", ylim = c(0, max(CE, BS)), \n        col = c(\"red3\", \"dodgerblue3\"),\n        panel.first = \n          { abline(h = seq(0, 1, by = 0.05), col = \"grey\", lty = 3)\n            abline(v = seq(0, 1, by = 0.05), col = \"grey\", lty = 3) \n          })\naxis(side = 1, at = seq(0, 1, by = 0.1))\nabline(v = mod$prop[2],             # training proportions\n       lty = 2, lwd = 2)            \nabline(v = mean(class_test == 1),   # test proportions (usually unknown)\n       lty = 3, lwd = 2)   \nlegend(\"topleft\", legend = c(\"ClassError\", \"BrierScore\"),\n       col = c(\"red3\", \"dodgerblue3\"), lty = 1, lwd = 2, inset = 0.02)\n\n\n\n\n\n\nFigure 4.12: Classification error and Brier score as functions of the prior probability for the minority class. The vertical segments show the biased sample proportion of cases in the training set (dashed line), and the sample proportion of cases in the test set (dotted line), which is usually unknown.\n\n\n\n\nVertical lines are drawn at the proportion for the minority class in the training set (dashed line), and at the proportion computed on the test set (dotted lines). However, the latter is usually unknown, but the plot clearly shows that there is room for improving the classification accuracy by using an unbiased estimate of the class prior probabilities.\n\nTo solve this problem, Saerens, Latinne, and Decaestecker (2002) proposed an EM algorithm that aims at estimating the adjusted posterior conditional probabilities of a classifier,  and, as a by-product, provides estimates of the prior class probabilities. This method can be easily adapted to classifiers based on Gaussian mixtures.\nSuppose a Gaussian mixture of the type in Equation 4.1 is estimated on the training set \\(\\mathcal{D}_\\text{train}= \\{(\\boldsymbol{x}_1, y_1), \\dots, (\\boldsymbol{x}_n, y_n)\\}\\), and a new test set \\(\\mathcal{D}_\\text{test}= \\{\\boldsymbol{x}^*_1, \\dots, \\boldsymbol{x}^*_m\\}\\) is available. From Equation 4.2 the posterior probabilities that an observation \\(\\boldsymbol{x}_i^{*}\\) belongs to class \\(C_k\\) are \\[\n\\widehat{z}_{ik}^{*} = \\widehat{\\Pr}(C_k | \\boldsymbol{x}_i^{*})\n\\quad\\text{for } k=1,\\dots,K.\n\\] Let \\(\\tilde{\\tau}_k = \\sum_{i = 1}^n \\mathnormal{I}(y_i = C_k)/n\\) be the proportion of cases from class \\(k\\) in the training set, and \\(\\widehat{\\tau}_k^{0} = \\sum_{i=1}^m \\widehat{z}^{*}_{ik}/m\\) be a preliminary estimate of the prior probabilities for class \\(k\\) (\\(k=1,\\dots,K\\)). Starting with \\(s=1\\), the algorithm iterates the following steps: \\[\\begin{align*}\n\\widehat{z}_{ik}^{(s)} & =\n\\frac{ \\dfrac{\\widehat{\\tau}_k^{(s-1)}}{\\tilde{\\tau}_k} \\widehat{z}_{ik}^{*} }\n     { \\displaystyle\\sum_{g=1}^K \\dfrac{\\widehat{\\tau}_g^{(s-1)}}{\\tilde{\\tau}_k} \\widehat{z}_{ig}^{*} } ~ , &\n\\widehat{\\tau}_k^{(s)} & = \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^m \\widehat{z}_{ik}^{(s)},\n\\end{align*}\\] until the estimates \\((\\widehat{\\tau}_1,\\dots,\\widehat{\\tau}_K)\\) stabilize.\nIn mclust the estimated class prior probabilities, computed following the algorithm outlined above, can be obtained using the function classPriorProbs(). \n\nExample 4.6   Adjusting prior probabilities in unbalanced synthetic two-class data    Continuing the analysis in Example 4.5, the class prior probabilities are obtained as follows:\n\n(priorProbs = classPriorProbs(mod, x_test))\n##       0       1 \n## 0.89452 0.10548\n\nwhich provides estimates that are close to the true parameters. These can then be used to adjust the predictions to get:\n\npred = predict(mod, newdata = x_test, prop = priorProbs)\nclassError(pred$classification, class = class_test)$error\n## [1] 0.035\nBrierScore(pred$z, class_test)\n## [1] 0.025576\n\nThe performance measures can be contrasted with those obtained from the (usually unknown) class proportions in the test set:\n\n(prior_test = prop.table(table(class_test)))\n## class_test\n##     0     1 \n## 0.896 0.104\npred = predict(mod, newdata = x_test, prop = prior_test)\nclassError(pred$classification, class = class_test)$error\n## [1] 0.0351\nBrierScore(pred$z, class_test)\n## [1] 0.025568",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#sec-classonedim",
    "href": "chapters/04_classification.html#sec-classonedim",
    "title": "4  Mixture-Based Classification",
    "section": "\n4.7 Classification of Univariate Data",
    "text": "4.7 Classification of Univariate Data\nThe classification of univariate data follows the same principles as discussed in previous sections, with the caveat that only two possible models are available in the EDDA case, namely E for equal within-class variance, and V for varying variances across classes. The same constraints on variances also apply to each mixture component within-class in MclustDA.\n\nExample 4.7   Classification of single-index linear combination for the Wisconsin diagnostic breast cancer data\nTo increase the accuracy of breast cancer diagnosis and prognosis, Mangasarian, Street, and Wolberg (1995) used linear programming to identify the linear combination of features that optimally discriminate the benign from the malignant tumor cases. The linear combination was estimated to be \\[\n0.2322\\;\\mathtt{Texture\\_mean} + 0.01117\\;\\mathtt{Area\\_extreme} + 68.37\\;\\mathtt{Smoothness\\_extreme}\n\\] With this extracted feature the authors reported being able to achieve a cross-validated predictive accuracy of 97.5% (which is equivalent to 0.025 misclassification error rate).\nTo illustrate the use of Gaussian mixtures to classify univariate data, we fit an MclustDA model to the same one-dimensional projection of the UCI wdbc data:\n\ndata(\"wdbc\", package = \"mclust\")\nx = with(wdbc, \n    0.2322*Texture_mean + 0.01117*Area_extreme + 68.37*Smoothness_extreme)\nClass = wdbc[, \"Diagnosis\"]\nmod = MclustDA(x, Class, modelType = \"MclustDA\")\nsummary(mod)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## MclustDA model summary: \n## \n##  log-likelihood   n df   BIC\n##         -1763.1 569 11 -3596\n##        \n## Classes   n     % Model G\n##       B 357 62.74     X 1\n##       M 212 37.26     V 3\n## \n## Training confusion matrix:\n##      Predicted\n## Class   B   M\n##     B 351   6\n##     M  14 198\n## Classification error = 0.0351 \n## Brier score          = 0.0226\n\nThe selected model uses a single Gaussian component for the benign cancer cases and a three-component heterogeneous mixture for the malignant cases. The estimated within-class densities are shown in Figure 4.13, indicating a bimodal distribution for the malignant tumors. This plot essentially agrees with previous findings reported in Mangasarian, Street, and Wolberg (1995, fig. 3) using non-parametric density estimation. The following code can be used to obtain Figure 4.13:\n\n(prop = mod$prop)\n##       B       M \n## 0.62742 0.37258\ncol = mclust.options(\"classPlotColors\")\nx0 = seq(0, max(x)*1.1, length = 1000)\npar1 = mod$models[[\"B\"]]$parameters\nf1 = dens(par1$variance$modelName, data = x0, parameters = par1)\npar2 = mod$models[[\"M\"]]$parameters\nf2 = dens(par2$variance$modelName, data = x0, parameters = par2)\nmatplot(x0, cbind(prop[1]*f1, prop[2]*f2), type = \"l\", lty = 1, \n        col = col, ylab = \"Class density\", xlab = \"x\")\nlegend(\"topright\", title = \"Diagnosis:\", legend = names(prop), \n       col = col, lty = 1, inset = 0.02)\n\n\n\n\n\n\nFigure 4.13: Densities for the benign and malignant tumors estimated using the univariate feature extracted from the breast cancer dataset.\n\n\n\n\nThe cross-validated misclassification error for the estimated MclustDA model is obtained as follows:\n\ncv = cvMclustDA(mod)  # by default: prop = mod$prop\nunlist(cv[c(\"ce\", \"se.ce\")])\n##        ce     se.ce \n## 0.0351494 0.0075474\n\nAdjusting the class prior probabilities, as suggested by Mangasarian, Street, and Wolberg (1995), we get:\n\ncv = cvMclustDA(mod, prop = c(0.5, 0.5))\nunlist(cv[c(\"ce\", \"se.ce\")])\n##        ce     se.ce \n## 0.0263620 0.0071121\n\nThus, assuming an equal class prior probability, the CV error rate goes down from 3.5% to 2.6%. It is possible to derive the corresponding discriminant thresholds numerically as:\n\nx0 = seq(min(x), max(x), length.out = 1000)\npred = predict(mod, newdata = x0) \n(threshold1 = approx(pred$z[, 2], x0, xout = 0.5)$y)\n## [1] 23.166\npred = predict(mod, newdata = x0, prop = c(0.5, 0.5))\n(threshold2 = approx(pred$z[, 2], x0, xout = 0.5)$y)\n## [1] 22.851\n\nNote that the first threshold is equivalent to the point where the two densities in Figure 4.13 intersect. Using the default discriminant threshold, we can represent the training data graphically with the observed and predicted classes:\nplot(mod, what = \"scatterplot\", main = TRUE)\nabline(v = threshold1, lty = 2)\nplot(mod, what = \"classification\", main = TRUE)\nabline(v = threshold1, lty = 2)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 4.14: Distribution of the training data conditional on the true classes and on the predicted classes for the univariate feature extracted from the breast cancer data.\n\n\nIn the search for the optimal classification rule by cross-validation, we can proceed by tuning either the probability threshold for the decision rule or the prior class probabilities. The following code computes and plots the misclassification error rate as a function of the probability threshold used for the classification (see Figure 4.15):\n\nthreshold = seq(0.1, 0.9, by = 0.05)\nngrid = length(threshold)\ncv = data.frame(threshold, error = numeric(ngrid))\ncverr = cvMclustDA(mod, verbose = FALSE)\nfor (i in seq(threshold))\n{\n  cv$error[i] = classError(ifelse(cverr$z[, 2] &gt; threshold[i], \"M\", \"B\"),\n                            Class)$errorRate\n}  \nmin(cv$error)\n## [1] 0.024605\nthreshold[which.min(cv$error)]\n## [1] 0.35\n\nggplot(cv, aes(x = threshold, y = error)) +\n  geom_point() + geom_line() +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  ylab(\"CV misclassification error\") + \n  xlab(\"Probability threshold of malignant (M) tumor class\")\n\n\n\n\n\n\nFigure 4.15: The cross-validated misclassification error rate as a function of the probability threshold for the univariate feature extracted from the breast cancer data.\n\n\n\n\nIt is thus possible to get a misclassification error of 2.46% by setting the probability threshold to 0.35.\nA similar result can also be achieved by adjusting the class prior probabilities. However, because the CV procedure must be replicated for each value specified over a regular grid, this is more computationally demanding. Nevertheless, the added computational effort has the advantage of providing an estimate of the standard error of the CV error estimate itself. This allows us to compute intervals around the CV estimate for assessing the significance of observed differences. The following code computes the CV misclassification error by adjusting the class prior probability of being a malignant tumor, followed by the code for plotting the CV results:\n\npriorProb = seq(0.1, 0.9, by = 0.05)\nngrid = length(priorProb)\ncv_error2 = data.frame(priorProb, \n                       cv = numeric(ngrid), \n                       lower = numeric(ngrid), \n                       upper = numeric(ngrid))\nfor (i in seq(priorProb))\n{\n  cv = cvMclustDA(mod, prop = c(1-priorProb[i], priorProb[i]), \n                  verbose = FALSE)\n  cv_error2$cv[i]    = cv$ce\n  cv_error2$lower[i] = cv$ce - cv$se.ce\n  cv_error2$upper[i] = cv$ce + cv$se.ce\n}  \nmin(cv_error2$cv)\n## [1] 0.026362\npriorProb[which.min(cv_error2$cv)]\n## [1] 0.5\n\nggplot(cv_error2, aes(x = priorProb, y = cv)) +\n  geom_point() + \n  geom_linerange(aes(ymin = lower, ymax = upper)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  ylab(\"CV misclassification error\") + \n  xlab(\"Malignant (M) tumor class prior probability\")\n\n\n\n\n\n\nFigure 4.16: Plot of the CV misclassification error rate as a function of the class prior probability, with error bars shown at \\(\\pm\\) one standard error of the CV procedure, for the univariate feature extracted from the breast cancer data.\n\n\n\n\nBy setting the class prior probabilities at 0.5 each, we get a misclassification error of 2.64%, which is similar to the result obtained by tuning the threshold. The strategy of assuming an equal prior class probability for each breast cancer class adopted by Mangasarian, Street, and Wolberg (1995) appears to be the best approach available.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#sec-ssc",
    "href": "chapters/04_classification.html#sec-ssc",
    "title": "4  Mixture-Based Classification",
    "section": "\n4.8 Semi-Supervised Classification",
    "text": "4.8 Semi-Supervised Classification\nSupervised learning methods require knowing the correct class labels for the training data. However, in certain situations the available labeled data may be scarce because they are difficult or expensive to collect, for instance, in anomaly detection, computer-aided diagnosis, drug discovery, and speech recognition. Semi-supervised learning  refers to models and algorithms that use both labeled and unlabeled data to perform certain learning tasks [Zhu and Goldberg (2009); VanEngelenHoos:2020]. In classification scenarios, the main goal of semi-supervised learning is to train a classifier using both the labeled and unlabeled data such that its classification performance is better than the one obtained using a classifier trained on the labeled data alone.\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 4.17: Example of two-class simulated dataset with (a) all labeled data (points are marked according to the true classes) and (b) only partial knowledge of labeled data (unlabeled data are shown as grey squares).\n\n\nFigure 4.17 shows a simulated two-class dataset with all the observations labeled (see left panel), and with only a few cases labeled (see right panel). In the typical supervised classification setting we would have access to the dataset in Figure 4.17 (a) where labels are available for all the observations. When most of the labels are unknown, as in Figure 4.17 (b), two possible approaches are available. We could use only the (few) labeled cases to estimate a classifier, or we could use both labeled and unlabeled data in a semi-supervised learning approach.\nFigure 4.18 shows the classification boundaries corresponding to a supervised classification model estimated under the assumption that the true classes are known (solid line), the boundary for the classification model trained on the labeled data alone (dotted line), and the boundary obtained from a semi-supervised classification model (dashed line). The latter coincides almost exactly with the boundary arising from the full knowledge of class labels. In contrast, the classification boundary from the model estimated using only the labeled data is quite different.\n\n\n\n\n\n\n\nFigure 4.18: Classification boundaries for the two-class simulated dataset obtained (i) under the assumption of full knowledge of class labels (cyan solid line), (ii) using only the labeled data (black dashed line), and (iii) both labeled and unlabeled data (black dotted line).\n\n\n\n\nConsider a training dataset \\(\\mathcal{D}_\\text{train}\\) made of both \\(l\\) labeled cases \\(\\{ (\\boldsymbol{x}_i, y_i) \\}_{i=1}^l\\) and \\(u\\) unlabeled cases \\(\\{ \\boldsymbol{x}_j \\}_{j=l+1}^{n}\\), where \\(n = l + u\\) is the overall sample size. As mentioned earlier, there are usually many more unlabeled than labeled data, so we assume that \\(u \\gg l\\). The likelihood of a semi-supervised mixture model  depends on both the labeled and the unlabeled data, so the observed log-likelihood is \\[\n\\ell(\\boldsymbol{\\Psi}) = \\sum_{i=1}^l \\sum_{k=1}^K c_{ik} \\log\\left\\{ \\pi_k f_k(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_k) \\right\\} +\n              \\sum_{j=l+1}^{n} \\log\\left\\{ \\sum_{g=1}^G \\pi_g f_g(\\boldsymbol{x}_j ; \\boldsymbol{\\theta}_g) \\right\\},\n\\] where \\(c_{ik} = 1\\) if observation \\(i\\) belongs to class \\(k\\) and \\(0\\) otherwise. Although in principle the number of components \\(G\\) can be greater than the number of classes \\(K\\), it is usually assumed that \\(G = K\\).\nIf we treat the unknown labels as missing data, the complete-data log-likelihood  can be written as \\[\\begin{align*}\n\\ell_C(\\boldsymbol{\\Psi}) = & \\sum_{i=1}^l \\sum_{k=1}^K c_{ik} \\left\\{ \\log(\\pi_k) + \\log(f_k(\\boldsymbol{x}_i ; \\boldsymbol{\\theta}_k)) \\right\\} + \\\\\n                & \\sum_{j=l+1}^{n} \\sum_{g=1}^G z_{jg} \\left\\{ \\log(\\pi_g) + \\log(f_g(\\boldsymbol{x}_j ; \\boldsymbol{\\theta}_g)) \\right\\},\n\\end{align*}\\] {#eq-ssmixcloglik} where \\(z_{jg} \\in (0,1)\\) is the conditional probability of observation \\(j\\) to belong to class \\(g\\). By assuming a specific parametric distribution for the class and component densities, such as the multivariate Gaussian distribution, the EM algorithm can be used to find maximum likelihood estimates for the unknown parameters of the model. The estimated parameters are then used to classify the unlabeled data, as well as future data. More details can be found in G. J. McLachlan (1977), O’Neill (1978), #McLachlan:Peel:2000 [section 2.19] and Dean, Murphy, and Downey (2006).\nmclust provides an implementation for fitting Gaussian semi-supervised classification models through the function MclustSSC().  This requires the input of a data matrix and a vector of class labels, %in which NA with unlabeled data encoded by NA. By default all the available models are fitted and the one with the largest BIC is returned. Optionally, the covariance decompositions described in Section 2.2.1 can be specified via the argument modelNames using the usual nomenclature from Table 2.1. In addition, the optional argument G can be used to specify the number of mixture components. By default, this is set equal to the number of classes from the labeled data.\n\nExample 4.8   Semi-supervised learning of Italian olive oils data\nConsider the Italian olive oils dataset available in the pgmm R package (McNicholas et al. 2022). The data are from a study conducted to determine the authenticity of olive oil (Forina et al. 1983) and provide the percentage composition of eight fatty acids found by lipid fraction of 572 Italian olive oils. The olive oils came from nine areas of Italy, which can be further grouped into three regions: Southern Italy, Sardinia, and Northern Italy. The following code reads the data, sets the data frame X of features to build the classifier, and creates the factor class containing the labels for all the observations:\n\ndata(\"olive\", package = \"pgmm\")\nX = olive[, 3:10]\nclass = factor(olive$Region, levels = 1:3, \n                labels = c(\"South\", \"Sardinia\", \"North\"))\ntable(class)\n## class\n##    South Sardinia    North \n##      323       98      151\n\nKnowing all the class labels, we can easily fit a discriminant analysis model using the EDDA mixture model discussed in Section 4.2:\n\nmod_EDDA_full = MclustDA(X, class, modelType = \"EDDA\")\npred_EDDA_full = predict(mod_EDDA_full, newdata = X)\nclassError(pred_EDDA_full$classification, class)$errorRate\n## [1] 0\nBrierScore(pred_EDDA_full$z, class)\n## [1] 0.00000010836\n\nThis model has very good performance, as measured by the classification error and the Brier score, although we must remember that this is an optimistic assessment of model performance because both metrics are computed on the data used for training. Despite this, we can use this performance as a benchmark.\nSuppose that only partial knowledge of class labels is available. For instance, we can randomly retain only 10% of the labels as follows:\n\npct_labeled_data = 10\nn = nrow(X)\ncl = class\nis.na(cl) = sample(1:n, round(n*(1-pct_labeled_data/100)))\ntable(cl, useNA = \"ifany\")\n## cl\n##    South Sardinia    North     &lt;NA&gt; \n##       28       11       18      515\n\nSince approximately 90% of the data are unlabeled, a semi-supervised classification is called for:\n\nmod_SSC = MclustSSC(X, cl)\nplot(mod_SSC, what = \"BIC\")\nmod_SSC$BIC\n##      EII    VII    EEI    VEI    EVI    VVI    EEE    VEE    EVE    VVE\n## 3 -55641 -55449 -48321 -47813 -47057 -47073 -44348 -44140 -43640 -43323\n##      EEV    VEV    EVV    VVV\n## 3 -43829 -43539 -43191 -42768\npickBIC(mod_SSC$BIC, 5) - max(mod_SSC$BIC)  # BIC diff for the top-5 models\n##   VVV,3   EVV,3   VVE,3   VEV,3   EVE,3 \n##    0.00 -422.20 -554.53 -770.71 -871.55\n\n\n\n\n\n\nFigure 4.19: BIC values for the semi-supervised classification models fitted to the Italian olive oils data using 10% of labeled data.\n\n\n\n\nFigure 4.19 plots the BIC values for the models fitted by MclustSSC() with \\(G= K = 3\\) classes. The best estimated model according to BIC is the unconstrained VVV model. The summary()  function can be used to obtain a summary of the fit:\n\nsummary(mod_SSC)\n## ---------------------------------------------------------------- \n## Gaussian finite mixture model for semi-supervised classification \n## ---------------------------------------------------------------- \n## \n##  log-likelihood   n  df    BIC\n##          -20959 572 134 -42768\n##           \n## Classes      n     % Model G\n##   South     28  4.90   VVV 1\n##   Sardinia  11  1.92   VVV 1\n##   North     18  3.15   VVV 1\n##   &lt;NA&gt;     515 90.03        \n## \n## Classification summary:\n##           Predicted\n## Class      South Sardinia North\n##   South       28        0     0\n##   Sardinia     0       11     0\n##   North        0        0    18\n##   &lt;NA&gt;       295       86   134\n\nWe can evaluate the estimated classifier by comparing the predicted classes with the true classes for the unlabeled observations:\n\npred_SSC = predict(mod_SSC, newdata = X[is.na(cl), ])\ntable(Predicted = pred_SSC$classification, Class = class[is.na(cl)])\n##           Class\n## Predicted  South Sardinia North\n##   South      295        0     0\n##   Sardinia     0       86     0\n##   North        0        1   133\nclassError(pred_SSC$classification, class[is.na(cl)])$errorRate\n## [1] 0.0019417\nBrierScore(pred_SSC$z, class[is.na(cl)])\n## [1] 0.0019417\n\nThe performance appears quite good with only one unlabeled observation misclassified.\nWe now compare the semi-supervised approach with the classification approach that uses only the labeled data, as a function of the percentage of full data information. The following code implements this comparison using the Brier score as performance metric:\n\npct_labeled_data = c(5, seq(10, 90, by = 10), 95)\nBS = matrix(as.double(NA), nrow = length(pct_labeled_data), ncol = 2,\n             dimnames = list(pct_labeled_data, c(\"EDDA\", \"SSC\")))\nfor (i in seq(pct_labeled_data))\n{\n  cl = class\n  labeled = sample(1:n, round(n*pct_labeled_data[i]/100))\n  cl[-labeled] = NA\n  # Classification on labeled data\n  mod_EDDA  = MclustDA(X[labeled, ], cl[labeled], \n                        modelType = \"EDDA\")\n  # prediction for the unlabeled data\n  pred_EDDA = predict(mod_EDDA, newdata = X[-labeled, ])\n  BS[i, 1]  = BrierScore(pred_EDDA$z, class[-labeled])\n  # Semi-supervised classification\n  mod_SSC  = MclustSSC(X, cl)\n  # prediction for the unlabeled data\n  pred_SSC = predict(mod_SSC, newdata = X[-labeled, ])\n  BS[i, 2] = BrierScore(pred_SSC$z, class[-labeled])\n}\nBS\n##                   EDDA                 SSC\n## 5  0.36757342343703858 0.00184075724060626\n## 10 0.14649511848830127 0.00194083725139750\n## 20 0.00000004794698661 0.00218238248453443\n## 30 0.00500200204812944 0.00000000044264099\n## 40 0.00356335895644648 0.00000009331253357\n## 50 0.01740046202639491 0.00349653566200317\n## 60 0.00000033388757151 0.00000014123579678\n## 70 0.00585344025094451 0.00000037530321144\n## 80 0.00002069320555273 0.00000027964277368\n## 90 0.00000000012729332 0.00000000000499843\n## 95 0.00000000000069483 0.00000000000052263\n\nmatplot(pct_labeled_data, BS, type = \"b\", \n        lty = 1, pch = c(19, 15), col = c(2, 4), xaxt = \"n\",\n        xlab = \"Percentage of labeled data\", ylab = \"Brier score\")\naxis(side = 1, at = pct_labeled_data)\nabline(h = BrierScore(pred_EDDA_full$z, class), lty = 2)\nlegend(\"topright\", pch = c(19, 15), col = c(2, 4), lty = 1, \n       legend = c(\"EDDA\", \"SSC\"), inset = 0.02)\n\n\n\n\n\n\nFigure 4.20: Brier score values for the EDDA classification model on labeled data and the semi-supervised classification (SSC) model as a function of the percentage of labeled data.\n\n\n\n\nLooking at the table of results and Figure 4.20, it is clear that EDDA requires a large percentage of labeled data to achieve a reasonable performance. In contrast, the semi-supervised classification (SSC) model is able to achieve almost perfect classification accuracy with a very small portion of labeled data.\n\n\n\n\n\nAlpaydin, Ethem. 2014. Introduction to Machine Learning. 3rd ed. MIT Press.\n\n\nAltman, Edward I. 1968. “Financial Ratios, Discriminant Analysis and the Prediction of Corporate Bankruptcy.” The Journal of Finance 23 (4): 589–609.\n\n\nBates, Stephen, Trevor Hastie, and Robert Tibshirani. 2021. “Cross-Validation: What Does It Estimate and How Well Does It Do It?” arXiv Preprint. https://arxiv.org/abs/2104.00673.\n\n\nBensmail, H., and G. Celeux. 1996. “Regularized Gaussian Discriminant Analysis Through Eigenvalue Decomposition.” Journal of the American Statistical Association 91: 1743–48.\n\n\nBishop, Christopher. 2006. Pattern Recognition and Machine Learning. New York: Springer-Verlag Inc.\n\n\nBreiman, L., J. Friedman, R. Olshen, and C. J. Stone. 1984. Classification and Regression Trees. New York: Wadsworth.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nDavis, J., and M. Goadrich. 2006. “The Relationship Between Precision-Recall and ROC Curves.” In Proceedings of the 23rd International Conference on Machine Learning, 233–40.\n\n\nDean, Nema, Thomas Brendan Murphy, and Gerard Downey. 2006. “Using Unlabelled Data to Update Classification Rules with Applications in Food Authenticity Studies.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 55 (1): 1–14.\n\n\nDua, Dheeru, and Casey Graff. 2017. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. http://archive.ics.uci.edu/ml.\n\n\nForina, M., C. Armanino, S. Lanteri, and E. Tiscornia. 1983. “Classification of Olive Oils from Their Fatty Acid Composition.” In Food Research and Data Analysis, edited by H. Martens and H. Russwurm Jr., 189–214. London: Applied Science Publishers.\n\n\nFraley, C., and A. E. Raftery. 2002. “Model-Based Clustering, Discriminant Analysis, and Density Estimation.” Journal of the American Statistical Association 97 (458): 611–31.\n\n\nGneiting, Tilmann, and Adrian E Raftery. 2007. “Strictly Proper Scoring Rules, Prediction, and Estimation.” Journal of the American Statistical Association 102 (477): 359–78.\n\n\nGrau, Jan, Ivo Grosse, and Jens Keilwagen. 2015. “PRROC: Computing and Visualizing Precision-Recall and Receiver Operating Characteristic Curves in R.” Bioinformatics 31 (15): 2595–97.\n\n\nHastie, Trevor, and Robert Tibshirani. 1996. “Discriminant Analysis by Gaussian Mixtures.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 58 (1): 155–76.\n\n\nHastie, T., R. Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer-Verlag. http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/.\n\n\nKeilwagen, Jens, Ivo Grosse, and Jan Grau. 2014. “Area Under Precision-Recall Curves for Weighted and Unweighted Data.” PLOS ONE 9 (3).\n\n\nKohavi, Ron. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2, 1137–43. IJCAI’95. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nKruppa, Jochen, Yufeng Liu, Gérard Biau, Michael Kohler, Inke R König, James D Malley, and Andreas Ziegler. 2014. “Probability Estimation with Machine Learning Methods for Dichotomous and Multicategory Outcome: Theory.” Biometrical Journal 56 (4): 534–63.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. New York: Springer. https://doi.org/10.1007/978-1-4614-6849-3.\n\n\nLantz, Brett. 2019. Machine Learning with R: Expert Techniques for Predictive Modeling. 3rd ed. Packt Publishing.\n\n\nMangasarian, Olvi L, W Nick Street, and William H Wolberg. 1995. “Breast Cancer Diagnosis and Prognosis via Linear Programming.” Operations Research 43 (4): 570–77.\n\n\nMcLachlan, Geoffrey. 2004. Discriminant Analysis and Statistical Pattern Recognition. New York: John Wiley & Sons.\n\n\nMcLachlan, Geoffrey John. 1977. “Estimating the Linear Discriminant Function from Initial Samples Containing a Small Number of Unclassified Observations.” Journal of the American Statistical Association 72 (358): 403–6.\n\n\nMcNicholas, Paul D., Aisha ElSherbiny, Aaron F. McDaid, and T. Brendan Murphy. 2022. pgmm: Parsimonious Gaussian Mixture Models. https://CRAN.R-project.org/package=pgmm.\n\n\nO’Neill, Terence J. 1978. “Normal Discrimination with Unclassified Observations.” Journal of the American Statistical Association 73 (364): 821–26.\n\n\nSaerens, Marco, Patrice Latinne, and Christine Decaestecker. 2002. “Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure.” Neural Computation 14 (1): 21–41.\n\n\nSing, T., O. Sander, N. Beerenwinkel, and T. Lengauer. 2005. “ROCR: Visualizing Classifier Performance in R.” Bioinformatics 21 (20): 7881. http://rocr.bioinf.mpi-sb.mpg.de.\n\n\nStreet, W Nick, William H Wolberg, and Olvi L Mangasarian. 1993. “Nuclear Feature Extraction for Breast Tumor Diagnosis.” In Biomedical Image Processing and Biomedical Visualization, 1905:861–70. International Society for Optics; Photonics.\n\n\nTortora, Cristina, Aisha ElSherbiny, Ryan P. Browne, Brian C. Franczak, and Paul D. McNicholas, and Donald D. Amos. 2022. MixGHD: Model Based Clustering, Classification and Discriminant Analysis Using the Mixture of Generalized Hyperbolic Distributions. https://CRAN.R-project.org/package=MixGHD.\n\n\nZhu, Xiaojin, and Andrew B Goldberg. 2009. Introduction to Semi-Supervised Learning. Vol. 3. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/04_classification.html#footnotes",
    "href": "chapters/04_classification.html#footnotes",
    "title": "4  Mixture-Based Classification",
    "section": "",
    "text": "In stratified cross-validation the data are randomly split in such a way that maintains the same class distribution in each fold. This is of particular relevance in the case of unbalanced class distribution. Furthermore, it has been noted that “stratification is generally a better scheme, both in terms of bias and variance, when compared to regular cross-validation” (Kohavi 1995).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mixture-Based Classification</span>"
    ]
  },
  {
    "objectID": "chapters/05_dens.html",
    "href": "chapters/05_dens.html",
    "title": "5  Model-Based Density Estimation",
    "section": "",
    "text": "5.1 Density Estimation\nDensity estimation is a valuable tool for summarizing data distributions. A good density estimate can reveal important characteristics of the data as well as provide a useful exploratory tool. Broadly speaking, three alternative approaches can be distinguished: parametric density estimation, nonparametric density estimation, and semiparametric density estimation via finite mixture modeling.\nIn the parametric approach, a distribution is assumed for the density with unknown parameters that are estimated by fitting a parametric function to the observed data. In the nonparametric approach, no density function is assumed a priori; rather, its form is completely determined by the data. Histograms and kernel density estimation (KDE) are two popular methods that belong to this class, and in both the number of parameters generally grows with the sample size and dimensionality of the dataset (Silverman 1998; Scott 2009). For univariate data, functions hist() and density() are available in base R for nonparametric density estimation, and several other packages, such as KernSmooth (Wand 2021), ks (Duong 2022), and sm (A. Bowman et al. 2022), either include functionality or are devoted to this approach. However, extension to higher dimensions is less well established.\nFinite mixture models provide a flexible semiparametric methodology for density estimation. In this approach, the unknown density is expressed as a convex linear combination of one or more probability density functions. The Gaussian mixture model (GMM), which assumes the Gaussian distribution for the underlying component densities, is a popular choice in this class of methods. GMMs can approximate any continuous density with arbitrary accuracy, provided the model has a sufficient number of components (Ferguson 1983; Marron and Wand 1992; Escobar and West 1995; Roeder and Wasserman 1997; Frühwirth-Schnatter 2006). The number of parameters increases with the number of components, as well as with the dimensionality of the data except in the simplest case.\nAdvantages of the Gaussian mixture modeling in this context include:\nDisadvantages of the Gaussian mixture modeling approach include:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model-Based Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/05_dens.html#density-estimation",
    "href": "chapters/05_dens.html#density-estimation",
    "title": "5  Model-Based Density Estimation",
    "section": "",
    "text": "No need to specify tuning parameters, such as the number of bins and the origin for histograms, or the bandwidth for kernel density estimation;\nEfficient even for multidimensional data;\nMaximum likelihood estimates are available.\n\n\n\nEstimation via the EM algorithm may be slow and requires good starting values;\nBias-variance trade-off: many GMM components may be needed for approximating a distribution, increasing the variance of the estimates, while more parsimonious models reduce the variability but may introduce some bias.\nThe number of components to include is not known in advance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model-Based Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/05_dens.html#finite-mixture-modeling-for-density-estimation-with-mclust",
    "href": "chapters/05_dens.html#finite-mixture-modeling-for-density-estimation-with-mclust",
    "title": "5  Model-Based Density Estimation",
    "section": "\n5.2 Finite Mixture Modeling for Density Estimation with mclust",
    "text": "5.2 Finite Mixture Modeling for Density Estimation with mclust\nConsider a vector of random variables \\(\\boldsymbol{x}\\) taking values in the sample space \\(\\Real{^d}\\) with \\(d \\ge 1\\), and assume that the probability density function can be written as a finite mixture density of \\(G\\) components as in equation Equation 2.1. The model adopted in this chapter assumes a Gaussian distribution for each component density, so the density of \\(\\boldsymbol{x}\\) can be written as  \\[\nf(\\boldsymbol{x}) = \\sum_{k=1}^G \\pi_k \\phi(\\boldsymbol{x}; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k),\n\\] where \\(\\phi(\\cdot)\\) is the (multivariate) Gaussian density with mean \\(\\boldsymbol{\\mu}_k\\), covariance matrix \\(\\boldsymbol{\\Sigma}_k\\), and mixing weight \\(\\pi_k\\) for component \\(k\\) (\\(\\pi_k &gt; 0\\), \\(\\sum_{k=1}^G \\pi_k = 1\\)), with \\(k=1,\\dots,G\\). This is essentially a GMM of the form given in Equation 2.3.\nGaussian finite mixture modeling is a general strategy for density estimation. Nonparametric kernel density estimation (KDE) can be viewed as a mixture of \\(G = n\\) components with uniform weights: \\(\\pi_k = 1/n\\) (Titterington, Smith, and Makov 1985, 28–29). Compared to KDE, finite mixture modeling typically uses a smaller number of components and hence fewer parameters. Conversely, compared to fully parametric density estimation, finite mixture modeling has the potential advantage of using more parameters and so introducing less estimation bias.\nMixture modeling also has its drawbacks, such as increased learning complexity and the need for iterative numerical procedures (such as the EM algorithm) for estimation. In certain cases there can also be identifiability issues (see Section 2.1.2).\nmclust provides a simple interface to Gaussian mixture models for univariate and multivariate density estimation through the densityMclust()  function. Available arguments and functionalities are analogous to those described in Section 3.2 for the Mclust() function in the clustering case. Several examples of its usage are provided in the following sections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model-Based Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/05_dens.html#univariate-density-estimation",
    "href": "chapters/05_dens.html#univariate-density-estimation",
    "title": "5  Model-Based Density Estimation",
    "section": "\n5.3 Univariate Density Estimation",
    "text": "5.3 Univariate Density Estimation\n\nExample 5.1   Density estimation of Hidalgo stamp data\nIzenman and Sommer (1988) considered fitting a Gaussian mixture to the distribution of the thickness of stamps in the 1872 Hidalgo stamp issue of Mexico. The dataset is available in R package multimode (Ameijeiras-Alonso et al. 2021).\n\ndata(\"stamps1\", package = \"multimode\")\nstr(stamps1)\n## 'data.frame':    485 obs. of  2 variables:\n## $ thickness: num 0.06 0.064 0.064 0.065 0.066 0.068 0.069 0.069 0.069\n##    0.069 ...\n## $ year : Factor w/ 2 levels \"1872\",\"1873-1874\": 2 2 2 2 2 1 1 1 1 2 ...\nThickness = stamps1$thickness\n\nA density estimate based on GMMs can be obtained in mclust with the function densityMclust(), which also produces a plot of the estimated density:\n\ndens = densityMclust(Thickness)\n\nThe plot can be suppressed by setting the optional argument plot = FALSE. A summary  can be obtained as follows:\n\nsummary(dens, parameters = TRUE)\n## ------------------------------------------------------- \n## Density estimation via Gaussian finite mixture modeling \n## ------------------------------------------------------- \n## \n## Mclust V (univariate, unequal variance) model with 3 components: \n## \n##  log-likelihood   n df    BIC    ICL\n##          1516.6 485  8 2983.8 2890.9\n## \n## Mixing probabilities:\n##       1       2       3 \n## 0.26535 0.30179 0.43286 \n## \n## Means:\n##        1        2        3 \n## 0.072145 0.079346 0.099189 \n## \n## Variances:\n##            1            2            3 \n## 0.0000047749 0.0000031193 0.0001886446\n\nThis summary output shows that the estimated model selected by BIC is a three-component mixture with different variances (V,3), and gives the values of the estimated parameters.\nThe density estimate can also be displayed with the associated plot() method. For instance, Figure 5.1 shows the estimated density drawn over a histogram of the observed data. The latter is added by providing the optional argument data and specifying the break points between histogram bars (in the base R hist() function) by setting the argument breaks:\n\nbr = seq(min(Thickness), max(Thickness), length.out = 21)\nplot(dens, what = \"density\", data = Thickness, breaks = br)\n\n\n\n\n\n\nFigure 5.1: Histogram of thickness for the Hidalgo 1872 stamps1 dataset, with the GMM density estimate superimposed.\n\n\n\n\nThree modes are clearly visible in the plot. These appear at the values of the mean of each mixture component: one component with larger mean and dispersion of stamp thickness, and two components having thinner and less variable stamps:\n\nwith(dens$parameters, \n     data.frame(mean = mean,\n                sd = sqrt(variance$sigmasq),\n                CoefVar = sqrt(variance$sigmasq)/mean*100))\n##       mean        sd CoefVar\n## 1 0.072145 0.0021852  3.0288\n## 2 0.079346 0.0017662  2.2259\n## 3 0.099189 0.0137348 13.8470\n\nA predict()  method is associated with objects of class \"densityMclust\", for computing either the overall density (what = \"dens\", the default), or the individual (unweighted) mixture component densities (what = \"cdens\") at specified data points. Densities are returned on the logarithmic scale if we set logarithm = TRUE. For instance:\n\nx = c(0.07, 0.08, 0.1, 0.12)\npredict(dens, newdata = x, what = \"dens\")\n## [1] 31.2340 68.4716 12.5509  3.9895\npredict(dens, newdata = x, logarithm = TRUE)\n## [1] 3.4415 4.2264 2.5298 1.3837\npredict(dens, newdata = x, what = \"cdens\")\n##                1           2       3\n## [1,]  1.1275e+02  1.8734e-04  3.0362\n## [2,]  2.8553e-01  2.1093e+02 10.9450\n## [3,]  9.4735e-34  4.5561e-28 28.9956\n## [4,] 1.3055e-102 2.0026e-113  9.2166\n\nThe matrix of posterior conditional probabilities is obtained by specifying what = \"z\":\n\npredict(dens, newdata = x, what = \"z\")\n##                1           2        3\n## [1,]  9.5792e-01  1.8102e-06 0.042077\n## [2,]  1.1065e-03  9.2970e-01 0.069191\n## [3,]  2.0029e-35  1.0955e-29 1.000000\n## [4,] 8.6832e-104 1.5149e-114 1.000000\n\nThe stamps1 dataset contains additional information that can be used to shed light on the selected model. In particular, thickness measurements can be grouped according to the year of consignment: the first 289 stamps refer to the 1872 issue and the remaining 196 stamps to the years 1873–1874. We may draw a (suitably scaled) histogram for each year-of-consignment and then add the estimated component densities as follows:\n\nYear = stamps1$year\ntable(Year)\n## Year\n##      1872 1873-1874 \n##       289       196\nh1 = hist(Thickness[Year == \"1872\"], breaks = br, plot = FALSE)\nh1$density = h1$density*prop.table(table(Year))[1]\nh2 = hist(Thickness[Year == \"1873-1874\"], breaks = br, plot = FALSE)\nh2$density = h2$density*prop.table(table(Year))[2]\nx = seq(min(Thickness)-diff(range(Thickness))/10, \n         max(Thickness)+diff(range(Thickness))/10, length = 200)\ncdens = predict(dens, x, what = \"cdens\")\ncdens = sweep(cdens, 2, dens$parameters$pro, \"*\")\ncol = adjustcolor(mclust.options(\"classPlotColors\")[1:2], alpha = 0.3)\nylim = range(h1$density, h2$density, cdens)\nplot(h1, xlab = \"Thickness\", freq = FALSE, main = \"\", border = \"white\", \ncol = col[1], xlim = range(x), ylim = ylim)\nplot(h2, add = TRUE, freq = FALSE, border = \"white\", col = col[2])\nmatplot(x, cdens, type = \"l\", lwd = 1, lty = 1, col = 1, add = TRUE)\nbox()\nlegend(\"topright\", legend = levels(Year), col = col, pch = 15, inset = 0.02,\n       title = \"Overprinted years:\", title.adj = 0.2)\n\n\n\n\n\n\nFigure 5.2: Histograms of thickness by overprinted year for the Hidalgo 1872 stamps1 dataset, with mixture component-density estimates superimposed.\n\n\n\n\nThe result is shown in Figure 5.2. Stamps from 1872 show a two-part distribution, with one component corresponding to the largest thickness, and one whose distribution essentially overlaps with the bimodal distribution of stamps for the years 1873–1874.\n\n\nExample 5.2   Density estimation of acidity data\nConsider the distribution of an acidity index measured on a sample of 155 lakes in the Northeastern United States. The data have been analyzed on the logarithmic scale using mixtures of normal distributions by Richardson and Green (1997) and (McLachlan and Peel 2000, sec. 6.6.2). The dataset is included in the CRAN package BNPdensity (Arbel et al. 2021; Barrios et al. 2021) and can be accessed as follows:\n\ndata(\"acidity\", package = \"BNPdensity\")\n\nUsing BIC as the model selection criterion, the “best” estimated model with respect to both the variance structure and the number of components can be obtained as follows:\n\nsummary(mclustBIC(acidity), k = 5)\n## Best BIC values:\n##              E,2       V,3       V,2       E,5      E,3\n## BIC      -392.07 -397.9108 -399.6945 -400.6745 -402.183\n## BIC diff    0.00   -5.8385   -7.6222   -8.6022  -10.111\n\nBIC supports the two-component model with equal variance across the components (E,2). This model can be estimated as follows:\n\ndens_E2 = densityMclust(acidity, G = 2, modelNames = \"E\")\nsummary(dens_E2, parameters = TRUE)\n## ------------------------------------------------------- \n## Density estimation via Gaussian finite mixture modeling \n## ------------------------------------------------------- \n## \n## Mclust E (univariate, equal variance) model with 2 components: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -185.95 155  4 -392.07 -398.56\n## \n## Mixing probabilities:\n##       1       2 \n## 0.62336 0.37664 \n## \n## Means:\n##      1      2 \n## 4.3709 6.3202 \n## \n## Variances:\n##       1       2 \n## 0.18637 0.18637\n\nNext, we provisionally assume a GMM with unequal variances and decide the number of mixture components via the bootstrap LRT discussed in Section 2.3.2 instead of the BIC:\n\nmclustBootstrapLRT(acidity, modelName = \"V\")\n## ------------------------------------------------------------- \n## Bootstrap sequential LRT for the number of mixture components \n## ------------------------------------------------------------- \n## Model        = V \n## Replications = 999 \n##             LRTS bootstrap p-value\n## 1 vs 2   77.0933             0.001\n## 2 vs 3   16.9140             0.005\n## 3 vs 4    5.1838             0.301\n\nLRT suggests a three-component Gaussian mixture (V,3), which was a fairly close second-best fit according to BIC. This (V,3) model can be estimated as follows:\n\ndens_V3 = densityMclust(acidity, G = 3, modelNames = \"V\")\nsummary(dens_V3, parameters = TRUE)\n## ------------------------------------------------------- \n## Density estimation via Gaussian finite mixture modeling \n## ------------------------------------------------------- \n## \n## Mclust V (univariate, unequal variance) model with 3 components: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -178.78 155  8 -397.91 -458.86\n## \n## Mixing probabilities:\n##       1       2       3 \n## 0.34061 0.31406 0.34534 \n## \n## Means:\n##      1      2      3 \n## 4.2040 4.6796 6.3809 \n## \n## Variances:\n##        1        2        3 \n## 0.044195 0.338309 0.178276\n\nThe parameter estimates for the GMMs are available from the summary() output, and the corresponding density functions can be plotted  with the following code:\nplot(dens_E2, what = \"density\",\n     ylim = c(0, max(dens_E2$density, dens_V3$density)))\nrug(acidity)\nplot(dens_V3, what = \"density\", \n     ylim = c(0, max(dens_E2$density, dens_V3$density)))\nrug(acidity)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 5.3: Density estimates provided by (a) the model (E,2) supported by BIC and (b) the model (V,3) supported by the LRT for the acidity data.\n\n\nFigure 5.3 shows the plots of the estimated densities. The two models largely agree, with the exception of a somewhat different shape for some lower-range values. In particular, both densities indicate a bimodal distribution.\n\n\n5.3.1 Diagnostics for Univariate Density Estimation\nTwo diagnostic plots for density estimation are available for univariate data (for details see Loader 1999, 87–90). The first plot is a graph of the estimated cumulative distribution function (CDF) against the empirical distribution function. The second is a Q-Q plot of the sample quantiles against the quantiles computed from the estimated density. For a GMM, the estimated CDF is just the weighted sum of individual CDFs for each component. It can be obtained in mclust via the function cdfMclust(). \n\nExample 5.3   Diagnostic for density estimation of acidity data\nRecalling Example 5.2 on acidity data, diagnostic plots for the (E,2) model can be obtained as follows:\nplot(dens_E2, what = \"diagnostic\", type = \"cdf\")\nplot(dens_E2, what = \"diagnostic\", type = \"qq\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 5.4: Density estimate diagnostics for the (E,2) model estimated for the acidity data: (a) estimated CDF and empirical distribution function, (b) Q-Q plot of sample quantiles vs. quantiles from density estimation.\n\n\nSimilarly, for the model (V,3):\nplot(dens_V3, what = \"diagnostic\", type = \"cdf\")\nplot(dens_V3, what = \"diagnostic\", type = \"qq\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 5.5: Density estimate diagnostics for the (V,3) model estimated for the acidity data: (a) estimated CDF and empirical distribution function, (b) Q-Q plot of sample quantiles vs. quantiles from density estimation.\n\n\nFigure 5.4 (a) and Figure 5.5 (a) compare the estimated CDFs to the empirical CDF. Figure 5.4 (b) and Figure 5.5 (b) show the corresponding Q-Q plots for the two models. Both types of diagnostics seem to suggest a better fit to the observed data for the more complex model, with perhaps one outlying point corresponding to the smallest value.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model-Based Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/05_dens.html#sec-densityhd",
    "href": "chapters/05_dens.html#sec-densityhd",
    "title": "5  Model-Based Density Estimation",
    "section": "\n5.4 Density Estimation in Higher Dimensions",
    "text": "5.4 Density Estimation in Higher Dimensions\nIn principle, density estimation in higher dimensions is straightforward with mclust. The structure imposed by the Gaussian components allows a parsimonious representation of the underlying density, especially when compared to KDE approaches. However, density estimation in high dimensions is problematic in general. Another issue is that visualizing densities beyond three dimensions is difficult. Following the suggestion of (Scott 2009, 217), we should investigate “density estimation in several dimensions rather than in very high dimensions”.\n\nExample 5.4   Density estimation of Old Faithful data\nConsider the Old Faithful data described in Example 3.4}. A scatterplot of the data is shown in Figure 5.6 (a). This and the corresponding density estimate can be obtained with the following code:\n\ndata(\"faithful\", package = \"datasets\")\nplot(faithful)\ndens = densityMclust(faithful)\nsummary(dens, parameters = TRUE)\n## ------------------------------------------------------- \n## Density estimation via Gaussian finite mixture modeling \n## ------------------------------------------------------- \n## \n## Mclust EEE (ellipsoidal, equal volume, shape and orientation) model\n## with 3 components: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -1126.3 272 11 -2314.3 -2357.8\n## \n## Mixing probabilities:\n##       1       2       3 \n## 0.16568 0.35637 0.47795 \n## \n## Means:\n##              [,1]    [,2]    [,3]\n## eruptions  3.7931  2.0376  4.4632\n## waiting   77.5211 54.4912 80.8334\n## \n## Variances:\n## [,,1]\n##           eruptions waiting\n## eruptions  0.078254  0.4802\n## waiting    0.480198 33.7671\n## [,,2]\n##           eruptions waiting\n## eruptions  0.078254  0.4802\n## waiting    0.480198 33.7671\n## [,,3]\n##           eruptions waiting\n## eruptions  0.078254  0.4802\n## waiting    0.480198 33.7671\n\nModel selection based on the BIC selects a three-component mixture with common covariance matrix (EEE,3). One component is used to model the group of observations having both low duration and low waiting times, whereas two components are needed to approximate the skewed distribution of the observations with larger duration and waiting times.\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 5.6: Scatterplot of the Old Faithful data (a), mclust density estimate represented as contour levels (b), image plot of density estimate (c), and perspective plot of the bivariate density estimate (d).\n\n\nFor two-dimensional data, the plot()  method for the argument what = \"density\" is by default a contour plot of the density estimate, as shown in Figure 5.6 (b):\n\nplot(dens, what = \"density\")\n\nA bivariate density estimate may also be plotted using an image plot or a perspective plot (see Figure 5.6 (c) and Figure 5.6 (d)):\n\nplot(dens, what = \"density\", type = \"image\")\nplot(dens, what = \"density\", type = \"persp\")\n\nIn the above code we specified the type of density plot to produce. Several optional arguments for customizing the plots are available; for a complete description see help(\"plot.densityMclust\") and help(\"surfacePlot\"). More details on producing graphical displays with mclust are discussed in Section 6.2 and Section 6.3.\nAnother useful approach to visualizing multivariate densities involves summarizing a density estimate with (possibly disjoint) regions of the sample space covering a specified probability. These are called Highest Density Regions (HDRs)  by Hyndman (1996); for details on the definition and the computational implementation see Section 5.6.\nIn mclust, HDRs can be easily obtained by specifying type = \"hdr\" in the plot() function applied to an object returned by a densityMclust() function call:\n\nplot(dens, what = \"density\", type = \"hdr\")\n\n\n\n\n\n\nFigure 5.7: Highest density regions from the density estimated on the faithful data at probability levels 0.25, 0.5, and 0.75.\n\n\n\n\n\nFor higher dimensional datasets densityMclust() provides graphical displays of density estimates using density contour, image, and perspective plots for pairs of variables arranged in a matrix.\n\nExample 5.5   Density estimation on principal components of the aircraft data\nConsider the dataset aircraft, available in the sm package (A. Bowman et al. 2022) for R, giving measurements on six physical characteristics of twentieth-century aircraft. Following the analysis of A. W. Bowman and Azzalini (1997, sec. 1.3), we consider the principal components computed from the data on the log scale for the time period 1956–1984:\n\ndata(\"aircraft\", package = \"sm\")\nX = log(subset(aircraft, subset = (Period == 3), select = 3:8))\nPCA = prcomp(X, scale = TRUE)\nsummary(PCA)\n## Importance of components:\n##                          PC1   PC2    PC3     PC4     PC5     PC6\n## Standard deviation     2.085 1.064 0.6790 0.16892 0.15239 0.08771\n## Proportion of Variance 0.725 0.189 0.0769 0.00476 0.00387 0.00128\n## Cumulative Proportion  0.725 0.913 0.9901 0.99485 0.99872 1.00000\nPCA$rotation       # loadings\n##             PC1       PC2      PC3        PC4       PC5       PC6\n## Power  -0.45612  0.262445  0.11357 -0.2267760 -0.565774 -0.581940\n## Span   -0.37433 -0.540127  0.32461 -0.5305967  0.417040 -0.085530\n## Length -0.46737 -0.088898  0.21657  0.7885789  0.260671 -0.192237\n## Weight -0.47409  0.036498  0.17017 -0.0121226 -0.365831  0.781647\n## Speed  -0.29394  0.731054 -0.15713 -0.2121675  0.551014  0.076387\n## Range  -0.34963 -0.309371 -0.88384  0.0045952 -0.024057 -0.016386\nZ = PCA$x[, 1:3]   # PCA projection\n\nConsider the fit of a GMM on the first three principal components (PCs), which explain 99% of total variability. The BIC criterion indicates a number of candidate models with almost the same support from the data.\n\nBIC = mclustBIC(Z)\nsummary(BIC, k = 5)\n## Best BIC values:\n##            VEE,5        VVE,4      VEI,6     VVE,3      VEE,4\n## BIC      -1958.7 -1958.752723 -1961.8724 -1962.876 -1965.7162\n## BIC diff     0.0    -0.014737    -3.1344    -4.138    -6.9782\n\nWe fit the simpler three-component model with varying volume and shape (VVE), and plot the estimated density (see Figure 5.8).\n\ndensAircraft = densityMclust(Z, G = 3, modelNames = \"VVE\", plot = FALSE)\nplot(densAircraft, what = \"density\", type = \"hdr\", \n     data = Z, points.cex = 0.5)\n\n\n\n\n\n\nFigure 5.8: Scatterplot matrix of selected principal components of the aircraft data with bivariate highest density regions at probability levels 0.25, 0.5, and 0.75.\n\n\n\n\nThe default probability levels \\(0.25\\), \\(0.5\\), and \\(0.75\\) are used in these plots (for details see Section 5.6). The plots are arranged in a symmetric matrix, where each off-diagonal panel shows the bivariate marginal density estimate. Data points have been also added to the upper-diagonal panels by providing the matrix of principal components to the optional argument data. The presence of several modes is clearly evident from this figure.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model-Based Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/05_dens.html#sec-densbounded",
    "href": "chapters/05_dens.html#sec-densbounded",
    "title": "5  Model-Based Density Estimation",
    "section": "\n5.5 Density Estimation for Bounded Data",
    "text": "5.5 Density Estimation for Bounded Data\nFinite mixtures of Gaussian distributions provide a flexible model for density estimation when the continuous variables under investigation have no boundaries. However, in practical applications variables may be partially bounded (taking non-negative values) or completely bounded (taking values in the unit interval). In this case, the standard Gaussian finite mixture model assigns non-zero densities to any possible value, even to those outside the ranges where the variables are defined, hence resulting in potentially severe bias.\nA transformation-based approach for Gaussian mixture modeling in the case of bounded variables has been recently proposed (Scrucca 2019). The basic idea is to carry out density estimation not on the original data \\(\\boldsymbol{x}\\) with bounded support \\(\\mathcal{S}_{\\mathcal{X}} \\subset \\Real^d\\), but on the transformed data \\(\\boldsymbol{y}= t(\\boldsymbol{x}; \\boldsymbol{\\lambda})\\) having unbounded support \\(\\mathcal{S}_{\\mathcal{Y}}\\). A range-power transformation  is used for \\(t(\\cdot ; \\boldsymbol{\\lambda})\\). This is a monotonic transformation which depends on parameters \\(\\boldsymbol{\\lambda}\\) and maps \\(\\mathcal{S}_{\\mathcal{X}}\\) to \\(\\mathcal{S}_{\\mathcal{Y}}\\). The range part of the transformation differs depending on whether a variable has only a lower bound or has both a lower and an upper bound, while the power part of the transformation involves the well-known Box-Cox transformation (Box and Cox 1964). Once the density on the transformed scale is estimated, the density for the original data can be obtained by a change of variables using the Jacobian of the transformation. Both the transformation parameters and the parameters of the Gaussian mixture are jointly estimated by the EM algorithm. For more details see Scrucca (2019).\nThe methodology briefly outlined above is implemented in the R package mclustAddons (Scrucca 2022):\n\nlibrary(\"mclustAddons\")\n\n\nExample 5.6   Density estimation of suicide data\nThe dataset gives the lengths (in days) of 86 spells of psychiatric treatment undergone by control patients in a suicide risk study (Silverman 1998). Since the variable can only take positive values, in the following we contrast the density estimated ignoring this fact with the estimate obtained using the approach outlined above.\nThe following code estimates the density using the function densityMclust() with its default settings:\n\ndata(\"suicide\", package = \"mclustAddons\")\ndens = densityMclust(suicide)\nrug(suicide)            # add data points at the bottom of the graph\nabline(v = 0, lty = 3)  # draw a vertical line at the natural boundary\n\nThe resulting density is shown in Figure 5.9 (a). This is clearly unsatisfactory because it assigns nonzero density to negative values and exhibits a bimodal distribution, the latter being an artifact due to failure to account for the non-negativity.\nThe function densityMclustBounded()  can be used for improved density estimation by specifying the lower bound of the variable:\n\nbdens = densityMclustBounded(suicide, lbound = 0)\nsummary(bdens, parameters = TRUE)\n## ── Density estimation for bounded data via GMMs ─────────── \n##            \n## Boundaries: suicide\n##       lower       0\n##       upper     Inf\n## \n## Model E (univariate, equal variance) model with 1 component\n## on the transformation scale:\n## \n##  log-likelihood  n df   BIC   ICL\n##         -497.82 86  3 -1009 -1009\n## \n##                             suicide\n## Range-power transformation: 0.19293\n## \n## Mixing probabilities:\n## 1 \n## 1 \n## \n## Means:\n##      1 \n## 6.7001 \n## \n## Variances:\n##      1 \n## 7.7883\nplot(bdens, what = \"density\")\nrug(suicide)            # add data points at the bottom of the graph\nabline(v = 0, lty = 3)  # draw a vertical line at the natural boundary\n\nThe estimated parameter of the range-power transformation is equal to \\(0.19\\), and on the transformed scale the optimal GMM is a model with a single mixture component. The corresponding density estimate on the original scale of the variable is shown in Figure 5.9 (b). As the plot shows, accounting for the natural boundary of the variable better represents the underlying density of the data.\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 5.9: Density estimates for the suicide data. Panel (a) shows the default estimate which ignores the lower boundary of the variable, while panel (b) shows the density estimated accounting for the natural boundary of the variable.\n\n\n\n\nExample 5.7   Density estimation of racial data\nThis dataset provides the proportion of white student enrollment in 56 school districts in Nassau County (Long Island, New York, USA), for the 1992–1993 school year (Simonoff 1996, sec. 3.2). Because each observation is a proportion, the density estimate for this data should not fall outside the \\([0,1]\\) range.\nThe following code reads the data, performs density estimation by specifying both the lower and upper bounds of the variable under study, and plots the data and the estimated density:\n\ndata(\"racial\", package = \"mclustAddons\")\nbdens = densityMclustBounded(racial$PropWhite, lbound = 0, ubound = 1)\nplot(bdens, what = \"density\", \n     lwd = 2, col = \"dodgerblue2\",\n     data = racial$PropWhite, breaks = 15,\n     xlab = \"Proportion of white students enrolled in schools\")\nrug(racial$PropWhite)        # add data points at the bottom of the graph\nabline(v = c(0, 1), lty = 3)  # draw a vertical line at the natural boundary\n\n\n\n\n\n\nFigure 5.10: Density estimate for the racial data obtained by taking into account the natural boundary of the proportion.\n\n\n\n\nFigure 5.10 displays the estimated density, showing that the majority of the schools have at least 70% of white students, but there is also a small peak near the lower boundary containing schools with almost 0% of white students.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model-Based Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/05_dens.html#sec-hdr",
    "href": "chapters/05_dens.html#sec-hdr",
    "title": "5  Model-Based Density Estimation",
    "section": "\n5.6 Highest Density Regions",
    "text": "5.6 Highest Density Regions\nDensity curves are a very effective and useful way to show the distribution of values in a dataset. For two-dimensional data, perspective and contour plots represent a standard way to visualize densities (see for instance Figure 5.6). An alternative method is discussed by Hyndman (1996), who proposed the use of Highest Density Regions (HDRs)  for summarizing probability distributions. These are defined as the (possibly disjoint) regions of the sample space covering a specified probability level.\nLet \\(f(x)\\) be the density function of a random variable \\(X\\); then the \\(100(1-\\alpha)\\%\\) HDR is the subset \\(R(f_\\alpha)\\) of the sample space of \\(X\\) such that \\[\nR(f_\\alpha) = \\{x : f(x) \\ge f_\\alpha \\},\n\\] where \\(f_\\alpha\\) is the largest value such that \\(\\Pr( X \\in R(f_\\alpha)) \\ge 1-\\alpha\\).\nFor estimating \\(R(f_\\alpha)\\), Hyndman (1996) proposed considering the \\((1-\\alpha)\\)-quantile of the density function evaluated at the observed data points. The accuracy of this method depends on the sample size, so for small to moderate sample sizes the degree of accuracy can be improved by enlarging the set of observations with additional simulated data points.\nFor example, consider the density of a univariate two-component Gaussian mixture defined with the following code and shown in Figure 5.11:\n\nf = function(x) \n  0.7*dnorm(x, mean = 0, sd = 1) + 0.3*dnorm(x, mean = 4, sd = 1)\ncurve(f, from = -4, to = 8)\n\n\n\n\n\n\nFigure 5.11: Density of the univariate two-component Gaussian mixture \\(f(x) = 0.7\\phi(x | \\mu = 0, \\sigma = 1) + 0.3\\phi(x | \\mu = 4, \\sigma = 1)\\).\n\n\n\n\nA simulated dataset drawn from this mixture can be obtained as follows:\n\npar = list(pro = c(0.7, 0.3), mean = c(0, 4), \n           variance = mclustVariance(\"E\", G = 2))\npar$variance$sigmasq = c(1, 1)\nx = sim(modelName = \"E\", parameters = par, n = 1e4)[, -1]\n\nThe first column returned by sim() contains the (randomly generated) classification and is therefore omitted. For details of the procedure, see Section 7.4.\nThe highest density regions corresponding to specified probability levels can be computed using the function hdrlevels(),  and then plotted as follows:\n(hdr = hdrlevels(f(x), prob))\n##      25%      50%      75%      95% \n## 0.250682 0.158753 0.096956 0.048063\nfor (j in seq(prob))\n{\n  curve(f, from = -4, to = 8)\n  mtext(side = 3, paste0(prob[j]*100, \"% HDR\"), adj = 0)\n  abline(h = hdr[j], lty = 2)\n  rug(x, col = \"lightgrey\")\n  rug(x[f(x) &gt;= hdr[j]])\n}\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 5.12: Highest density regions at specified probability levels from the density of a univariate two-component Gaussian mixture.\n\n\nThe resulting plots are shown in Figure 5.12.\nIn general the true density function is unknown, so that an estimate must be used, but the procedure remains essentially unchanged. For instance, we may estimate the density with mclust and then obtain the HDR levels as follows:\n\ndens = densityMclust(x, plot = FALSE)\nhdrlevels(predict(dens, x), prob)\n##      25%      50%      75%      95% \n## 0.251234 0.159436 0.096781 0.048798\n\n\n\n\n\nAmeijeiras-Alonso, Jose, Rosa M. Crujeiras, Alberto Rodríguez-Casal, The R Core Team 1996-2012, and The R Foundation 2005. 2021. multimode: Mode Testing and Exploring. https://CRAN.R-project.org/package=multimode.\n\n\nArbel, Julyan, Guillaume Kon Kam King, Antonio Lijoi, Luis Nieto-Barajas, and Igor Prünster. 2021. “BNPdensity: Bayesian Nonparametric Mixture Modelling in R.” Australian & New Zealand Journal of Statistics 63 (3): 542–64.\n\n\nBarrios, Ernesto, Guillaume Kon Kam King, Antonio Lijoi, Luis E. Nieto-Barajas, and Igor Prünster. 2021. BNPdensity: Ferguson-Klass Type Algorithm for Posterior Normalized Random Measures. https://doi.org/10.1111/anzs.12342.\n\n\nBowman, A. W., and A. Azzalini. 1997. Applied Smoothing Techniques for Data Analysis. Oxford: Oxford University Press.\n\n\nBowman, Adrian, Adelchi Azzalini. Ported to R by B. D. Ripley up to version 2.0, version 2.1 by Adrian Bowman, Adelchi Azzalini, and version 2.2 by Adrian Bowman. 2022. sm: Smoothing Methods for Nonparametric Regression and Density Estimation. https://CRAN.R-project.org/package=sm.\n\n\nBox, G. E., and D. R. Cox. 1964. “An Analysis of Transformations.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 26 (2): 211–52.\n\n\nDuong, Tarn. 2022. ks: Kernel Smoothing. https://CRAN.R-project.org/package=ks.\n\n\nEscobar, Michael D, and Mike West. 1995. “Bayesian Density Estimation and Inference Using Mixtures.” Journal of the American Statistical Association 90 (430): 577–88.\n\n\nFerguson, Thomas. 1983. “Bayesian Density Estimation by Mixtures of Normal Distributions.” In Recent Advances in Statistics, edited by M. Haseeb Rizvi, Jagdish S. Rustagi, and David Siegmund, 287–302. Academic Press.\n\n\nFrühwirth-Schnatter, Sylvia. 2006. Finite Mixture and Markov Switching Models. Springer.\n\n\nHyndman, Rob J. 1996. “Computing and Graphing Highest Density Regions.” The American Statistician 50 (2): 120–26.\n\n\nIzenman, Alan J, and Charles J Sommer. 1988. “Philatelic Mixtures and Multimodal Densities.” Journal of the American Statistical Association 83 (404): 941–53.\n\n\nLoader, C. 1999. Local Regression and Likelihood. New York: Springer Verlag.\n\n\nMarron, J Steve, and Matt P Wand. 1992. “Exact Mean Integrated Squared Error.” Annals of Statistics 20 (2): 712–36.\n\n\nMcLachlan, G. J., and D. Peel. 2000. Finite Mixture Models. New York: Wiley.\n\n\nRichardson, Sylvia, and Peter J Green. 1997. “On Bayesian Analysis of Mixtures with an Unknown Number of Components (with Discussion).” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 59 (4): 731–92.\n\n\nRoeder, K., and L. Wasserman. 1997. “Practical Bayesian Density Estimation Using Mixtures of Normals.” Journal of the American Statistical Association 92 (439): 894–902.\n\n\nScott, David W. 2009. Multivariate Density Estimation: Theory, Practice, and Visualization. 2nd ed. John Wiley & Sons.\n\n\nScrucca, Luca. 2019. “A Transformation-Based Approach to Gaussian Mixture Density Estimation for Bounded Data.” Biometrical Journal 61 (4): 1–16. https://doi.org/10.1002/bimj.201800174.\n\n\n———. 2022. mclustAddons: Addons for the ’Mclust’ Package. https://CRAN.R-project.org/package=mclustAddons.\n\n\nSilverman, Bernard W. 1998. Density Estimation for Statistics and Data Analysis. Chapman & Hall/CRC.\n\n\nSimonoff, J S. 1996. Smoothing Methods in Statistics. Springer.\n\n\nTitterington, D Michael, Adrian FM Smith, and Udi E Makov. 1985. Statistical Analysis of Finite Mixture Distributions. Chichester; New York: John Wiley & Sons.\n\n\nWand, Matt. 2021. KernSmooth: Functions for Kernel Smoothing Supporting Wand & Jones (1995). https://CRAN.R-project.org/package=KernSmooth.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model-Based Density Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/06_graphics.html",
    "href": "chapters/06_graphics.html",
    "title": "6  Visualizing Gaussian Mixture Models",
    "section": "",
    "text": "6.1 Displays for Univariate Data\nmclust models of univariate data can be visualized with mclust1Dplot().",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing Gaussian Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/06_graphics.html#sec-display1",
    "href": "chapters/06_graphics.html#sec-display1",
    "title": "6  Visualizing Gaussian Mixture Models",
    "section": "",
    "text": "Example 6.1   Additional graphs for the fish length data\nConsider the dataset introduced in Example 2.1 on the length (in inches) of 256 snapper fish. The following code fits a four-component varying-variance (\"V\") GMM to this data with Mclust():\n\ndata(\"Snapper\", package = \"FSAdata\")\nx = Snapper[,1]\nmod = Mclust(x, G = 4, modelNames = \"V\")\nsummary(mod, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust V (univariate, unequal variance) model with 4 components: \n## \n##  log-likelihood   n df     BIC   ICL\n##         -489.28 256 11 -1039.6 -1098\n## \n## Clustering table:\n##   1   2   3   4 \n##  26 143  56  31 \n## \n## Mixing probabilities:\n##        1        2        3        4 \n## 0.098263 0.542607 0.177391 0.181739 \n## \n## Means:\n##      1      2      3      4 \n## 3.3631 5.4042 7.5761 8.9021 \n## \n## Variances:\n##        1        2        3        4 \n## 0.073383 0.408370 0.197323 2.839843\n\nClassification and uncertainty plots for this model can be produced with the following code:\nmclust1Dplot(x, what = \"classification\", \n             parameters = mod$parameters, z = mod$z,\n             xlab = \"Fish length\")\nmclust1Dplot(x, what = \"uncertainty\",\n             parameters = mod$parameters, z = mod$z,\n             xlab = \"Fish length\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 6.1: Classification (a) and uncertainty (b) plots created withmclust1Dplot() for the GMM fit to the fishery data.\n\n\nThe resulting plots are shown in Figure 6.1. Classification errors and density for Mclust() models can also be displayed with mclust1Dplot(). Additional arguments can be specified for fine tuning, as described in help(\"mclust1Dplot\"). MclustDA() and densityMclust() models can also be vizualized with mclust1Dplot() in the same way as described above.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing Gaussian Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/06_graphics.html#sec-display2",
    "href": "chapters/06_graphics.html#sec-display2",
    "title": "6  Visualizing Gaussian Mixture Models",
    "section": "\n6.2 Displays for Bivariate Data",
    "text": "6.2 Displays for Bivariate Data\nmclust models fitted to bivariate data can be visualized with mclust2Dplot()  and, for finer details, surfacePlot(). \n\nExample 6.2   Additional graphs for the Old Faithful data\nIn the following code, classification and uncertainty plots are produced for a Mclust() model estimated on the faithful dataset discussed in Example 3.4:\nmod = Mclust(faithful)\nmclust2Dplot(data = faithful, what = \"classification\", \n             parameters = mod$parameters, z = mod$z)\nmclust2Dplot(data = faithful, what = \"uncertainty\",\n             parameters = mod$parameters, z = mod$z)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 6.2: Classification (a) and uncertainty (b) plots created with mclust2Dplot() for the model fitted with Mclust() to the faithful dataset.\n\n\nThe resulting plots are shown in Figure 6.2. In both plots, the ellipses are the multivariate analogs of the standard deviations for each mixture component. In the classification plot, points in different clusters are marked by different symbols and colors. In the uncertainty plot, larger symbols and more opaque shades correspond to higher levels of uncertainty. Data points on the boundaries between the two clusters on the top-right part of the plot are associated with higher values of clustering uncertainty.\nDensity or uncertainty for mclust models of bivariate data can also be displayed using surfacePlot().  This function invisibly returns the grid coordinates and the corresponding surface values employed for plotting, information that advanced users can use for further processing. For example, Figure 6.3 shows contour, image, perspective, and uncertainty plots for the model fitted to the faithful dataset:\nsurfacePlot(data = faithful, parameters = mod$parameters,\n            what = \"density\", type = \"contour\",\n            transformation = \"log\")\nsurfacePlot(data = faithful, parameters = mod$parameters,\n            what = \"density\", type = \"image\")\nsurfacePlot(data = faithful, parameters = mod$parameters,\n            what = \"density\", type = \"persp\")\nsurfacePlot(data = faithful, parameters = mod$parameters,\n            what = \"uncertainty\", type = \"image\",\n            transformation = \"sqrt\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 6.3: Density (a–c) and uncertainty (d) surfaces created with surfacePlot() for the faithful dataset. A logarithmic transformation is used for the density plot in panel (a), which is drawn as a contour surface. No transformation is applied to panels (b) and (c) which show, respectively, an image plot of the density surface and a 3D perspective plot. Panel (d) displays the image surface of the square root of the clustering uncertainty.\n\n\nNote that plots in Figure 6.3 are computed over an evenly spaced grid of 200 points (the default) along each axis. More arguments are available for fine tuning; a detailed description can be found via help(\"surfacePlot\").",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing Gaussian Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/06_graphics.html#sec-displayhd",
    "href": "chapters/06_graphics.html#sec-displayhd",
    "title": "6  Visualizing Gaussian Mixture Models",
    "section": "\n6.3 Displays for Higher Dimensional Data",
    "text": "6.3 Displays for Higher Dimensional Data\n\n6.3.1 Coordinate Projections\nCoordinate projections can be plotted in mclust with coordProj(). \n\nExample 6.3   Coordinate projections for the iris data\nConsider the best (according to BIC) three-group GMM estimated on the iris dataset described in Example 3.13:\n\ndata(\"iris\", package = \"datasets\")\nmod = Mclust(iris[,1:4], G = 3)\nsummary(mod)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VEV (ellipsoidal, equal shape) model with 3 components: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -186.07 150 38 -562.55 -566.47\n## \n## Clustering table:\n##  1  2  3 \n## 50 45 55\n\nThe following code produces projection plots for coordinates 2 and 4 of the iris data (see Figure 6.4).\ncoordProj(data = iris[,1:4], dimens = c(2,4), what = \"classification\",\n          parameters = mod$parameters, z = mod$z)\ncoordProj(data = iris[,1:4], dimens = c(2,4), what = \"uncertainty\",\n          parameters = mod$parameters, z = mod$z)\ncoordProj(data = iris[,1:4], dimens = c(2,4), what = \"error\",\n          parameters = mod$parameters, z = mod$z, truth = iris$Species)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\nFigure 6.4: Coordinate projection plots created with coordProj() for the variables Sepal.Width and Petal.Width from the iris dataset. Panel (a) shows the three-group model-based clustering, with the associated uncertainty in panel (b) and the classification errors in panel (c).\n\n\n\n\n6.3.2 Random Projections\nIn the previous section, we looked at bivariate marginal coordinate projections that involve selected pairs of variables. Higher dimensional data prove to be much more difficult to visualize. A different approach, discussed in this section, consists of looking at the data from different random perspectives.\n\nTwo-dimensional random projections can be computed and plotted in mclust with randProj().  These projections are obtained by simulating a set of random orthogonal coordinates, each spanning a two-dimensional projection subspace. mclust provides a wrapper function randomOrthogonalMatrix()  for generating orthogonal coordinates from a QR factorization of a matrix whose entries are generated randomly from a normal distribution (Heiberger 1978).\nThe following code produces a \\(4 \\times 2\\) orthogonal matrix \\(Q\\) suitable for projecting the iris data:\n\nnrow(iris)\n## [1] 150\nQ = randomOrthogonalMatrix(ncol(iris[,1:4]), 2)\ndim(Q)\n## [1] 4 2\nQTQ = crossprod(Q)           # equivalently t(Q) %*% Q\nzapsmall(QTQ)                # 2 x 2 identity matrix\n##      [,1] [,2]\n## [1,]    1    0\n## [2,]    0    1\n# projection of iris data onto coordinates Q\nirisProj = as.matrix(iris[,1:4]) %*% Q  \n\nFor a discussion of methods for generating random orthogonal matrices, see Anderson, Olkin, and Underhill (1987).\nThe data \\(\\boldsymbol{X}\\) can be projected onto a subspace spanned by orthogonal coordinates \\(\\boldsymbol{\\beta}\\) by computing \\(\\boldsymbol{X}\\boldsymbol{\\beta}\\). It can be shown that for a given GMM with estimated means \\(\\widehat{\\boldsymbol{\\mu}}_k\\) and covariance matrices \\(\\widehat{\\boldsymbol{\\Sigma}}_k\\) (\\(k=1,\\dots,G\\)), the parameters of the Gaussian components on the projection subspace are \\(\\boldsymbol{\\beta}{}^{\\!\\top}\\widehat{\\boldsymbol{\\mu}}_k\\) and \\(\\boldsymbol{\\beta}{}^{\\!\\top}\\widehat{\\boldsymbol{\\Sigma}}_k \\boldsymbol{\\beta}\\), respectively. Finally, note that the number of two-dimensional projections generated by randProj() depends on the number of values provided through the optional argument seeds.\n\nExample 6.4   Random projections for the iris data\nConsider again the three-group GMM for the iris dataset from the previous subsection. The following code produces four random projections of the iris data with corresponding cluster ellipses and model classification:\nrandProj(data = iris[,1:4], seeds = c(1,13,79,201), \n         what = \"classification\",\n         parameters = mod$parameters, z = mod$z)\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 6.5: Random projection plots, all showing the three-group model-based clustering of the iris dataset. Each plot was created with randProj() with a different seed.\n\n\nThe resulting plots are shown in Figure 6.5. Since the basis of each projection is random, to make the results reproducible we fixed the seeds using the seeds argument in the randProj() function call. If not provided, each call of randProj() will result in a different projection using a system-generated seed.\n\n\n6.3.3 Discriminant Coordinate Projections\nBoth the coordinate and random projection plots decribed above %are intended to display GMMs in different subspaces, without attempting to select coordinates based on any specific visual criteria. By contrast, discriminant coordinates or crimcoords  (Gnanadesikan 1977; Bernard Flury 1997) are specifically designed to reveal group separation, and they can be used both in the case of known groups and of groups identified by a clustering model or algorithm.\nConsider a data matrix \\(\\boldsymbol{X}\\) of dimension \\(n \\times d\\), for \\(n\\) observations on \\(d\\) variables, with an associated group structure composed either by \\(G\\) known classes or estimated clusters. Let \\(\\boldsymbol{\\bar{x}}\\) be the overall sample mean vector, and \\(\\boldsymbol{\\bar{x}}_k\\) and \\(\\boldsymbol{S}_k\\), respectively, the group-specific sample mean vector and sample covariance matrix on \\(n_k\\) observations (\\(k = 1, \\dots, G\\); \\(n = \\sum_{k=1}^G n_k\\)). Then, the \\(d \\times d\\) between-groups covariance matrix \\[\n\\boldsymbol{B}= \\frac{1}{G} \\sum_{k=1}^G n_k (\\boldsymbol{\\bar{x}}_k - \\boldsymbol{\\bar{x}})(\\boldsymbol{\\bar{x}}_k - \\boldsymbol{\\bar{x}}){}^{\\!\\top}\n\\] represents the group separation, while the \\(d \\times d\\) pooled within-groups covariance matrix \\[\n\\boldsymbol{W}= \\frac{1}{n} \\sum_{k=1}^G (n_k - 1) \\boldsymbol{S}_k.\n\\] represents the within-group dispersion.\nFrom the point of view of searching for the maximal separation of groups, the optimal projection subspace is given by the set of linear transformations of the original variables that maximizes the ratio of the between-groups covariance to the pooled within-groups covariance. The basis of this projection subspace is spanned by the eigenvectors \\(\\boldsymbol{v}_i\\) corresponding to the non-zero eigenvalues of the generalized eigenvalue problem: \\[\\begin{align*}\n\\boldsymbol{B}\\boldsymbol{v}_i & = \\gamma_i \\boldsymbol{W}\\boldsymbol{v}_i, \\\\\n\\boldsymbol{v}{}^{\\!\\top}_i \\boldsymbol{W}\\boldsymbol{v}_j & = 1 \\qquad \\text{if $i=j$, and $0$ otherwise.}\n\\end{align*}\\]\nThe discriminant coordinates or crimcoords are computed as \\(\\boldsymbol{X}\\boldsymbol{V}\\), where \\(\\boldsymbol{V}\\equiv [\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_p]\\) is the \\(d \\times p\\) matrix of \\(p = {\\min(d,G-1),r}\\) eigenvectors, where \\(r\\) is the number of non-zero eigenvalues. The directions of the discriminant subspace spanned by \\(\\boldsymbol{V}\\) are in decreasing order of effectiveness in identifying the separation among groups as expressed by the associated eigenvalues, \\(\\gamma_1 \\ge \\gamma_2 \\ge \\dots \\ge \\gamma_p &gt; 0\\).\nAlternatively, unbiased-sample estimates may be used for these computatons: \\(\\frac{G}{G-1}\\boldsymbol{B}\\) for the between-groups covariance and \\(\\frac{n}{n-G}\\boldsymbol{W}\\) for the within-groups covariance. These would change the eigenvalues and eigenvectors by a constant of proportionality, equal to \\(n(G-1)/(G(n-G))\\) and \\(\\sqrt{n/(n-G)}\\), respectively.\nThe method described above is implemented in the function crimcoords()  available in mclust. This requires the arguments data, a matrix or data frame of observed data, and classification, a vector or a factor giving the groups classification, either known class labels or estimated cluster assignments. Optional arguments are numdir, an integer value specifying the number of directions to return (by default all those corresponding to non-zero eigenvalues), and unbiased, a logical value specifying whether unbiased estimates should be used (by default set to FALSE so that MLE estimates are used). summary() and plot() methods are available to show the estimated projections basis and the corresponding data projection.\n\nExample 6.5   Discriminant coordinates for the iris data\nDiscriminant coordinates of the iris data are computed and plotted with the following code:\n\nplot(crimcoords(iris[,1:4], mod$classification))\n\n\n\n\n\n\nFigure 6.6: Discriminant coordinates or crimcoords projection for the clustering for the 3-group model-based clustering of the iris dataset.\n\n\n\n\nwith the resulting plot shown in Figure 6.6.\n\n\nExample 6.6   Discriminant coordinates for the thyroid disease data\nConsider the data introduced in Example 3.2 from the Thyroid Disease Data from the UCI Repository (Dua and Graff 2017). We compute the discriminant coordinates with crimcoords() using the thyroid diagnosis as the classification variable, and then print the computed eigenvectors and eigenvalues using the summary()  function:\n\ndata(\"thyroid\", package = \"mclust\")\nCRIMCOORDS = crimcoords(thyroid[,-1], thyroid$Diagnosis)\nsummary(CRIMCOORDS)\n## ------------------------------------- \n## Discriminant coordinates (crimcoords) \n## ------------------------------------- \n## \n## Estimated basis vectors: \n##      crimcoords1 crimcoords2\n## RT3U   -0.025187  -0.0019885\n## T4      0.307600   0.1042255\n## T3      0.116701   0.4371644\n## TSH    -0.038588   0.1484593\n## DTSH   -0.073398   0.0745668\n## \n##             crimcoords1 crimcoords2\n## Eigenvalues     275.143      52.473\n## Cum. %           83.983     100.000\n\nThe associated plot()  method can then be used for plotting the data points projected onto the discriminant coordinates subspace. In the following code we also add a projection of the corresponding group-centroids:\n\nplot(CRIMCOORDS)\npoints(CRIMCOORDS$means %*% CRIMCOORDS$basis, pch = 3, cex = 1.5, lwd = 2)\nlegend(\"topright\", legend = levels(thyroid$Diagnosis), inset = 0.02,\n       col = mclust.options(\"classPlotColors\")[1:3],\n       pch = mclust.options(\"classPlotSymbols\")[1:3])\n\n\n\n\n\n\nFigure 6.7: Discriminant coordinates or crimcoords projection for the diagnosis classes of the thyroid dataset. Group centroids are represented with the \\(+\\) symbol.\n\n\n\n\nThe resulting plot is shown in Figure 6.7. It is interesting to note that patients with normal thyroid function line up along the left arm, while the remaining patients with hyperthyroidism or hypothyroidism line up along the right arm.\n\nAn alternative method for selecting an informative basis of the projection subspace is discussed in Scrucca and Serafini (2019). They use a projection pursuit approach that maximizes an approximation to the negative entropy for Gaussian mixtures to determine coordinates for display.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing Gaussian Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/06_graphics.html#sec-displaydr",
    "href": "chapters/06_graphics.html#sec-displaydr",
    "title": "6  Visualizing Gaussian Mixture Models",
    "section": "\n6.4 Visualizing Model-Based Clustering and Classification on Projection Subspaces",
    "text": "6.4 Visualizing Model-Based Clustering and Classification on Projection Subspaces\nWhen the number of clustering or classification features is larger than two, the marginal or random projections described in Section 6.3 are not guaranteed to produce displays that reveal the underlying structure of the data. Discriminant coordinates (Section 6.3.3) provide an attempt at showing maximal separation of groups without explicitly assuming any specific model.\nWith the aim of visualizing the clustering structure and geometric characteristics induced by a GMM, Scrucca (2010),Scrucca:2014 proposed a methodology for projecting the data onto subspaces of reduced dimension. These subspaces are spanned by a set of linear combinations of the original variables, called GMMDR directions  (“DR” for Dimension Reduction). They are obtained through a procedure that looks for the smallest subspace that captures the clustering information contained in the data. Thus, the goal is to identify those directions where the cluster means \\(\\boldsymbol{\\mu}_k\\) and/or the cluster covariances \\(\\boldsymbol{\\Sigma}_k\\) vary as much as possible, provided that each direction is orthogonal to the others in a transformed space. These methods are implemented in the mclust functions MclustDR() and MclustDRsubsel().\n\n6.4.1 Projection Subspaces for Visualizing Cluster Separation\nThe variation among the cluster means is captured by the \\(d \\times d\\) matrix \\[\n\\mathbb{\\mathcal{B}}= \\sum_{k=1}^G \\pi_k (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}){}^{\\!\\top},\n\\tag{6.1}\\] where \\(\\boldsymbol{\\mu}= \\sum_{k=1}^G \\pi_k \\boldsymbol{\\mu}_k\\) is the marginal mean vector. The projection subspace associated with this variation is spanned by the eigenvectors \\(\\boldsymbol{v}_i\\) corresponding to the non-zero eigenvalues of the following generalized eigenvalue problem: \\[\n\\begin{align}\n\\mathbb{\\mathcal{K}}_{\\mu}\\boldsymbol{v}_i  &= \\gamma_i \\boldsymbol{\\Sigma}\\boldsymbol{v}_i, \\\\\n\\boldsymbol{v}{}^{\\!\\top}_i \\boldsymbol{\\Sigma}\\boldsymbol{v}_j &= 1 \\qquad \\text{if $i=j$, and $0$ otherwise,} \\nonumber\n\\end{align}\n\\tag{6.2}\\] in which \\[\n\\mathbb{\\mathcal{K}}_{\\mu}= \\mathbb{\\mathcal{B}}\\boldsymbol{\\Sigma}^{-1} \\mathbb{\\mathcal{B}}\n\\tag{6.3}\\] is the kernel matrix associated with the cluster means, and \\[\n\\boldsymbol{\\Sigma}= \\frac{1}{n} \\sum_{i=1}^{n} (\\boldsymbol{x}_i - \\boldsymbol{\\mu})(\\boldsymbol{x}_i - \\boldsymbol{\\mu}){}^{\\!\\top}\n\\tag{6.4}\\] is the marginal covariance matrix. \\(\\boldsymbol{\\Sigma}\\) is symmetric positive definite, and \\(\\mathbb{\\mathcal{K}}_{\\mu}\\) is also symmetric with nonnegative eigenvalues, of which \\(p = \\min(d,G-1)\\) are non-zero. The GMMDR variables \\(\\boldsymbol{X}\\boldsymbol{\\beta}\\) are the projection of the \\(n \\times d\\) matrix \\(\\boldsymbol{X}\\) onto the subspace spanned by \\(\\boldsymbol{\\beta}\\equiv [\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_p]\\). The basis vectors \\(\\boldsymbol{v}_i\\) are orthogonal in the space transformed by \\(\\boldsymbol{\\Sigma}\\).\n\nExample 6.7   Visualizing wine data clustering\nIn this example, we consider the Italian wines from the package gclus (Hurley 2019) data previously discussed in Example 3.3, where the estimated VVE model with three components was selected by BIC. We first obtain the Mclust() model for this data and the confusion matrix for the resulting classification:\n\ndata(\"wine\", package = \"gclus\")\nClass = factor(wine$Class, levels = 1:3,\n                labels = c(\"Barolo\", \"Grignolino\", \"Barbera\"))\nX = data.matrix(wine[,-1])\nmod = Mclust(X, G = 3, modelNames = \"VVE\")\ntable(Class, Cluster = mod$classification)\n##             Cluster\n## Class         1  2  3\n##   Barolo     59  0  0\n##   Grignolino  0 69  2\n##   Barbera     0  0 48\n\nMclustDR() is then applied to obtain a projection subspace:\n\ndrmod = MclustDR(mod, lambda = 1)\nsummary(drmod)\n## ----------------------------------------------------------------- \n## Dimension reduction for model-based clustering and classification \n## ----------------------------------------------------------------- \n## \n## Mixture model type: Mclust (VVE, 3) \n##         \n## Clusters  n\n##        1 59\n##        2 69\n##        3 50\n## \n## Estimated basis vectors: \n##                        Dir1        Dir2\n## Alcohol          0.13399009  0.19209123\n## Malic           -0.03723778  0.06424412\n## Ash             -0.01313103  0.62738796\n## Alcalinity      -0.04299147 -0.03715437\n## Magnesium       -0.00053971  0.00051772\n## Phenols         -0.13507235 -0.04687991\n## Flavanoids       0.51323644 -0.13391186\n## Nonflavanoid     0.68462875 -0.61863302\n## Proanthocyanins -0.07506153 -0.04652587\n## Intensity       -0.08855450  0.04877118\n## Hue              0.28941727 -0.39564601\n## OD280            0.36197696 -0.00779361\n## Proline          0.00070724  0.00075867\n## \n##                Dir1    Dir2\n## Eigenvalues  1.6189   1.292\n## Cum. %      55.6156 100.000\n\nSetting the tuning parameter \\(\\lambda\\) to \\(1\\) (the default in MclustDR()) results in a projection subspace for maximal separation of clusters. Other options that incorporate the variation in covariances are discussed in Section 6.4.2.\nRecall that the basis from Equation 6.2 consists of \\(p = \\min(d, G-1)\\) directions, where \\(d\\) is the number of variables and \\(G\\) the number of mixture components or clusters. In this example, there are \\(d=13\\) features and \\(G=3\\) clusters, so the reduced subspace is two-dimensional. The projected data are shown in Figure 6.8 (a), obtained with the code:\n\nplot(drmod, what = \"contour\")\n\nOn the same subspace we can also plot the uncertainty boundaries corresponding to the MAP classification (see Figure 6.8 (b)):\n\nplot(drmod, what = \"boundaries\")\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 6.8: Contour plot of estimated mixture densities (a) and uncertainty boundaries (b) projected onto the subspace estimated with MclustDR() for the wine dataset.\n\n\n\nAlthough the GMMDR subspaces discussed in this section are not guaranteed to show cluster separation, they do provide useful clustering information in many cases of practical interest. When there are only two clusters, there is just one basis vector and the projections will be univariate.\n\n6.4.2 Incorporating Variation in Covariances\nAnalogous to the scenario for cluster means discussed in the previous section, the variation among the cluster covariances is captured by the \\(d \\times d\\) kernel matrix \\[\n\\mathbb{\\mathcal{K}}_{\\Sigma}= \\sum_{k=1}^G \\pi_k (\\boldsymbol{\\Sigma}_k - \\bar{\\boldsymbol{\\Sigma}}) \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\Sigma}_k - \\bar{\\boldsymbol{\\Sigma}}){}^{\\!\\top},\n\\] where \\[\n\\bar{\\boldsymbol{\\Sigma}} = \\sum_{k=1}^G \\pi_k \\boldsymbol{\\Sigma}_k\n\\] is the pooled within-cluster covariance matrix, and \\(\\boldsymbol{\\Sigma}\\) is the marginal covariance matrix in Equation 6.4.\nThe variation in both means and variances can then be combined, and the associated projection subspace is spanned by the eigenvectors \\(\\boldsymbol{v}_i\\) that solve the generalized eigenvalue problem \\[\n\\begin{align}\n\\big(\\lambda \\mathbb{\\mathcal{K}}_{\\mu}+ (1-\\lambda) \\mathbb{\\mathcal{K}}_{\\Sigma}\\big) \\boldsymbol{v}_i  &= \\gamma_i \\boldsymbol{\\Sigma}\\boldsymbol{v}_i, \\\\\n\\boldsymbol{v}{}^{\\!\\top}_i \\boldsymbol{\\Sigma}\\boldsymbol{v}_j &= 1 \\qquad \\text{if $i=j$, and $0$ otherwise,}\n\\end{align}\n\\tag{6.5}\\] in which \\(\\mathbb{\\mathcal{K}}_{\\mu}\\) is the kernel matrix Equation 6.3 associated with the cluster means in Section 6.4.1, and \\(\\lambda \\in [0,1]\\) is a parameter controlling the relative contributions of the mean and covariance variations (Scrucca 2014).\nFor larger values of \\(\\lambda\\), the estimated directions will tend to focus on differences in location. For \\(\\lambda = 1\\) (the default in MclustDR()), differences in class covariances are ignored, and the most discriminant directions (those that show maximal separation among classes) are recovered (Section 6.4.1). For \\(\\lambda = 0.5\\), the two types of information are equally weighted (Scrucca 2010). When \\(\\lambda &lt; 1\\), the GMMDR variables \\(\\boldsymbol{X}\\boldsymbol{\\beta}\\) are the projection of the \\(n \\times d\\) matrix \\(\\boldsymbol{X}\\) onto the subspace spanned by \\(\\boldsymbol{\\beta}\\equiv [\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_d]\\), which has the same dimension as the data, regardless of the number of GMM components. Note that each generalized eigenvalue \\(\\gamma_i\\) is the sum of contributions from both the means and the variances: \\[\n\\gamma_i  = \\gamma_i \\boldsymbol{v}_i{}^{\\!\\top}\\boldsymbol{\\Sigma}\\boldsymbol{v}_i = \\lambda \\boldsymbol{v}_i{}^{\\!\\top}\\mathbb{\\mathcal{K}}_{\\mu}\\boldsymbol{v}_i +\n(1-\\lambda) \\boldsymbol{v}_i{}^{\\!\\top}\\mathbb{\\mathcal{K}}_{\\Sigma}\\boldsymbol{v}_i, ~~ i = 1, \\dots, d.\n\\]\n\nExample 6.8   Visualizing iris data clustering\nAs an example of the above methodology, consider again the three-group model for the iris dataset described in Section 6.3. Once a GMM model has been fitted, the subspace estimated by MclustDR() can be used for plots that attempt to capture most of the clustering structure:\n\nmod = Mclust(iris[, 1:4], G = 3)\ndrmod = MclustDR(mod, lambda = .5)\nsummary(drmod)\n## ----------------------------------------------------------------- \n## Dimension reduction for model-based clustering and classification \n## ----------------------------------------------------------------- \n## \n## Mixture model type: Mclust (VEV, 3) \n##         \n## Clusters  n\n##        1 50\n##        2 45\n##        3 55\n## \n## Estimated basis vectors: \n##                  Dir1      Dir2     Dir3     Dir4\n## Sepal.Length  0.14546 -0.220270  0.65191 -0.43651\n## Sepal.Width   0.52097  0.097857  0.25527  0.57873\n## Petal.Length -0.62095 -0.293850 -0.44704  0.46137\n## Petal.Width  -0.56732  0.924963  0.55678 -0.51153\n## \n##                 Dir1     Dir2      Dir3       Dir4\n## Eigenvalues  0.94861  0.62354  0.074295   0.032765\n## Cum. %      56.49126 93.62436 98.048774 100.000000\n\nThe basis vectors spanning the reduced subspace are available through the summary() method for MclustDR(). These basis vectors are expressed as linear combinations of the original features, ordered by importance via the associated generalized eigenvalues Equation 6.5. In this case the first two directions account for most of the clustering structure. A plot of the eigenvalues, shown in Figure 6.9, is obtained with the following code:\n\nplot(drmod, what = \"evalues\")\n\n\n\n\n\n\nFigure 6.9: Plot of generalized eigenvalues from MclustDR() and the corresponding contributions from means and variances for the 3-group model-based clustering of the iris dataset.\n\n\n\n\nAccording to the previous discussion on the contributions to the generalized eigenvalues, we would expect to see a separation among the groups along the first two directions only, with the first associated with differences in location, and the second associated with differences in spread. This is confirmed by a scatterplot matrix of data projected onto the estimated subspace (see Figure 6.10) obtained as follows:\n\nplot(drmod, what = \"pairs\")\n\n\n\n\n\n\nFigure 6.10: The iris data projected onto the principal eigenvectors from MclustDR(). The colors and symbols correspond to the 3-group model-based clustering.\n\n\n\n\nAs already mentioned, the last two directions appear negligible in this example. This may be further investigated by applying the greedy subset selection step proposed in Scrucca (2010, sec. 3) that applies the subset selection method of Raftery and Dean (2006) to prune the subset of GMMDR features: \n\nsdrmod = MclustDRsubsel(drmod, verbose = TRUE)\n## \n## Cycle 1 ...\n## \n##   Variable Model G     BIC BIC.dif\n## 1     Dir1     E 3 -374.60 187.035\n## 2     Dir2   EEI 4 -315.66  45.242\n## 3     Dir3   VVE 3 -395.01  37.666\n## \n## Cycle 2 ...\n## \n##   Variable Model G     BIC BIC.dif\n## 1     Dir1     E 3 -354.64 186.167\n## 2     Dir2   VVI 3 -294.07  47.343\n## 3     Dir3   VVE 3 -389.47  21.632\nsummary(sdrmod)\n## ----------------------------------------------------------------- \n## Dimension reduction for model-based clustering and classification \n## ----------------------------------------------------------------- \n## \n## Mixture model type: Mclust (VVE, 3) \n##         \n## Clusters  n\n##        1 50\n##        2 49\n##        3 51\n## \n## Estimated basis vectors: \n##                  Dir1     Dir2     Dir3\n## Sepal.Length  0.10835 -0.18789  0.71035\n## Sepal.Width   0.54164  0.10527  0.25902\n## Petal.Length -0.65930 -0.30454 -0.43979\n## Petal.Width  -0.51010  0.92783  0.48465\n## \n##                 Dir1     Dir2      Dir3\n## Eigenvalues  0.94257  0.64868   0.03395\n## Cum. %      57.99696 97.91101 100.00000\n\nThe subset selection chooses three directions and essentially the first three previously obtained:\n\nzapsmall(cor(drmod$dir, sdrmod$dir)^2)\n##         Dir1    Dir2    Dir3\n## Dir1 0.99975 0.00020 0.00006\n## Dir2 0.00021 0.99537 0.00443\n## Dir3 0.00004 0.00444 0.99552\n## Dir4 0.00000 0.00000 0.00000\n\nIn the following plots, we use the default argument dimens = c(1, 2) to project onto the first two GMMDR directions, which are of most interest for visualizing the clustering structure:\n\nplot(sdrmod, what = \"contour\", nlevels = 7)\nplot(sdrmod, what = \"classification\")\nplot(sdrmod, what = \"boundaries\")\n\nThe first command draws a bivariate contour plot of the mixture component densities, the second command draws the classification boundaries based on the MAP principle, while the last command draws a scatterplot showing uncertainty boundaries (panels (a), (b), and (c) in Figure 6.11).\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 6.11: Contour plot of mixture densities for each cluster (a), classification regions (b), and uncertainty boundaries (c) drawn on the first two GMMDR directions of the projection subspace estimated with MclustDRsubsel() for the 3-group model-based clustering of the iris dataset. Panel (d) shows the conditional densities along the first GMMDR direction.\n\n\nWe can also produce a plot of the densities conditional on the estimated cluster membership for the first GMMDR direction as follows (panel (d) in Figure 6.11):\n\nplot(sdrmod, what = \"density\")\n\nOther plots can be obtained via the argument what, and fine tuning for some parameters is also available; see help(\"plot.MclustDR\") for a comprehensive list and examples.\n\n\n6.4.3 Projection Subspaces for Classification\nThe approach discussed in the previous sections for clustering has been further extended to the case of supervised classification (Scrucca 2014). In this scenario, the fact that classes are known and that they can be made up of one or more mixture components must be taken into account.\n\nExample 6.9   Visualizing Swiss banknote classification\nThe banknote dataset containing six physical measurements of a sample of Swiss Franc bills is given in Tables 1.1 and 1.2 of Bernhard Flury and Riedwyl (1988). One hundred banknotes were classified as genuine and 100 as counterfeits.\nWe fit a MclustDA() classification model to this data:\n\ndata(\"banknote\", package = \"mclust\")\nmod = MclustDA(data = banknote[, -1], class = banknote$Status)\nsummary(mod)\n## ------------------------------------------------ \n## Gaussian finite mixture model for classification \n## ------------------------------------------------ \n## \n## MclustDA model summary: \n## \n##  log-likelihood   n df     BIC\n##         -646.08 200 67 -1647.1\n##              \n## Classes         n  % Model G\n##   counterfeit 100 50   EVE 2\n##   genuine     100 50   XXX 1\n## \n## Training confusion matrix:\n##              Predicted\n## Class         counterfeit genuine\n##   counterfeit         100       0\n##   genuine               0     100\n## Classification error = 0 \n## Brier score          = 0\n\nWe then apply MclustDR() to obtain a projection subspace:\n\ndrmod = MclustDR(mod, lambda = 0.5)\nsummary(drmod)\n## ----------------------------------------------------------------- \n## Dimension reduction for model-based clustering and classification \n## ----------------------------------------------------------------- \n## \n## Mixture model type: MclustDA \n##              \n## Classes         n Model G\n##   counterfeit 100   EVE 2\n##   genuine     100   XXX 1\n## \n## Estimated basis vectors: \n##              Dir1      Dir2      Dir3      Dir4       Dir5      Dir6\n## Length   -0.10139 -0.328225 -0.797068  0.033629 -0.3174275  0.085062\n## Left     -0.21718 -0.305014  0.303111  0.893349  0.3700659 -0.565410\n## Right     0.29222 -0.018401  0.495891 -0.407413 -0.8612986  0.480799\n## Bottom    0.57591  0.445352 -0.120173  0.034595  0.0043174 -0.078640\n## Top       0.57542  0.385535 -0.100865  0.103623  0.1359128  0.625902\n## Diagonal -0.44089  0.672250  0.047784  0.151252 -0.0443255  0.209691\n## \n##                 Dir1     Dir2     Dir3     Dir4      Dir5       Dir6\n## Eigenvalues  0.87242  0.55373  0.48546  0.13291  0.053075   0.027273\n## Cum. %      41.05755 67.11689 89.96377 96.21866 98.716489 100.000000\n\nA plot of the generalized eigenvalues associated with the estimated directions is obtained using:\n\nplot(drmod, what = \"evalues\")\n\n\n\n\n\n\nFigure 6.12: Plot of generalized eigenvalues from MclustDR() applied to a classification model for the banknote data.\n\n\n\n\nThe graph in Figure 6.12 suggests that the first two directions mainly contain information on class separation, with the remaining dimensions showing differences in variances.\nA pairs plot of the data points projected along these directions seems to suggest that a further reduction could be achieved by selecting a subset of them (see Figure 6.13):\n\nplot(drmod, what = \"pairs\", lower.panel = NULL)\nclPairsLegend(0.1, 0.4, class = levels(drmod$classification), \n              col = mclust.options(\"classPlotColors\")[1:2],\n              pch = mclust.options(\"classPlotSymbols\")[1:2],\n              title = \"Swiss banknote data\")\n\n\n\n\n\n\nFigure 6.13: Pairs plot of points projected onto the directions estimated with MclustDR() for the banknote dataset.\n\n\n\n\nThis option can be investigated by applying the subset selection procedure described in Scrucca (2010) with the code:\n\nsdrmod = MclustDRsubsel(drmod, verbose = TRUE)\n## \n## Cycle 1 ...\n## \n##   Variable   Model   G     BIC BIC.dif\n## 1     Dir1     E|X 2|1 -527.10  206.74\n## 2     Dir2 EEE|XXX 2|1 -667.69  192.65\n## \n## Cycle 2 ...\n## \n##   Variable   Model   G     BIC BIC.dif\n## 1     Dir1     E|X 2|1 -528.93  204.88\n## 2     Dir2 EEE|XXX 2|1 -667.66  194.51\nsummary(sdrmod)\n## ----------------------------------------------------------------- \n## Dimension reduction for model-based clustering and classification \n## ----------------------------------------------------------------- \n## \n## Mixture model type: MclustDA \n##              \n## Classes         n Model G\n##   counterfeit 100   EEE 2\n##   genuine     100   XXX 1\n## \n## Estimated basis vectors: \n##              Dir1      Dir2\n## Length   -0.10552 -0.328204\n## Left     -0.22054 -0.304765\n## Right     0.29083 -0.018957\n## Bottom    0.57981  0.444505\n## Top       0.57850  0.384658\n## Diagonal -0.42989  0.673420\n## \n##                 Dir1      Dir2\n## Eigenvalues  0.85929   0.53687\n## Cum. %      61.54666 100.00000\n\nThe subset selection procedure identifies only two directions as being important, essentially the first two directions previously obtained:\n\nzapsmall(cor(drmod$dir, sdrmod$dir)^2)\n##         Dir1    Dir2\n## Dir1 0.99997 0.00003\n## Dir2 0.00003 0.99997\n## Dir3 0.00000 0.00000\n## Dir4 0.00000 0.00000\n## Dir5 0.00000 0.00000\n## Dir6 0.00000 0.00000\n\nSummary plots can then be obtained on the 2-dimensional estimated subspace as follows:\nplot(sdrmod, what = \"contour\", nlevels = 15)\nplot(sdrmod, what = \"classification\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 6.14: Contour plot of mixture densities for each class (a) and MAP classification regions (b) drawn on the projection subspace estimated with MclustDRsubsel() for the banknote dataset.\n\n\nThese plots show, respectively, a contour plot of the mixture component densities for each class, and the classification regions based on the MAP principle (see Figure 6.14). The group of counterfeit banknotes is clearly composed of two distinct subgroups, whereas the genuine banknotes appear as a homogeneous group with the presence of an outlying note.\n\n\n6.4.4 Relationship to Other Methods\nApproaches analogous to projection subspaces have been proposed for the cases of finite mixtures of multivariate \\(t\\) distributions (Morris, McNicholas, and Scrucca 2013), mixtures of shifted asymmetric Laplace distributions (Morris and McNicholas 2013), and generalized hyperbolic mixtures (Morris and McNicholas 2016).\nThe MclustDR() directions, obtained using the generalized eigen-decompositions discussed in Section 6.4.1 and in Section 6.4.2 for the specific case \\(\\lambda = 1\\) in Equation 6.5, are essentially equivalent to the discriminant coordinates or crimcoords described in Section 6.3.3 for GMMs with one mixture component per group. Within mclust, this applies to Mclust() clustering models, EDDA classification models, and MclustDA classification models with one component (but possibly different covariance structures) per class.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing Gaussian Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/06_graphics.html#using-ggplot2-with-mclust",
    "href": "chapters/06_graphics.html#using-ggplot2-with-mclust",
    "title": "6  Visualizing Gaussian Mixture Models",
    "section": "\n6.5 Using ggplot2 with mclust",
    "text": "6.5 Using ggplot2 with mclust\nAll of the plots produced by mclust and discussed so far use the base plotting system in R. There are historical reasons for this, but it also helps achieve one of the main goals of the package, namely to provide simple, fast, accurate, and nice-looking plots without introducing further dependencies on other packages.\nggplot2  is a popular R package for data visualization based on the Grammar of Graphics (Wilkinson 2005). The ggplot2 package facilitates creation of production-quality statistical graphics. For a comprehensive introduction, see Wickham (2016). Experienced R users can easily produce ggplot2 plots using the information contained in the objects returned by mclust functions, by collecting them in a data frame and then (gg)plotting them using a suitable plot\n\nExample 6.10   ggplot2 graphs of Old Faithful data clustering\nAs a first example, consider the following code which produces the usual classification plot (see Figure 6.15):\n\nmod = Mclust(faithful)\nDF = data.frame(mod$data, cluster = factor(mod$classification))\nlibrary(\"ggplot2\")\nggplot(DF, aes(x = eruptions, y = waiting, \n               colour = cluster, shape = cluster)) +\n  geom_point()\n\n\n\n\n\n\nFigure 6.15: Scatterplot of the faithful data with points marked according to the GMM clusters identified by Mclust.\n\n\n\n\nA more complex example involves plotting the table of BIC values (see Figure 6.16). In this case, we need to convert the object into a data.frame (or a tibble in tidyverse — see Wickham et al. (2019)) by reshaping it from the “wide” to the “long” format. The latter step can be accomplished in several ways; here, we use the tidyr R package (Wickham and Henry 2022):\n\nlibrary(\"tidyr\")\nDF = data.frame(mod$BIC[], G = 1:nrow(mod$BIC))\nDF = pivot_longer(DF, cols = 1:14, names_to = \"Model\", values_to = \"BIC\")\nDF$Model = factor(DF$Model, levels = mclust.options(\"emModelNames\"))\nggplot(DF, aes(x = G, y = BIC, colour = Model, shape = Model)) +\n  geom_point() + \n  geom_line() +\n  scale_shape_manual(values = mclust.options(\"bicPlotSymbols\")) +\n  scale_color_manual(values = mclust.options(\"bicPlotColors\")) +\n  scale_x_continuous(breaks = unique(DF$G)) +\n  xlab(\"Number of mixture components\") +\n  guides(shape = guide_legend(ncol=2))\n\n\n\n\n\n\nFigure 6.16: BIC traces for the GMMs estimated for the faithful data.\n\n\n\n\nAs another example, we may draw a latent profiles plot of estimated means for each variable by cluster membership. Consider the following clustering model for the iris data:\n\nmod = Mclust(iris[, 1:4], G = 3)\n\nA latent profiles plot can be obtained by extracting the component means, reshaping it in a “long” format, and then drawing the desired plot shown in Figure 6.17:\n\nmeans = data.frame(Profile = 1:mod$G, t(mod$parameters$mean))\nmeans = pivot_longer(means, cols = -1, \n                      names_to = \"Variable\",\n                      values_to = \"Mean\")\nmeans$Profile  = factor(means$Profile)\nmeans$Variable = factor(means$Variable, \n                         levels = rownames(mod$parameters$mean))\nmeans\n## # A tibble: 12 × 3\n##    Profile Variable      Mean\n##    &lt;fct&gt;   &lt;fct&gt;        &lt;dbl&gt;\n##  1 1       Sepal.Length 5.01 \n##  2 1       Sepal.Width  3.43 \n##  3 1       Petal.Length 1.46 \n##  4 1       Petal.Width  0.246\n##  5 2       Sepal.Length 5.92 \n##  6 2       Sepal.Width  2.78 \n##  7 2       Petal.Length 4.20 \n##  8 2       Petal.Width  1.30 \n##  9 3       Sepal.Length 6.55 \n## 10 3       Sepal.Width  2.95 \n## 11 3       Petal.Length 5.48 \n## 12 3       Petal.Width  1.99\n\nggplot(means, aes(Variable, Mean, group = Profile, \n                  shape = Profile, color = Profile)) +\n  geom_point(size = 2) +\n  geom_line() +\n  labs(x = NULL, y = \"Latent profiles means\") +\n  scale_color_manual(values = mclust.options(\"classPlotColors\")) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1), \n        legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 6.17: Latent profiles plot for the (VEV,3) model estimated for the iris data.\n\n\n\n\nNote that for a plot like the one in Figure 6.17 to make sense, all variables must be expressed in the same unit of measurement (as in the example above), or else they must be scaled to a common unit, for instance by a preliminary standardization. Ordering of variables along the \\(x\\)-axis is arbitrary, so a user must carefully choose the most appropriate order for the data under study.\n\n\nExample 6.11   ggplot2 graphs of iris data classification\nThe plotting facilities of ggplot2 can be used with any mclust model. For instance, the following code estimates a classification model for the iris data, extracts the first two GMMDR directions using the methodology described in Section 6.4, and then produces a two-dimensional scatterplot with added convex hulls for the various iris classes:\n\ndamod = MclustDA(iris[, 1:4], iris$Species)\ndrmod = MclustDR(damod)\nDF1 = data.frame(drmod$dir[, 1:2], class = damod$class)\nDF2 = do.call(\"rbind\", by(DF1, DF1[, 3], \n                           function(x) x[chull(x), ]))\nggplot() + \n  geom_point(data = DF1, \n             aes(x = Dir1, y = Dir2, color = class, shape = class)) + \n  geom_polygon(data = DF2, \n               aes(x = Dir1, y = Dir2, fill = class), \n               alpha = 0.3) +\n  scale_color_manual(values = mclust.options(\"classPlotColors\")) +\n  scale_fill_manual(values = mclust.options(\"classPlotColors\")) +\n  scale_shape_manual(values = mclust.options(\"classPlotSymbols\"))\n\n\n\n\n\n\nFigure 6.18: Scatterplot of the first two GMMDR directions with added convex hulls for the iris data classes.\n\n\n\n\nThe resulting plot is shown in Figure 6.18. Notice that in the code above we created two different data frames, one used for plotting points and one containing the vertices of the convex hull, the latter computed using the chull() function in base R.\n\n\nExample 6.12   ggplot2 graph of density estimate of the waiting time from the Old Faithful data\nNow consider the case of plotting a univariate density estimate obtained via densityMclust(). The following code draws a histogram of waiting times from the faithful data frame, then adds the estimated density. The latter is obtained by first creating a data frame x of equispaced grid points and the corresponding densities computed using the predict method associated with the densityMclust object. The resulting data frame is then used in geom_line() to draw the density estimate.\n\nmod = densityMclust(faithful$waiting, plot = FALSE)\nx = extendrange(faithful$waiting, f = 0.1)\nx = seq(x[1], x[2], length.out = 101)\npred = data.frame(x, density = predict(mod, newdata = x))\nggplot(faithful, aes(waiting)) +\n  geom_histogram(aes(y = stat(density)), bins = 15, \n                 fill = \"slategray3\", colour = \"grey92\") +\n  geom_line(data = pred, aes(x, density))\n\n\n\n\n\n\nFigure 6.19: Plot of histogram and density estimated by densityMclust() for the waiting time of the faithful data.\n\n\n\n\n\n\nExample 6.13   ggplot2 graphs of bootstrap distributions for the hemophilia data\nA final example involves “faceting”, an efficient technique for presenting information in panels conditioning on one or more variables. Consider the bootstrap procedure discussed in Example 3.7, where the MclustBootstrap() function is applied to the two-component VVV model for the hemophilia data from the rrcov package (Todorov 2022). Using the information returned and stored in boot, a ggplot2 plot of the bootstrap distribution for the mixing proportions can be obtained as follows:\n\ndata(\"hemophilia\", package = \"rrcov\")\nX = hemophilia[, 1:2]\nmod = Mclust(X, G = 2, modelName = \"VVV\")\nboot = MclustBootstrap(mod, nboot = 999, type = \"bs\")\nDF = data.frame(mixcomp = rep(1:boot$G, each = boot$nboot), \n                pro = as.vector(boot$pro))\nggplot(DF, aes(x = pro)) + \n  geom_histogram(aes(y = stat(density)), bins = 15, \n                 fill = \"slategray3\", colour = \"grey92\") +\n  facet_grid(~ mixcomp) +\n  xlab(\"Mixing proportions\") +\n  ylab(\"Density of bootstrap distribution\") \n\n\n\n\n\n\nFigure 6.20: Bootstrap distribution for the mixture proportions of (VVV,2) model fitted to the hemophilia data.\n\n\n\n\nThe resulting graph is shown in Figure 6.20. Note that the code above uses facet_grid() to obtain a panel for each of the bootstrap distributions.\nThe same function can also be used for conditioning on more than one variable. For instance, the following code produces Figure 6.21 showing the bootstrap distribution conditioning on both the mixture components (along the rows) and the variables (along the columns):\n\nDF = rbind(\n  data.frame(\"mixcomp\"  = 1,\n             \"variable\" = rep(colnames(boot$mean[, , 1]), \n                              each = dim(boot$mean)[1]),\n             \"mean\"     = as.vector(boot$mean[, , 1])),\n  data.frame(\"mixcomp\"  = 2,\n             \"variable\" = rep(colnames(boot$mean[, , 2]), \n                              each = dim(boot$mean)[1]),\n             \"mean\"     = as.vector(boot$mean[, , 2])))\nggplot(DF, aes(x = mean)) +\n   geom_histogram(aes(y = stat(density)), bins = 15,\n                  fill = \"slategray3\", colour = \"grey92\") +\n   facet_grid(mixcomp ~ variable, scales = \"free_x\") +\n   xlab(\"Means of mixture\") +\n   ylab(\"Density of bootstrap distribution\")\n\n\n\n\n\n\nFigure 6.21: Bootstrap distribution for the mixture component means of (VVV,2) model fitted to the hemophilia data.\n\n\n\n\nAs there are a great many possible options for customizing plots (such as themes and aesthetics), each user may want to experiment to get a suitable plot.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing Gaussian Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/06_graphics.html#sec-colorblind",
    "href": "chapters/06_graphics.html#sec-colorblind",
    "title": "6  Visualizing Gaussian Mixture Models",
    "section": "\n6.6 Using Color-Blind-Friendly Palettes",
    "text": "6.6 Using Color-Blind-Friendly Palettes\nMost of the plots produced by mclust use colors that by default are defined by the following options:\n\nmclust.options(\"bicPlotColors\")\n##       EII       VII       EEI       EVI       VEI       VVI       EEE \n##    \"gray\"   \"black\" \"#218B21\" \"#41884F\" \"#508476\" \"#58819C\" \"#597DC3\" \n##       VEE       EVE       VVE       EEV       VEV       EVV       VVV \n## \"#5178EA\" \"#716EE7\" \"#9B60B8\" \"#B2508B\" \"#C03F60\" \"#C82A36\" \"#CC0000\" \n##         E         V \n##    \"gray\"   \"black\"\nmclust.options(\"classPlotColors\")\n##  [1] \"dodgerblue2\"    \"red3\"           \"green3\"         \"slateblue\"     \n##  [5] \"darkorange\"     \"skyblue1\"       \"violetred4\"     \"forestgreen\"   \n##  [9] \"steelblue4\"     \"slategrey\"      \"brown\"          \"black\"         \n## [13] \"darkseagreen\"   \"darkgoldenrod3\" \"olivedrab\"      \"royalblue\"     \n## [17] \"tomato4\"        \"cyan2\"          \"springgreen2\"\n\nThe first option controls the colors to be used for plotting the BIC, ICL, and similar curves, whereas the second option is used to assign colors associated with clusters or classes when plotting data. These colors have been chosen to facilitate visual interpretation of the plots. However, for color-blind individuals, plots with these default colors may be problematic.\nStarting with R version 4.0, the function palette.colors() can be used for retrieving colors from some pre-defined palettes.  For instance\n\npalette.colors(palette = \"Okabe-Ito\")\n## [1] \"#000000\" \"#E69F00\" \"#56B4E9\" \"#009E73\" \"#F0E442\" \"#0072B2\"\n## [7] \"#D55E00\" \"#CC79A7\" \"#999999\"\n\nreturns a color-blind-friendly palette proposed by Okabe and Ito (2008) for individuals suffering from protanopia or deuteranopia, the two most common forms of inherited color blindness; see also Wong (2011).\n\nExample 6.14   Using color-blind-friendly palette in mclust for the iris data\nA palette suitable for color vision deficiencies can thus be defined and used as the default in mclust with the following code:\n\n# get and save default palettes\nbicPlotColors = mclust.options(\"bicPlotColors\")\nclassPlotColors = mclust.options(\"classPlotColors\")\n# set Okabe-Ito palette for use in mclust\nbicPlotColors_Okabe_Ito = palette.colors(palette = \"Okabe-Ito\")[c(9,1,2:8,2:6,9,1)]\nnames(bicPlotColors_Okabe_Ito) = names(bicPlotColors)\nclassPlotColorsWong = palette.colors(palette = \"Okabe-Ito\")[-1]\nmclust.options(\"bicPlotColors\" = bicPlotColors_Okabe_Ito)\nmclust.options(\"classPlotColors\" = classPlotColorsWong)\n\nAll of the plots subsequently produced by mclust functions will use this palette. For instance, the following code produces the plots in Figure 6.22.\nmod = Mclust(iris[,3:4])\nplot(mod, what = \"BIC\")\nplot(mod, what = \"classification\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 6.22: mclust plots with a color-blind-friendly palette.\n\n\nTo restore the default mclust palettes use:\n\nmclust.options(\"bicPlotColors\" = bicPlotColors)\nmclust.options(\"classPlotColors\" = classPlotColors)\n\n\nFor more advanced treatment of color issues, the package colorspace (Zeileis et al. 2020; Ihaka et al. 2022) is available for manipulating and assessing colors and palettes in R.\n\n\n\n\nAnderson, T. W., I. Olkin, and L. G. Underhill. 1987. “Generation of Random Orthogonal Matrices.” SIAM Journal on Scientific and Statistical Computing 8 (4): 625–29.\n\n\nDua, Dheeru, and Casey Graff. 2017. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. http://archive.ics.uci.edu/ml.\n\n\nFlury, Bernard. 1997. A First Course in Multivariate Statistics. New York: Springer.\n\n\nFlury, Bernhard, and Hans Riedwyl. 1988. Multivariate Statistics: A Practical Approach. Chapman & Hall Ltd.\n\n\nGnanadesikan, R. 1977. Methods for Statistical Data Analysis of Multivariate Observations. New York: John Wiley & Sons.\n\n\nHeiberger, Richard M. 1978. “Algorithm AS 127: Generation of Random Orthogonal Matrices.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 27 (2): 199–206.\n\n\nHurley, Catherine. 2019. gclus: Clustering Graphics. https://CRAN.R-project.org/package=gclus.\n\n\nIhaka, Ross, Paul Murrell, Kurt Hornik, Jason C. Fisher, Reto Stauffer, Claus O. Wilke, Claire D. McWhite, and Achim Zeileis. 2022. Colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes. https://doi.org/10.18637/jss.v096.i01.\n\n\nMorris, Katherine, and Paul D. McNicholas. 2013. “Dimension Reduction for Model-Based Clustering via Mixtures of Shifted Asymmetric Laplace Distributions.” Statistics & Probability Letters 83 (9): 2088–93. https://doi.org/http://dx.doi.org/10.1016/j.spl.2013.04.011.\n\n\n———. 2016. “Clustering, Classification, Discriminant Analysis, and Dimension Reduction via Generalized Hyperbolic Mixtures.” Computational Statistics & Data Analysis 97: 133–50. https://doi.org/http://dx.doi.org/10.1016/j.csda.2015.10.008.\n\n\nMorris, Katherine, PaulD. McNicholas, and Luca Scrucca. 2013. “Dimension Reduction for Model-Based Clustering via Mixtures of Multivariate t-Distributions.” Advances in Data Analysis and Classification 7 (3): 321–38. https://doi.org/10.1007/s11634-013-0137-3.\n\n\nOkabe, Masataka, and Kei Ito. 2008. “Color Universal Design (CUD) - How to Make Figures and Presentations That Are Friendly to Colorblind People.” J* Fly: Data Depository for Drosophila Researchers. https://jfly.uni-koeln.de/color/.\n\n\nRaftery, Adrian E., and N. Dean. 2006. “Variable Selection for Model-Based Clustering.” Journal of the American Statistical Association 101 (473): 168–78.\n\n\nScrucca, Luca. 2010. “Dimension Reduction for Model-Based Clustering.” Statistics and Computing 20 (4): 471–84. https://doi.org/10.1007/s11222-009-9138-7.\n\n\n———. 2014. “Graphical Tools for Model-Based Mixture Discriminant Analysis.” Advances in Data Analysis and Classification 8 (2): 147–65. https://doi.org/10.1007/s11634-013-0147-1.\n\n\nScrucca, Luca, and Alessio Serafini. 2019. “Projection Pursuit Based on Gaussian Mixtures and Evolutionary Algorithms.” Journal of Computational and Graphical Statistics 28 (4): 847–60. https://doi.org/10.1080/10618600.2019.1598871.\n\n\nTodorov, Valentin. 2022. rrcov: Scalable Robust Estimators with High Breakdown Point. https://CRAN.R-project.org/package=rrcov.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. 2nd ed. New York: Springer-Verlag. http://ggplot2.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain Francois, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Lionel Henry. 2022. tidyr: Tidy Messy Data. https://doi.org/10.18637/jss.v059.i10.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. New York: Springer-Verlag. https://doi.org/10.1007/0-387-28695-0.\n\n\nWong, Bang. 2011. “Points of View: Color Blindness.” Nature Methods 8 (441). https://doi.org/0.1038/nmeth.1618.\n\n\nZeileis, Achim, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D. McWhite, Paul Murrell, Reto Stauffer, and Claus O. Wilke. 2020. “colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes.” Journal of Statistical Software 96 (1): 1–49. https://doi.org/10.18637/jss.v096.i01.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Visualizing Gaussian Mixture Models</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html",
    "href": "chapters/07_miscellanea.html",
    "title": "7  Miscellanea",
    "section": "",
    "text": "7.1 Accounting for Noise and Outliers\nIn the finite mixture framework, noisy data are characterized by the presence of outlying observations that do not belong to any mixture component. There are two main ways to accommodate noise and outliers in a mixture model:\nSome other alternatives for robust Gaussian mixture modeling are proposed in Garcı́a-Escudero et al. (2008), Punzo and McNicholas (2016), Coretto and Hennig (2016), and Dotto and Farcomeni (2019).\nIn mclust, the strategy for accommodating noise is to include a constant–rate Poisson process mixture component to represent the noise,  resulting in the following mixture log-likelihood \\[\n\\ell(\\boldsymbol{\\Psi}, \\pi_0) = \\sum_{i=1}^n \\log\n  \\left\\{ \\frac{\\pi_0}{V} +\n          \\sum_{k=1}^{G} \\pi_k \\phi (\\boldsymbol{x}_i \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n\\right\\},\n\\tag{7.1}\\] where \\(V\\) is the hypervolume of the data region, and \\(\\pi_k \\ge 0\\) are the mixing weights under the constraint \\(\\sum_{k=0}^{G} \\pi_k = 1\\). An observation contributes \\(1/V\\) to the likelihood if it belongs to the noise component; otherwise its contribution comes from the Gaussian components. This model has been used successfully in a number of applications (Banfield and Raftery 1993; Dasgupta and Raftery 1998; J. Campbell et al. 1997; J. G. Campbell et al. 1999).\nThe model-fitting procedure is as follows. Given a preliminary noise assignment for the observations, model-based hierarchical clustering is applied to the correspondingly denoised data to obtain initial clustering partitions for the Gaussian portion of the mixture model. EM is then initialized, using the preliminary noise, together with the hierarchical clustering assignment for each number of clusters specified.\nThe effectiveness of this approach hinges on obtaining a good initial specification of the noise. Some possible strategies for initial denoising include methods based on Voronoï tessellation (Allard and Fraley 1997), nearest neighbors (Byers and Raftery 1998), and robust covariance estimation (Wang and Raftery 2002).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html#sec-noise_outliers",
    "href": "chapters/07_miscellanea.html#sec-noise_outliers",
    "title": "7  Miscellanea",
    "section": "",
    "text": "Adding one or more components to the mixture to represent noise and modifying the EM algorithm accordingly to estimate parameters (Dasgupta and Raftery 1998; C. Fraley and Raftery 1998).\nModeling by mixtures of distributions with heavier tails than the Gaussian, such as a mixture of \\(t\\) distributions (McLachlan and Peel 2000).\n\n\n\n\n\n\nExample 7.1   Clustering with noisy minefields data\nDasgupta and Raftery (1998) discussed the problem of detecting surface-laid minefields from an aircraft image. The main goal was to distinguish the mines from the clutter (metal objects or rocks). As an example, consider the simulated minefield data with clutter analyzed in C. Fraley and Raftery (1998) and available in mclust.\ndata(\"chevron\", package = \"mclust\")\nsummary(chevron)\n##    class           x                y         \n##  data :350   Min.   :  1.18   Min.   :  1.01  \n##  noise:754   1st Qu.: 34.39   1st Qu.: 27.51  \n##              Median : 58.82   Median : 49.50  \n##              Mean   : 61.09   Mean   : 56.71  \n##              3rd Qu.: 86.52   3rd Qu.: 86.67  \n##              Max.   :127.49   Max.   :127.86\nnoise = with(chevron, class == \"noise\")\nX = chevron[,2:3]\nplot(X, cex = 0.5)\nplot(X, cex = 0.5, col = ifelse(noise, \"grey\", \"black\"), \n     pch = ifelse(noise, 3, 1))\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 7.1: Simulated minefield data (a), marked to distinguish the known noise (b).\n\n\nThe plot of simulated minefield data is shown in panel (a) of Figure 7.1, while panel (b) displays the dataset with noise (clutter) marked by a grey cross. Note that about 70% of the data points are clutter, and the true minefield is contained within the chevron-shaped area, consisting of two intersecting rectangles.\nTo include noise in modeling with Mclust() or mclustBIC(), an initial guess of the noise observations must be supplied via the noise component of the initialization argument.  The function NNclean() in the R package prabclus (Hennig and Hausdorf 2020) is an implementation of the methodology proposed by Byers and Raftery (1998). Under the assumption that the noise is distributed as a homogeneous Poisson point process and that the remaining data is also distributed as a homogeneous Poisson process but with higher intensity on a (possibly disconnected) subregion, the observed \\(k\\)th nearest neighbor distances are modeled as a mixture of two transformed Gamma distributions with parameters estimated by the EM algorithm. The observed points can then be assigned to either the noise or higher-density component depending on the estimated posterior probabilities.\n\nlibrary(\"prabclus\")\nnnc = NNclean(X, k = 5)\ntable(nnc$z)\n## \n##   0   1 \n## 662 442\nclPairs(X, nnc$z, colors = c(\"darkgrey\", \"black\"), symbols = c(3, 1))\n\nThis example uses \\(k=5\\) nearest neighbors to detect the noise, which amounts to \\(662/1104 \\approx 60\\%\\) of the observed data points. The resulting denoised data is shown in Figure 7.2 (a)}. With this initial estimate of the noise, a GMM is fitted with the following code:\n\nmodNoise = Mclust(X, initialization = list(noise = (nnc$z == 0)))\nsummary(modNoise$BIC)\n## Best BIC values:\n##           EEV,2       EVE,2       EVV,2\n## BIC      -21128 -21128.2301 -21134.7385\n## BIC diff      0     -0.4463     -6.9546\n\nThe model-based clustering procedure with the noise component selects the (EEV,2) model, closely followed by the (EVE,2) model. In both cases two clusters are correctly identified. A summary of the estimated model with the highest BIC and the corresponding classification plot are obtained as follows:\n\nsummary(modNoise, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEV (ellipsoidal, equal volume and shape) model with 2\n## components and a noise term: \n## \n##  log-likelihood    n df    BIC    ICL\n##          -10525 1104 11 -21128 -21507\n## \n## Clustering table:\n##   1   2   0 \n## 160 151 793 \n## \n## Mixing probabilities:\n##       1       2       0 \n## 0.12290 0.12618 0.75091 \n## \n## Means:\n##     [,1]   [,2]\n## x 71.473 39.519\n## y 36.310 31.566\n## \n## Variances:\n## [,,1]\n##         x       y\n## x  195.85 -176.99\n## y -176.99  197.30\n## [,,2]\n##        x      y\n## x 183.00 176.47\n## y 176.47 210.15\n## \n## Hypervolume of noise component:\n## 16022\naddmargins(table(chevron$class, modNoise$classification), 2)\n##        \n##           0   1   2 Sum\n##   data   50 156 144 350\n##   noise 743   4   7 754\nplot(modNoise, what = \"classification\")\n\nThe summary output consists of the usual statistics for the selected GMM, plus an estimate of the hypervolume associated with the noise component. The classification obtained is shown in Figure 7.2, which reflects the generating process for the data quite well. The cross-tabulation of the estimated classification and the known data labels shows that most of the clutter is correctly identified, as is the true minefield. The sensitivity is \\(= 743/754 = 98.54\\%\\), and the specificity is \\(= (156+144)/350 = 85.71\\%\\).\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 7.2: Model-based clustering of a simulated minefield with noise: (a) initial denoised minefield data; (b) model-based clustering partition obtained with the noise component.\n\n\nBy default mclust uses the function hypvol()  to compute the hypervolume  \\(V\\) in Equation 7.1. This gives a simple approximation to the hypervolume of a multivariate dataset by taking the minimum of the volume hyperrectangle (box) containing the observed data and the box obtained from principal components. In other words, the estimate is the minimum of the Cartesian product of the variable intervals and the Cartesian product of the corresponding projection onto principal components.\n\nhypvol(X)\n## [1] 16022\n\nAnother option is to compute the ellipsoid hull,  namely the ellipsoid of minimal volume such that all observed points lie either inside or on the boundary of the ellipsoid. This method is available via the function ellipsoidhull() from the cluster R package (Maechler et al. 2022):\n\nlibrary(\"cluster\")\nehull = ellipsoidhull(as.matrix(X))\nvolume(ehull)\n## [1] 23150\nmodNoise.ehull = Mclust(X, Vinv = 1/volume(ehull),\n                         initialization = list(noise = (nnc$z == 0)))\nsummary(modNoise.ehull)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVE (ellipsoidal, equal orientation) model with 3 components and\n## a noise term: \n## \n##  log-likelihood    n df    BIC    ICL\n##          -10777 1104 17 -21673 -22618\n## \n## Clustering table:\n##   1   2   3   0 \n## 187 192 302 423\ntab = table(chevron$class, modNoise.ehull$classification)\naddmargins(tab, 2)\n##        \n##           0   1   2   3 Sum\n##   data    0 164 164  22 350\n##   noise 423  23  28 280 754\n\nIn this case the number of clusters identified is too large, yielding a poor recovery of the noise component (sensitivity \\(= 423/754 = 56.1\\%\\), and specificity \\(= (164+164+22)/350 = 100\\%\\)). This behavior is a consequence of the ellipsoid hull over-estimating the data volume (to be expected because the noise is distributed over a square, so that a larger volume ellipsoid is needed to contain all the data points). The hyperrectangle approach discussed above would in all cases be a better approximation to the data volume than the ellipsoid hull. However, the volume of the convex hull of the data better approximates the data volume than either of these approaches.\nThe following code obtains the volume of the convex hull  for the bivariate minefield simulated data, using the convhulln() function from package geometry (Habel et al. 2022):\n\nlibrary(\"geometry\")\nchull = convhulln(X, options = \"FA\")\nchull$vol\n## [1] 15828\n\n\nThe reciprocal of this value may be used as input for the argument Vinv of function Mclust():\n\nmodNoise.chull = Mclust(X, Vinv = 1/chull$vol,\n                        initialization = list(noise = (nnc$z == 0)))\nsummary(modNoise.chull)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEV (ellipsoidal, equal volume and shape) model with 2\n## components and a noise term: \n## \n##  log-likelihood    n df    BIC    ICL\n##          -10515 1104 11 -21108 -21487\n## \n## Clustering table:\n##   1   2   0 \n## 157 149 798\ntab = table(chevron$class, modNoise.chull$classification)\naddmargins(tab, 2)\n##        \n##           0   1   2 Sum\n##   data   55 153 142 350\n##   noise 743   4   7 754\n\nFor this dataset, the default value of the hypervolume and that computed using convhulln() are quite close, so the corresponding estimated models provide essentially the same fit.\nAn issue with the convex hull is that it may not be practical to compute in high dimensions. Moreover, implementations without the option to return the logarithm could result in overflow or underflow depending on the scaling of the data.\n\nOther strategies for obtaining an initial noise estimate could be adopted. The function cov.nnve() in the contributed R package covRobust (Wang, Raftery, and Fraley 2017) is an implementation of robust covariance estimation via nearest neighbor cleaning  (Wang and Raftery 2002). An example of its usage is the following:\n\nlibrary(\"covRobust\")\nnnve = cov.nnve(X, k = 5)\ntable(nnve$classification)\n## \n##    0    1 \n##    5 1099\n\nFor the case of \\(k = 5\\) nearest neighbors, only \\(5/1104 \\approx 0.05\\%\\) of data is classified as noise. A relatively good clustering model is nevertheless obtained with this initial estimate of the noise:\n\nmodNoise.nnve = Mclust(X, initialization = \n                       list(noise = (nnve$classification == 0)))\nsummary(modNoise.nnve$BIC)\n## Best BIC values:\n##           EEV,2        EVE,2       EVV,2\n## BIC      -21128 -21128.20252 -21134.8273\n## BIC diff      0     -0.36423     -6.9891\nsummary(modNoise.nnve)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEV (ellipsoidal, equal volume and shape) model with 2\n## components and a noise term: \n## \n##  log-likelihood    n df    BIC    ICL\n##          -10525 1104 11 -21128 -21509\n## \n## Clustering table:\n##   1   2   0 \n## 160 150 794\naddmargins(table(chevron$class, modNoise.nnve$classification), 2)\n##        \n##           0   1   2 Sum\n##   data   51 153 146 350\n##   noise 743   7   4 754\n\nAlthough the final model is somewhat different from the one previously selected, this one also has 2 components, and the classification of data as signal or noise is almost identical (sensitivity \\(= 743/754 = 98.54\\%\\), and specificity \\(= (153+146)/350 = 85.43\\%\\)). By increasing the number of nearest neighbors \\(k\\) for the initial noise estimate, the results for the cov.nnve() denoising approach those obtained with NNclean.\n\n\nExample 7.2   Clustering with outliers on simulated data\nPeel and McLachlan (2000) developed a simulation-based example in the context of fitting of mixtures of (multivariate) \\(t\\) distributions to model data containing observations with longer than normal tails or atypical observations. The data used in their example is generated as follows:\n\n(Sigma = array(c(2, 0.5, 0.5, 0.5, 1, 0, 0, 0.1, 2, -0.5, -0.5, 0.5), \n                dim = c(2, 2, 3)))\n## , , 1\n## \n##      [,1] [,2]\n## [1,]  2.0  0.5\n## [2,]  0.5  0.5\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    1  0.0\n## [2,]    0  0.1\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]  2.0 -0.5\n## [2,] -0.5  0.5\nvar.decomp = sigma2decomp(Sigma)\nstr(var.decomp)\n## List of 7\n## $ sigma : num [1:2, 1:2, 1:3] 2 0.5 0.5 0.5 1 0 0 0.1 2 -0.5 ...\n## $ d : int 2\n## $ modelName : chr \"VVI\"\n## $ G : int 3\n## $ scale : num [1:3] 0.866 0.316 0.866\n## $ shape : num [1:2, 1:3] 2.484 0.403 3.162 0.316 2.484 ...\n## $ orientation: num [1:2, 1:2] -0.957 -0.29 0.29 -0.957\npar = list(pro = c(1/3, 1/3, 1/3), \n            mean = cbind(c(0, 3), c(3, 0), c(-3, 0)),\n            variance = var.decomp)\ndata = sim(par$variance$modelName, parameters = par, n = 200)\nnoise = matrix(runif(100, -10, 10), nrow = 50, ncol = 2)\nX = rbind(data[, 2:3], noise)\ncluster = c(data[, 1], rep(0, 50))\nclPairs(X, ifelse(cluster == 0, 0, 1), \n        colors = \"black\", symbols = c(16, 1), cex = c(0.5, 1))\n\nIn the code above we used the function sim() to simulate 200 observations from a GMM with the specified mixing probabilities pro, component means mean, and variance decomposition var.decomp obtained using the function sigma2decomp() (see Section 7.4). Then, we added 50 outlying observations from a uniform distribution on \\((-10,10)\\). The simulated data is shown in Figure 7.3 (a).\nWe apply the same procedure adopted for noisy data in Example 7.1. We get an initial estimate of the outlying observations with NNclean(), and then we apply Mclust() to all the data using this estimate of noise in the initialization step:\n\nnnc = NNclean(X, k = 5)\nmodNoise = Mclust(X, initialization = list(noise = (nnc$z == 0)))\nsummary(modNoise$BIC)\n## Best BIC values:\n##            VEI,3      VEE,3      VVI,3\n## BIC      -2243.8 -2249.2536 -2253.0499\n## BIC diff     0.0    -5.4888    -9.2851\nsummary(modNoise, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VEI (diagonal, equal shape) model with 3 components and a noise\n## term: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -1083.2 250 14 -2243.8 -2267.4\n## \n## Clustering table:\n##  1  2  3  0 \n## 66 63 74 47 \n## \n## Mixing probabilities:\n##       1       2       3       0 \n## 0.25532 0.24244 0.28647 0.21578 \n## \n## Means:\n##        [,1]     [,2]     [,3]\n## x1 2.968281 -0.40142 -3.13151\n## x2 0.022729  3.01644  0.13098\n## \n## Variances:\n## [,,1]\n##        x1      x2\n## x1 0.7887 0.00000\n## x2 0.0000 0.12971\n## [,,2]\n##       x1      x2\n## x1 1.896 0.00000\n## x2 0.000 0.31181\n## [,,3]\n##        x1      x2\n## x1 1.9577 0.00000\n## x2 0.0000 0.32195\n## \n## Hypervolume of noise component:\n## 377.35\n\nThe clusters identified are shown in Figure 7.3 (b), and the following code can be used to plot the result and obtain the confusion matrix:\n\nplot(modNoise, what = \"classification\")\ntable(cluster, Classification = modNoise$classification)\n##        Classification\n## cluster  0  1  2  3\n##       0 45  0  3  2\n##       1  2  0 60  0\n##       2  0 64  0  0\n##       3  0  2  0 72\n\nThis shows that the three groups (labeled “1”, “2”, “3”) and the outlying observations (labeled “0”) have been well identified.\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 7.3: Model-based clustering of a bivariate mixture of normal distributions with outlying observations: (a) the simulated data; (b) model-based clustering partition obtained with the noise component. The small black points represent the simulated noise in (a) and the estimated noise in (b).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html#sec-prior",
    "href": "chapters/07_miscellanea.html#sec-prior",
    "title": "7  Miscellanea",
    "section": "\n7.2 Using a Prior for Regularization",
    "text": "7.2 Using a Prior for Regularization\nMaximum likelihood estimation for Gaussian mixtures using the EM algorithm may fail as the result of singularities or degeneracies. These problems can arise depending on the data and the starting values for the EM algorithm, as well as the model parameterizations and number of components specified for the mixture.\nTo address these situations, Chris Fraley and Raftery (2007) proposed replacing the maximum likelihood estimate (MLE) by a maximum a posteriori (MAP) estimate,  also estimated by the EM algorithm. A highly dispersed proper conjugate prior  is used, containing a small fraction of one observation’s worth of information. The resulting method avoids degeneracies and singularities, but when these are not present it gives similar results to the standard maximum likelihood method. The prior also has the effect of smoothing noisy behavior of the BIC, which is often observed in conjunction with instability in estimation.\nFor univariate data, a Gaussian prior on the mean (conditional on the variance) is used: \\[\n\\begin{align}\n\\mu \\mid \\sigma^2\n& \\sim \\mathcal{N}(\\mu_{\\scriptscriptstyle \\mathcal{P}}, \\sigma^2/\\kappa_{\\scriptscriptstyle \\mathcal{P}})\n   \\nonumber\\\\\n& \\propto\n   \\left(\\sigma^2\\right)^{-\\frac{1}{2}}\n         \\exp\\left\\{ - \\frac{\\kappa_{\\scriptscriptstyle \\mathcal{P}}}{2\\sigma^2}\n         \\left(\\mu - \\mu_{\\scriptscriptstyle \\mathcal{P}}\\right)^2\n   \\right\\},  \n\\end{align}\n\\tag{7.2}\\] and an inverse gamma prior on the variance: \\[\n\\begin{align}\n\\sigma^2\n& \\sim \\mbox{inverseGamma}(\\nu_{\\scriptscriptstyle \\mathcal{P}}/2, \\varsigma^2_{\\scriptscriptstyle \\mathcal{P}}/2)\n  \\nonumber\\\\\n& \\propto\n  \\left(\\sigma^2\\right)^{-\\frac{\\nu_{\\scriptscriptstyle \\mathcal{P}}+2}{2}}\n        \\exp \\left\\{ -\\frac{\\varsigma^2_{\\scriptscriptstyle \\mathcal{P}}}{2\\sigma^2}\n   \\right\\}.\n\\end{align}\n\\tag{7.3}\\]\nFor multivariate data, a Gaussian prior on the mean (conditional on the covariance matrix) is used: \\[\n\\begin{align}\n\\boldsymbol{\\mu}\\mid \\boldsymbol{\\Sigma}\n& \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}, \\boldsymbol{\\Sigma}/\\kappa_{\\scriptscriptstyle \\mathcal{P}})\n  \\nonumber\\\\\n& \\propto\n  \\left|\\boldsymbol{\\Sigma}\\right|^{-\\frac{1}{2}}\n  \\exp\\left\\{ -\\frac{\\kappa_{\\scriptscriptstyle \\mathcal{P}}}{2}\n              \\tr\\left(\\left(\\boldsymbol{\\mu}- \\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\right){}^{\\!\\top}\n                       \\boldsymbol{\\Sigma}^{-1}\n                       \\left(\\boldsymbol{\\mu}- \\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\right)\n                  \\right)\n      \\right\\},\n\\end{align}\n\\tag{7.4}\\] together with an inverse Wishart prior on the covariance matrix: \\[\n\\begin{align}\n\\boldsymbol{\\Sigma}\n& \\sim \\mbox{inverseWishart}(\\nu_{\\scriptscriptstyle \\mathcal{P}}, \\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}})\n  \\nonumber\\\\\n& \\propto\n  \\left|\\boldsymbol{\\Sigma}\\right|^{-\\frac{\\nu_{\\scriptscriptstyle \\mathcal{P}}+d+1}{2}}\n  \\exp\\left\\{ -\\frac{1}{2}\n              \\tr\\left(\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}^{-1}\n                 \\right)\n      \\right\\}.\n\\end{align}\n\\tag{7.5}\\]\nThe hyperparameters \\(\\mu_{\\scriptscriptstyle \\mathcal{P}}\\) (in the univariate case) or \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\) (in the multivariate case), \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}\\), and \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}\\) are called the mean, shrinkage, and degrees of freedom, respectively. Parameters \\(\\varsigma^2_{\\scriptscriptstyle \\mathcal{P}}\\) (a scalar, in the univariate case) and \\(\\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}\\) (a matrix, in the multivariate case) are the scale of the prior distribution. These priors are called conjugate priors for the normal distribution because the posterior can be expressed as the product of a normal distribution and an inverse gamma or Wishart distribution.\nWhen a prior is included, a modified version of the BIC must be used to select the number of mixture components and the model parameterization. The BIC is computed as described in Section 2.3.1, but with the log-likelihood evaluated at the MAP instead of at the MLE. Further details on model-based clustering using prior distributions can be found in Chris Fraley and Raftery (2007).\n\n7.2.1 Adding a Prior in mclust\nAlthough maximum likelihood estimation is the default, functions such as Mclust() and mclustBIC() have a prior argument that allows specification of a conjugate prior on the means and variances of the type described above. mclust provides two functions to facilitate inclusion of a prior:\n\npriorControl()  supplies input for the prior argument. It gives the name of the function that defines the desired conjugate prior and specifies values for its arguments.\ndefaultPrior()  is the default function named in priorControl(), as well as a template for specifying conjugate priors in mclust.\n\nThe defaultPrior() function has the following arguments:\n\ndata A numeric vector, matrix, or data frame of observations.\nG An integer value specifying the number of mixture components.\nmodelName A character string indicating the model to be fitted. Note that in the multivariate case only 10 out of 14 models are available, as indicated in Table 3.1.\n\nThe following optional arguments can also be specified:\n\nmean A single value \\(\\mu_{\\scriptscriptstyle \\mathcal{P}}\\) or a vector \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\) of values for the mean parameter of the prior. By default, the sample mean of each variable is used.\nshrinkage The shrinkage parameter \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}\\) for the prior on the mean. By default, shrinkage = 0.01. The posterior mean \\[\n\\frac{n_k \\bar{\\boldsymbol{x}}_k + \\kappa_{\\scriptscriptstyle \\mathcal{P}}\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}}{\\kappa_{\\scriptscriptstyle \\mathcal{P}}+ n_k},\n\\] can be viewed as adding \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}\\) observations with value \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\) to each group in the data. The default value was determined by experimentation; values close to and larger than 1 caused large perturbations in the modeling in cases where there were no missing BIC values without the prior. The value \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}= 0.01\\) resulted in BIC curves that appeared to be smooth extensions of their counterparts without the prior. By setting shrinkage = 0 or shrinkage = NA no prior is assumed for the mean.\ndof The degrees of freedom \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}\\) for the prior on the variance. By analogy to the univariate case, the marginal prior distribution of \\(\\boldsymbol{\\mu}\\) is a \\(t\\) distribution centered at \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\) with \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}- d + 1\\) degrees of freedom. The mean of this distribution is \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\) provided that \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}&gt; d\\), and it has a finite covariance matrix provided \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}&gt; d + 1\\) (see, for example, Joseph L. Schafer 1997). By default, \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}= d+2\\), the smallest integer value for the degrees of freedom that gives a finite covariance matrix.\nscale The scale parameter for the prior on the variance.\nFor univariate models and multivariate spherical or diagonal models, it is computed by default as \\[\n\\varsigma^2_{\\scriptscriptstyle \\mathcal{P}}= \\frac{\\tr(\\boldsymbol{S})/d}{G^{2/d}},\n\\] which is the average of the diagonal elements of the empirical covariance matrix \\(\\boldsymbol{S}\\) of the data, divided by the square of the number of components to the \\(1/d\\) power. This is roughly equivalent to partitioning the range of the data into \\(G\\) intervals of fairly equal size.\nFor multivariate ellipsoidal models, it is computed by default as \\[\n\\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}= \\frac{\\boldsymbol{S}}{G^{2/d}},\n\\] the covariance matrix, divided by the square of the number of components to the \\(1/d\\) power.\n\n\nExample 7.3   Density estimation with a prior on the Galaxies data\nConsider the data on the estimated velocity (km/sec) of 82 galaxies in the Corona Borealis region from Kathryn Roeder (1990), available in the MASS package (Venables and Ripley 2013; Ripley 2022). The interest lies in the identification of multimodality as evidence for clustering structures in the pattern of expansion.\n\ndata(\"galaxies\", package = \"MASS\")\n# now fix a typographical error in the data\n# see help(\"galaxies\", package = \"MASS\")\ngalaxies[78] = 26960 \ngalaxies = galaxies / 1000\n\nWe start the analysis with no prior, but using multiple random starts and retaining the best estimated fit in terms of BIC:\n\nBIC = NULL\nfor(i in 1:50)\n{\n  BIC0 = mclustBIC(galaxies, verbose = FALSE,\n                    initialization = list(hcPairs = hcRandomPairs(galaxies)))\n  BIC  = mclustBICupdate(BIC, BIC0)\n}\nsummary(BIC, k = 5)\n## Best BIC values:\n##              V,6       V,3       V,5       V,4       E,6\n## BIC      -440.06 -442.2177 -442.5948 -443.8970 -447.4629\n## BIC diff    0.00   -2.1543   -2.5314   -3.8335   -7.3994\nplot(BIC)\n\nmod = densityMclust(galaxies, x = BIC)\nsummary(mod, parameters = TRUE)\n## ------------------------------------------------------- \n## Density estimation via Gaussian finite mixture modeling \n## ------------------------------------------------------- \n## \n## Mclust V (univariate, unequal variance) model with 6 components: \n## \n##  log-likelihood  n df     BIC    ICL\n##         -182.57 82 17 -440.06 -447.6\n## \n## Mixing probabilities:\n##        1        2        3        4        5        6 \n## 0.403204 0.085366 0.024390 0.024355 0.036585 0.426099 \n## \n## Means:\n##       1       2       3       4       5       6 \n## 19.7881  9.7101 16.1270 26.9775 33.0443 22.9162 \n## \n## Variances:\n##          1          2          3          4          5          6 \n## 0.45348851 0.17851527 0.00184900 0.00030625 0.84956356 1.45114183\n\nplot(mod, what = \"density\", data = galaxies, breaks = 11)\nrug(galaxies)\n\nThe model with the largest BIC has 6 components, each with a different variance. Components with very small mixing weights correspond to narrow spikes in the density estimate (see Figure 7.4), suggesting them to be spurious.\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 7.4: BIC plot without the prior (a) and density estimate for the (V,6) model selected according to BIC for the galaxies dataset.\n\n\nThe previous analysis can be modified by including the prior as follows:\n\nBICp = NULL\nfor(i in 1:50)\n{\n  BIC0p = mclustBIC(galaxies, verbose = FALSE, \n                     prior = priorControl(),\n            initialization = list(hcPairs = hcRandomPairs(galaxies)))\n  BICp  = mclustBICupdate(BICp, BIC0p)\n}\nsummary(BICp, k = 5)\n## Best BIC values:\n##              V,3      V,4       E,6       E,7       E,3\n## BIC      -443.96 -445.153 -447.6407 -450.6451 -452.0662\n## BIC diff    0.00   -1.196   -3.6833   -6.6878   -8.1088\nplot(BICp)\n\nmodp = densityMclust(galaxies, x = BICp)\nsummary(modp, parameters = TRUE)\n## ------------------------------------------------------- \n## Density estimation via Gaussian finite mixture modeling \n## ------------------------------------------------------- \n## \n## Mclust V (univariate, unequal variance) model with 3 components: \n## \n## Prior: defaultPrior() \n## \n##  log-likelihood  n df     BIC     ICL\n##         -204.35 82  8 -443.96 -443.96\n## \n## Mixing probabilities:\n##        1        2        3 \n## 0.085366 0.036585 0.878050 \n## \n## Means:\n##      1      2      3 \n##  9.726 33.004 21.404 \n## \n## Variances:\n##       1       2       3 \n## 0.36949 0.70599 4.51272\n\nplot(modp, what = \"density\", data = galaxies, breaks = 11)\nrug(galaxies)\n\nFigure 7.5 shows the BIC trace and the density estimate for the selected model. By including the prior, BIC fairly clearly chooses the model with three components and unequal variances. This is in agreement with other analyses reported in the literature (K. Roeder and Wasserman 1997; Chris Fraley and Raftery 2007).\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 7.5: BIC plot with the prior (a) and density estimate for the (V,3) model selected according to the BIC (based on the MAP, not the MLE) for the galaxies dataset.\n\n\nAs explained in Section 7.2.1, the function priorControl() is provided for specifying a prior and its parameters. When used with its default settings, it specifies another function called defaultPrior() which can serve as a template for alternative priors. An example of the result of a call to defaultPrior() for the final model selected is shown below:\n\ndefaultPrior(galaxies, G = 3, modelName = \"V\")\n## $shrinkage\n## [1] 0.01\n## \n## $mean\n## [1] 20.831\n## \n## $dof\n## [1] 3\n## \n## $scale\n## [1] 2.3187\n\n\n\nExample 7.4   Clustering with a prior on the Italian olive oils data\nConsider the olive oils dataset presented in Example 4.8.\n\ndata(\"olive\", package = \"pgmm\")\n# recode of labels for Region and Area\nRegions = c(\"South\", \"Sardinia\", \"North\")\nAreas = c(\"North Apulia\", \"Calabria\", \"South Apulia\", \"Sicily\", \n           \"Inland Sardinia\", \"Coastal Sardinia\", \"East Liguria\", \n           \"West Liguria\", \"Umbria\")\nolive$Region = factor(olive$Region, levels = 1:3, labels = Regions)\nolive$Area = factor(olive$Area, levels = 1:9, labels = Areas)\nwith(olive, table(Area, Region))\n##                   Region\n## Area               South Sardinia North\n##   North Apulia        25        0     0\n##   Calabria            56        0     0\n##   South Apulia       206        0     0\n##   Sicily              36        0     0\n##   Inland Sardinia      0       65     0\n##   Coastal Sardinia     0       33     0\n##   East Liguria         0        0    50\n##   West Liguria         0        0    50\n##   Umbria               0        0    51\n\nWe model the data on the standardized scale as follows:\n\nX = scale(olive[, 3:10])\n\nBIC = mclustBIC(X, G = 1:15)\nsummary(BIC)\n## Best BIC values:\n##           VVE,14     VVE,11    VVE,13\n## BIC      -4461.6 -4471.3336 -4480.081\n## BIC diff     0.0    -9.6941   -18.442\nplot(BIC, legendArgs = list(x = \"bottomright\", ncol = 5))\n\nBICp = mclustBIC(X, G = 1:15, prior = priorControl())\nsummary(BICp)\n## Best BIC values:\n##            VVV,6    VVV,7    VVV,5\n## BIC      -5446.7 -5584.27 -5590.59\n## BIC diff     0.0  -137.55  -143.86\nplot(BICp, legendArgs = list(x = \"bottomright\", ncol = 5))\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 7.6: BIC plot without prior (a) and with prior (b) for the olive oil data.\n\n\nThe BIC plots without and with a prior are shown in Figure 7.6. Without a prior, the model selected has a large number of mixture components, and a fairly poor clustering accuracy:\n\nmod = Mclust(X, x = BIC)\nsummary(mod)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVE (ellipsoidal, equal orientation) model with 14 components: \n## \n##  log-likelihood   n  df     BIC     ICL\n##         -1389.6 572 265 -4461.6 -4504.4\n## \n## Clustering table:\n##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 \n## 29 51 34 65 54 90 66 32 11 29 16 41 38 16\n\ntable(Region = olive$Region, cluster = mod$classification)\n##           cluster\n## Region      1  2  3  4  5  6  7  8  9 10 11 12 13 14\n##   South    29 51 34 65 54 90  0  0  0  0  0  0  0  0\n##   Sardinia  0  0  0  0  0  0 66 32  0  0  0  0  0  0\n##   North     0  0  0  0  0  0  0  0 11 29 16 41 38 16\nadjustedRandIndex(olive$Region, mod$classification)\n## [1] 0.24185\n\ntable(Area = olive$Area, cluster = mod$classification)\n##                   cluster\n## Area                1  2  3  4  5  6  7  8  9 10 11 12 13 14\n##   North Apulia     23  0  1  1  0  0  0  0  0  0  0  0  0  0\n##   Calabria          0 48  2  6  0  0  0  0  0  0  0  0  0  0\n##   South Apulia      0  0  6 57 53 90  0  0  0  0  0  0  0  0\n##   Sicily            6  3 25  1  1  0  0  0  0  0  0  0  0  0\n##   Inland Sardinia   0  0  0  0  0  0 65  0  0  0  0  0  0  0\n##   Coastal Sardinia  0  0  0  0  0  0  1 32  0  0  0  0  0  0\n##   East Liguria      0  0  0  0  0  0  0  0  0  1  4 41  4  0\n##   West Liguria      0  0  0  0  0  0  0  0  0  0  0  0 34 16\n##   Umbria            0  0  0  0  0  0  0  0 11 28 12  0  0  0\nadjustedRandIndex(olive$Area, mod$classification)\n## [1] 0.54197\n\nThe model selected when the prior is included has 6 components with unconstrained variances and a much improved clustering accuracy:\n\nmodp = Mclust(X, x = BICp)\nsummary(modp)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVV (ellipsoidal, varying volume, shape, and orientation) model\n## with 6 components: \n## \n## Prior: defaultPrior() \n## \n##  log-likelihood   n  df     BIC     ICL\n##         -1869.4 572 269 -5446.7 -5451.4\n## \n## Clustering table:\n##   1   2   3   4   5   6 \n## 127 196  98  36  58  57\n\ntable(Region = olive$Region, cluster = modp$classification)\n##           cluster\n## Region       1   2   3   4   5   6\n##   South    127 196   0   0   0   0\n##   Sardinia   0   0  98   0   0   0\n##   North      0   0   0  36  58  57\nadjustedRandIndex(olive$Region, modp$classification)\n## [1] 0.56313\n\ntable(Area = olive$Area, cluster = modp$classification)\n##                   cluster\n## Area                 1   2   3   4   5   6\n##   North Apulia      25   0   0   0   0   0\n##   Calabria          56   0   0   0   0   0\n##   South Apulia      10 196   0   0   0   0\n##   Sicily            36   0   0   0   0   0\n##   Inland Sardinia    0   0  65   0   0   0\n##   Coastal Sardinia   0   0  33   0   0   0\n##   East Liguria       0   0   0   0  43   7\n##   West Liguria       0   0   0   0   0  50\n##   Umbria             0   0   0  36  15   0\nadjustedRandIndex(olive$Area, modp$classification)\n## [1] 0.78261\n\nThe clusters identified correspond approximately to the areas of origin of the olive oil samples. One cluster encompasses both the inland and coastal areas of Sardinia, and two clusters include the southern regions, with one almost completely composed of the south of Apulia. Another three clusters correspond to northern regions: one for all the olive oils from west Liguria and a few from east Liguria, one mostly composed of east Liguria and a few from Umbria, and the last cluster composed only of olive oils from Umbria.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html#sec-nongausclust",
    "href": "chapters/07_miscellanea.html#sec-nongausclust",
    "title": "7  Miscellanea",
    "section": "\n7.3 Non-Gaussian Clusters from GMMs",
    "text": "7.3 Non-Gaussian Clusters from GMMs\nCluster analysis aims to identify groups of similar observations that are relatively separated from each other. In the model-based approach to clustering, data is modeled by a finite mixture of density functions belonging to a given parametric class. Each component is typically associated with a group or cluster. This framework can be extended to non-Gaussian clusters by modeling them with more than one mixture component. This problem was addressed by Baudry et al. (2010), who proposed a method based on an entropy criterion for hierarchically combining mixture components to form clusters.  Hennig (2010) also discussed hierarchical merging methods based on unimodality and misclassification probabilities. Another approach based on the identification of cluster cores corresponding to regions of high density is introduced by Scrucca (2016).\n\n7.3.1 Combining Gaussian Mixture Components for Clustering\nThe function clustCombi()  provides a means for obtaining models with clusters represented by multiple mixture components, as illustrated below.\n\nExample 7.5   Merging Gaussian mixture components on simulated data\nConsider the following simulated two-dimensional dataset with overlapping components as discussed in Baudry et al. (2010, sec. 4.1).\n\ndata(\"Baudry_etal_2010_JCGS_examples\", package = \"mclust\")\nplot(ex4.1)\n\n\n\n\n\n\nFigure 7.7: The ex4.1 simulated dataset.\n\n\n\n\nA sample of 600 observations was generated from a mixture of six Gaussian components, although it would be reasonable to conclude from the spatial distribution of the data that there are only four clusters. The data is shown in Figure 7.7. Two of the clusters are formed from overlapping axis-aligned components (diagonal covariance matrices), while the remaining two clusters are elliptical but not axis-aligned.\nThe estimated model maximizing the BIC criterion is the following:\n\nmod_ex4.1 = Mclust(ex4.1)\nsummary(mod_ex4.1)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEV (ellipsoidal, equal volume and shape) model with 6\n## components: \n## \n##  log-likelihood   n df     BIC     ICL\n##         -1957.8 600 25 -4075.5 -4248.4\n## \n## Clustering table:\n##   1   2   3   4   5   6 \n##  78 122 121 107 132  40\n\nThe function clustCombi(), following the methodology described in Baudry et al. (2010), combines the mixture components hierarchically according to an entropy criterion. The estimated models with numbers of classes ranging from a single class to the number of components selected by BIC are returned as follows:\n\nCLUSTCOMBI = clustCombi(mod_ex4.1)\nsummary(CLUSTCOMBI)\n## ----------------------------------------------------\n## Combining Gaussian mixture components for clustering \n## ----------------------------------------------------\n## \n## Mclust model name: EEV \n## Number of components: 6 \n## \n## Combining steps:\n## \n##   Step | Classes combined at this step | Class labels after this step\n## -------|-------------------------------|-----------------------------\n##    0   |              ---              | 1 2 3 4 5 6 \n##    1   |             3 & 4             | 1 2 3 5 6 \n##    2   |             1 & 6             | 1 2 3 5 \n##    3   |             3 & 5             | 1 2 3 \n##    4   |             1 & 2             | 1 3 \n##    5   |             1 & 3             | 1\n\nThe summary output shows the mixture model EEV with 6 components selected by the BIC criterion, followed by information describing the combining steps. The hierarchy of combined components can be displayed graphically as follows (see Figure 7.8):\n\npar(mfrow = c(3, 2), mar = c(4, 4, 3, 1))\nplot(CLUSTCOMBI, ex4.1, what = \"classification\")\n\n\n\n\n\n\nFigure 7.8: Hierarchy of mixture component combinations for the ex4.1 dataset.\n\n\n\n\nThe process of merging mixture components can also be represented using tree diagrams:\nplot(CLUSTCOMBI, what = \"tree\")\nplot(CLUSTCOMBI, what = \"tree\", type = \"rectangle\", yaxis = \"step\")\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 7.9: Dendrogram tree plots of the merging structure of mixture component combinations for the simulated ex4.1 dataset.\n\n\nFigure 7.9 (a) shows the resulting tree obtained using the default arguments, type = \"triangle\" and yaxis = \"entropy\". The first argument controls the type of dendrogram to plot; two options are available, namely, \"triangle\" and \"rectangle\". The second argument controls the quantity to appear on the \\(y\\)-axis; by default one minus the normalized entropy at each merging step is used. The option yaxis = \"step\" is also available to simply represent heights as successive steps, as shown in Figure 7.9 (b).\nThe clustering structures obtained with clustCombi() can be compared on substantive grounds or by selecting the number of clusters via a piecewise linear regression fit to the (possibly rescaled) “entropy plot”. An automatic procedure to help the user to select the optimal number of clusters using the above methodology based on the entropy is available through the function clustCombiOptim().  For example, the following code\n\noptimClust = clustCombiOptim(CLUSTCOMBI, reg = 2, plot = TRUE)\nstr(optimClust)\n## List of 3\n## $ numClusters.combi: int 4\n## $ z.combi : num [1:600, 1:4] 1.00 1.00 5.78e-04 1.85e-42 3.85e-35 ...\n## $ cluster.combi : num [1:600] 1 1 2 3 3 4 4 3 3 2 ...\n\n\n\n\n\n\nFigure 7.10: Entropy plot for selecting the final clustering by merging mixture components for the simulated ex4.1 dataset.\n\n\n\n\nproduces the entropy plot in Figure 7.10 and returns a list containing the number of clusters (numClusters.combi), the probabilities (z.combi) obtained by summing posterior conditional probabilities over merged components, and the clustering labels (cluster.combi) obtained by merging the mixture components. The optional argument reg is used to specify the number of segments to use in the piecewise linear regression fit. Possible choices are 2 (the default, for a model with two segments or one change point) and 3 (for a model with three segments or two change points). The entropy plot clearly suggests a four-cluster solution.\n\n\n\n\n7.3.2 Identifying Connected Components in GMMs\nA limitation of approaches that successively combine mixture components is that, once merged, components cannot subsequently be reallocated to different clusters at a later stage of the algorithm.\nAmong the several definitions of “cluster” available in the literature, (Hartigan 1975, 205) proposed that “[…] clusters may be thought of as regions of high density separated from other such regions by regions of low density.” From this perspective, Scrucca (2016) defined high density clusters as connected components of density level sets.  Starting with a density estimate obtained from a Gaussian finite mixture model, cluster cores — those data points which form the core of the clusters — are obtained from the connected components at a given density level. A mode function gives the number of connected components as the level is varied. Once cluster cores are identified, the remaining observations are allocated to those cluster cores for which the probability of cluster membership is the highest. Details are provided in Scrucca (2016) and implemented in the gmmhd()  function available in mclust. A non-parametric variant proposed by Azzalini and Menardi (2014) is implemented in pdfCluster (Menardi and Azzalini 2022).\n\nExample 7.6   Identifying high density clusters on the Old Faithful data\nAs a first example, consider the Old Faithful data described in Example 3.4.\n\ndata(\"faithful\", package = \"datasets\")\nmod = Mclust(faithful)\nclPairs(faithful, mod$classification)\nplot(as.densityMclust(mod), what = \"density\", add = TRUE)\n\nFigure 7.11 (a) shows the clustering partition obtained from the best estimated GMM according to BIC. The model selected is (EEE,3), a mixture of three components with common full covariance matrix. The portion of the data with high values for both waiting and eruptions is fitted with two Gaussian components. However, there appear to be two separate groups of points in the data as a whole, and the corresponding bivariate density estimate shown in Figure 7.11 (a) indicates the presence of two separate regions of high density.\nThe GMMHD approach (Scrucca 2016) briefly outlined at the beginning of this section is implemented in the gmmhd() function available in mclust. This function requires as input an object returned by Mclust() or densityMclust() as the initial GMM. Many other input parameters can be set, and the interested reader should consult the documentation available through help(\"gmmhd\").\nWe apply the GMMHD algorithm to our example by invoking function gmmhd() and producing its summary output as follows:\n\nGMMHD = gmmhd(mod)\nsummary(GMMHD)\n## ---------------------------------------------------------\n## GMM with high-density connected components for clustering \n## ---------------------------------------------------------\n## \n## Initial model:  Mclust (EEE,3)\n## \n## Cluster cores:\n##    1    2 &lt;NA&gt; \n##  166   91   15 \n## \n## Final clustering:\n##   1   2 \n## 178  94\n\nStarting with the density estimate from the initial (EEE,3) Gaussian mixture model, gmmhd() computes the empirical mode function and the connected components at different density levels, which are then used to identify cluster cores. Figure 7.11 (b) plots the number of modes as a function of the proportion of data points above a threshold density level. There is a clear indication of a bimodal distribution. Figure 7.11 (c) shows the points assigned to the cluster cores, as well as the remaining data points not initially allocated.\nThe summary above shows that 166 and 91 observations have been assigned to the two cluster cores, and that 15 points were initially unlabeled. Figure 7.11 (d) shows the final clustering obtained after the unlabeled data have also been allocated to the cluster cores. Figure 7.11 (b)–Figure 7.11 (d) can be produced with the following commands:\n\nplot(GMMHD, what = \"mode\")\nplot(GMMHD, what = \"cores\")\nplot(GMMHD, what = \"clusters\")\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 7.11: Identification of connected components for the Old Faithful data: (a) clustering and density contours for the initial GMM; (b) mode function; (c) cluster cores (marked as \\(\\bullet\\) and \\(\\square\\)) and unlabeled data (marked as \\(\\circ\\)); (d) final clustering obtained after unlabeled data have been allocated.\n\n\n\n\nExample 7.7   Simulated data {.unnumbered #sec-sim_data}\nWe now compare the results of the GMMHD procedure with the component combination methods of Section 7.3.1 on the simulated dataset from that section.\n\ndata(\"Baudry_etal_2010_JCGS_examples\", package = \"mclust\")\nmod = Mclust(ex4.1)\nGMMHD = gmmhd(mod)\nsummary(GMMHD)\n## ---------------------------------------------------------\n## GMM with high-density connected components for clustering \n## ---------------------------------------------------------\n## \n## Initial model:  Mclust (EEV,6)\n## \n## Cluster cores:\n##    1    2    3    4 &lt;NA&gt; \n##  100  116  111  206   67 \n## \n## Final clustering:\n##   1   2   3   4 \n## 118 133 122 227\noptimClust = clustCombiOptim(clustCombi(mod), reg = 2)\ntable(GMMHD = GMMHD$cluster, CLUSTCOMBI = optimClust$cluster)\n##      CLUSTCOMBI\n## GMMHD   1   2   3   4\n##     1 118   0   0   0\n##     2   0   0   1 132\n##     3   0 122   0   0\n##     4   0   0 227   0\n\nThe GMMHD procedure identifies essentially the same four clusters as clustCombi(), with the exception of a single point on the edge of the two clusters on the left of graphs in Figure 7.8.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html#sec-sim",
    "href": "chapters/07_miscellanea.html#sec-sim",
    "title": "7  Miscellanea",
    "section": "\n7.4 Simulation from Mixture Densities",
    "text": "7.4 Simulation from Mixture Densities\nGiven the parameters of a mixture model, data can be simulated from that model for evaluation and verification. The mclust function sim()  enables simulation from all of the structured GMMs supported. Besides the model, sim() allows a seed to be input for reproducibility.\n\nExample 7.8   Simulating data from GMM estimated from the Old Faithful data\nIn the example below we simulate two different datasets of the same size as the faithful dataset from the model returned by Mclust():\n\nmod = Mclust(faithful)\nsim0 = sim(modelName = mod$modelName, \n            parameters = mod$parameters,\n            n = nrow(faithful), seed = 0)\nsim1 = sim(modelName = mod$modelName, \n            parameters = mod$parameters,\n            n = nrow(faithful), seed = 1)\n\nThe results can be plotted as follows (graphs not shown):\n\nxlim = range(c(faithful[, 1], sim0[, 2], sim1[, 2]))\nylim = range(c(faithful[, 2], sim0[, 3], sim1[, 3]))\nmclust2Dplot(data = sim0[, -1], parameters = mod$parameters,\n             classification = sim0[, 1], xlim = xlim, ylim = ylim)\nmclust2Dplot(data = sim1[, -1], parameters = mod$parameters,\n             classification = sim1[, 1], xlim = xlim, ylim = ylim)\n\nNote that sim() produces a dataset in which the first column is the true classification:\n\nhead(sim0)\n##      group     x1     x2\n## [1,]     1 4.1376 71.918\n## [2,]     3 4.4703 77.198\n## [3,]     3 4.5090 74.671\n## [4,]     2 1.9596 49.380\n## [5,]     1 4.0607 78.773\n## [6,]     3 4.3597 82.190",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html#large-datasets",
    "href": "chapters/07_miscellanea.html#large-datasets",
    "title": "7  Miscellanea",
    "section": "\n7.5 Large Datasets",
    "text": "7.5 Large Datasets\nGaussian mixture modeling may in practice be too slow to be applied directly to datasets having a large number of observations or cases. In order to extend the method to larger datasets, both Mclust() and mclustBIC() include a provision for using a subsample of the data in the initial hierarchical clustering phase before applying the EM algorithm to the full dataset. Other functions in the mclust package also have this feature. Some alternatives for handling large datasets are discussed by Wehrens et al. (2004) and C. Fraley, Raftery, and Wehrens (2005).\nStarting with version 5.4 of mclust, a subsample of size specified by mclust.options(\"subset\") is used in the initial hierarchical clustering phase. The default subset size is 2000. The subsampling can be removed by the command mclust.options(subset = Inf). The subset size can be specified for all the models in the current session through the subset  argument to mclust.options(). Furthermore, the subsampling approach can also be implemented in the Mclust() function by setting initialization = list(subset = s), where s is a vector of logical values or numerical indices specifying the subset of data to be used in the initial hierarchical clustering phase.\n\nExample 7.9   Clustering on a simulated large dataset\nConsider a simulated dataset of \\(n=10 000\\) points from a three-component Gaussian mixture with common covariance matrix. First, we create a list, par, of the model parameters to be used in the simulation:\n\npar = list(\n  pro = c(0.5, 0.3, 0.2),\n  mean = matrix(c(0, 0, 3, 3, -4, 1), nrow = 2, ncol = 3),\n  variance = sigma2decomp(matrix(c(1, 0.6, 0.6, 1.5), nrow = 2, ncol = 2), \n                          G = 3))\nstr(par)\n## List of 3\n## $ pro : num [1:3] 0.5 0.3 0.2\n## $ mean : num [1:2, 1:3] 0 0 3 3 -4 1\n## $ variance:List of 7\n## ..$ sigma : num [1:2, 1:2, 1:3] 1 0.6 0.6 1.5 1 0.6 0.6 1.5 1 0.6 ...\n## ..$ d : int 2\n## ..$ modelName : chr \"EEI\"\n## ..$ G : num 3\n## ..$ scale : num 1.07\n## ..$ shape : num [1:2] 1.78 0.562\n## ..$ orientation: num [1:2, 1:2] 0.555 0.832 -0.832 0.555\n\nWe then simulate the data with the sim() function as described in Section 7.4:\n\nsim = sim(modelName = \"EEI\", parameters = par, n = 10000, seed = 123)\ncluster = sim[, 1]\nx = sim[, 2:3]\n\nThe following code is used to fit GMMs with no subsampling, using the default subsampling, and a random sample of 1000 observations provided to the Mclust() function:\n\nmclust.options(subset = Inf)  # no subsetting\nsystem.time(mod1 = Mclust(x))\n##    user  system elapsed \n## 172.981   0.936 173.866\nsummary(mod1$BIC)\n## Best BIC values:\n##              EEI,3         EEE,3        EVI,3\n## BIC      -76523.89 -76531.836803 -76541.59835\n## BIC diff      0.00     -7.948006    -17.70955\n\nmclust.options(subset = 2000)  # reset to default setting\nsystem.time(mod2 = Mclust(x))\n##  user  system elapsed \n## 14.247   0.196  14.395 \nsummary(mod2$BIC)\n## Best BIC values:\n##              EEI,3         EEE,3        EVI,3\n## BIC      -76524.09 -76531.845624 -76541.89691\n## BIC diff      0.00     -7.754063    -17.80535\n\ns = sample(1:nrow(x), size = 1000)  # one-time subsetting\nsystem.time(mod3 = Mclust(x, initialization = list(subset = s)))\n##   user  system elapsed \n## 12.091   0.146  12.197 \nsummary(mod3$BIC)\n## Best BIC values:\n##              EEI,3         EEE,3        VEI,3\n## BIC      -76524.17 -76532.043460 -76541.78983\n## BIC diff      0.00     -7.874412    -17.62078\n\nFunction system.time() gives a rough indication of the computational effort associated without and with subsampling. In the latter case, using a random sample of 20% of the full set of observations, we have been able to obtain a \\(10\\) fold speedup, ending up with essentially the same final clustering model:\n\ntable(mod1$classification, mod2$classification)\n##       1    2    3\n##  1 5090    0    0\n##  2    0    0 3007\n##  3   10 1893    0\n\n\nThe same strategies that we have described for clustering very large datasets can also be used for classification. First, discriminant analysis with MclustDA() can be performed on a subset of the data. Then, the remaining data points can be classified (in reasonable sized blocks) using the associated predict() method.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html#high-dimensional-data",
    "href": "chapters/07_miscellanea.html#high-dimensional-data",
    "title": "7  Miscellanea",
    "section": "\n7.6 High-Dimensional Data",
    "text": "7.6 High-Dimensional Data\nModels in which the orientation is allowed to vary between components (EEV, VEV, EVV, and VVV), have \\({\\cal O}(d^2)\\) parameters per cluster, where \\(d\\) is the dimension of the data (see Table 2.2 and Figure 2.3). For this reason, mclust may not work well or may otherwise be inefficient for these models when applied to high-dimensional data. It may still be possible to analyze such data with mclust by restricting to models with fewer parameters (spherical or diagonal models) or else by applying a dimension-reduction technique such as principal components analysis. Moreover, some of the more parsimonious models (spherical, diagonal, or fixed covariance) can be applied to datasets in which the number of observations is less than the data dimension.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/07_miscellanea.html#missing-data",
    "href": "chapters/07_miscellanea.html#missing-data",
    "title": "7  Miscellanea",
    "section": "\n7.7 Missing Data",
    "text": "7.7 Missing Data\nAt present, mclust has no direct provision for handling missing values in data. However, a function imputeData()  for creating datasets with missing data imputations using the mix R package (Joseph L. Schafer 2022) has been added to the mclust package.\nThe algorithm used in imputeData() starts with a preliminary maximum likelihood estimation step to obtain model parameters for the subset of complete observations in the data. These are used as initial values for a data augmentation step for generating posterior draws of parameters via Markov Chain Monte Carlo. Finally, missing values are imputed with simulated values drawn from the predictive distribution of the missing data given the observed data and the simulated parameters.\n\nExample 7.10   Imputation on the child development data\nThe stlouis dataset, available in the mix package, provides data from an observational study to assess the affects of parental psychological disorders on child development. Here we use imputeData() to fill in missing values in the continuous portion of the stlouis dataset; we remove the first 3 categorical variables, because mclust is intended for continuous variables.\n\ndata(\"stlouis\", package = \"mix\")\nx = data.frame(stlouis[, -(1:3)], row.names = NULL)\ntable(complete.cases(x))\n## \n## FALSE  TRUE \n##    42    27\napply(x, 2, function(x) prop.table(table(complete.cases(x))))\n##            R1      V1      R2      V2\n## FALSE 0.30435 0.43478 0.23188 0.24638\n## TRUE  0.69565 0.56522 0.76812 0.75362\n\nThe dataset contains only 27 complete cases out of 69 observations (39%). For each variable, the percentage of missing values ranges from 23% to 43%. The pattern of missing values can be displayed graphically using the following code (see Figure 7.12):\n\nlibrary(\"ggplot2\")\ndf = data.frame(obs = rep(1:nrow(x), times = ncol(x)),\n                var = rep(colnames(x), each = nrow(x)),\n                missing = as.vector(is.na(x)))\nggplot(data = df, aes(x = var, y = obs)) +\n  geom_tile(aes(fill = missing)) +\n  scale_fill_manual(values = c(\"lightgrey\", \"black\")) +\n  labs(x = \"Variables\", y = \"Observations\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(margin = margin(b = 10))) +\n  theme(axis.ticks.y = element_blank()) +\n  theme(axis.text.y = element_blank())\n\n\n\n\n\n\nFigure 7.12: Image plot of the missing values pattern for the stlouis dataset.\n\n\n\n\nA single imputation of simulated values drawn from the predictive distribution of the missing data is easily obtained as\n\nximp = imputeData(x, seed = 123)\n\nNote that the values obtained for the missing entries will vary depending on the random number seed set as argument in the imputeData() function call. By default, a randomly chosen seed is used.\nMissing data imputations can be visualized with the function imputePairs()  as follows:\n\nimputePairs(x, ximp)\n\n\n\n\n\n\nFigure 7.13: Pairs plot showing an instance of applying imputeData() to the continuous variables in the stlouis dataset. The open black circles correspond to non-missing data, while the filled red circles correspond to imputed missing values.\n\n\n\n\nThe corresponding pairs plot is shown in Figure 7.13.\n\nIt is usually desirable to combine multiple imputations in analyses involving missing data. See Joseph L. Schafer (1997), Little and Rubin (2002), and van Buuren and Groothuis-Oudshoorn (2011) for details and references on multiple imputation. Besides mix, R packages for handling missing data include Amelia (Honaker, King, and Blackwell 2011) and mice (van Buuren 2012).\n\nExample 7.11   Clustering with missing values on the Iris data\nWe now illustrate a model-based clustering analysis in the presence of missing values. Consider the iris data, and suppose that we want to fit a three-component model with common full covariance matrix. The model using the full set of observations is obtained as follows:\n\nx = as.matrix(iris[, 1:4])\nmod = Mclust(x, G = 3, modelNames = \"EEE\")\ntable(iris$Species, mod$classification)\n##             \n##               1  2  3\n##   setosa     50  0  0\n##   versicolor  0 48  2\n##   virginica   0  1 49\nadjustedRandIndex(iris$Species, mod$classification)\n## [1] 0.94101\nmod$parameters$mean  # component means\n##               [,1]   [,2]   [,3]\n## Sepal.Length 5.006 5.9426 6.5749\n## Sepal.Width  3.428 2.7607 2.9811\n## Petal.Length 1.462 4.2596 5.5394\n## Petal.Width  0.246 1.3194 2.0254\n\nHere we assume that the interest is in the final clustering partition and in the corresponding component means. Other mixture parameters, such as mixing probabilities and (co)variances, can be dealt with analogously.\nNow, let us randomly designate 10% of the data values as missing:\n\nisMissing = sample(c(TRUE, FALSE), size = prod(dim(x)), \n                    replace = TRUE, prob = c(0.1, 0.9))\nx[isMissing] = NA\ntable(cmpObs = complete.cases(x))\n## cmpObs\n## FALSE  TRUE \n##    50   100\n\nThe last command shows that 100 out of 150 observations have no missing values. We can use multiple imputation to create nImp complete datasets by replacing the missing values with plausible data values. Then, an (EEE,3) clustering model is fitted to each of these datasets, and the results are pooled to get the final estimates.\nA couple of issues should be mentioned. First, a well-known issue of finite mixture modeling is nonidentifiability of the components due to invariance to relabeling. This is solved in the code below using matchCluster(), which takes the first clustering result as a template for adjusting the labeling of the successive clusters. The adjusted ordering is also used for matching the other estimated parameters.\nA second issue is related to how pooling of results is carried out at the end of the imputation procedure. For the clustering labels, the majority vote rule is used as implemented in function majorityVote(), whereas a simple average of estimates is calculated for pooling results for the mean parameters.\n\nnImp  = 100\nmuImp = array(NA, c(ncol(x), 3, nImp))\nclImp = array(NA, c(nrow(x), nImp))\nfor(i in 1:nImp)\n{ \n  xImp = imputeData(x, verbose = FALSE)\n  modImp = Mclust(xImp, G = 3, modelNames = \"EEE\", verbose = FALSE)\n  if (i == 1) clImp[, i] = modImp$classification\n  mcl = matchCluster(clImp[, 1], modImp$classification)\n  clImp[, i]  = mcl$cluster\n  muImp[, , i] = modImp$parameters$mean[, mcl$ord]\n}\n\n# majority rule\ncl = apply(clImp, 1, function(x) majorityVote(x)$majority)\ntable(iris$Species, cl)\n##             cl\n##               1  2  3\n##   setosa     50  0  0\n##   versicolor  0 49  1\n##   virginica   0  5 45\nadjustedRandIndex(iris$Species, cl)\n## [1] 0.88579\n\n# pooled estimate of cluster means\napply(muImp, 1:2, mean)\n##         [,1]   [,2]   [,3]\n## [1,] 4.99885 5.9388 6.5667\n## [2,] 3.44614 2.7684 2.9675\n## [3,] 1.48357 4.2783 5.5453\n## [4,] 0.25557 1.3333 2.0267\n\nIt is interesting to note that both the final clustering obtained by majority vote and cluster means obtained by pooling results from imputed datasets are quite close to those obtained on the original dataset.\n\n\n\n\n\nAllard, Denis, and Chris Fraley. 1997. “Nonparametric Maximum Likelihood Estimation of Features in Spatial Point Processes Using Voronoı̈ Tessellation.” Journal of the American Statistical Association 92 (440): 1485–93.\n\n\nAzzalini, Adelchi, and Giovanna Menardi. 2014. “Clustering via Nonparametric Density Estimation: The R Package pdfCluster.” Journal of Statistical Software 57 (11): 1–26. http://www.jstatsoft.org/v57/i11/.\n\n\nBanfield, J., and Adrian E. Raftery. 1993. “Model-Based Gaussian and Non-Gaussian Clustering.” Biometrics 49: 803–21.\n\n\nBaudry, J. P., A. E. Raftery, G. Celeux, K. Lo, and R. Gottardo. 2010. “Combining Mixture Components for Clustering.” Journal of Computational and Graphical Statistics 19 (2): 332–53.\n\n\nByers, Simon, and Adrian E Raftery. 1998. “Nearest-Neighbor Clutter Removal for Estimating Features in Spatial Point Processes.” Journal of the American Statistical Association 93 (442): 577–84.\n\n\nCampbell, JG, Chris Fraley, Fionn Murtagh, and Adrian E Raftery. 1997. “Linear Flaw Detection in Woven Textiles Using Model-Based Clustering.” Pattern Recognition Letters 18 (14): 1539–48.\n\n\nCampbell, Jonathan G, Chris Fraley, D Stanford, Fionn Murtagh, and Adrian E Raftery. 1999. “Model-Based Methods for Textile Fault Detection.” International Journal of Imaging Systems and Technology 10 (4): 339–46.\n\n\nCoretto, Pietro, and Christian Hennig. 2016. “Robust Improper Maximum Likelihood: Tuning, Computation, and a Comparison with Other Methods for Robust Gaussian Clustering.” Journal of the American Statistical Association 111 (516): 1648–59.\n\n\nDasgupta, Abhijit, and Adrian E Raftery. 1998. “Detecting Features in Spatial Point Processes with Clutter via Model-Based Clustering.” Journal of the American Statistical Association 93 (441): 294–302.\n\n\nDotto, Francesco, and Alessio Farcomeni. 2019. “Robust Inference for Parsimonious Model-Based Clustering.” Journal of Statistical Computation and Simulation 89 (3): 414–42.\n\n\nFraley, Chris, and Adrian E Raftery. 2007. “Bayesian Regularization for Normal Mixture Estimation and Model-Based Clustering.” Journal of Classification 24 (2): 155–81.\n\n\nFraley, C., and A. E. Raftery. 1998. “How Many Clusters? Which Clustering Method? Answers via Model-Based Cluster Analysis.” The Computer Journal 41: 578–88.\n\n\nFraley, C., A. E. Raftery, and R. Wehrens. 2005. “Incremental Model-Based Clustering for Large Datasets with Small Clusters.” Journal of Computational and Graphical Statistics 14 (3): 529–46.\n\n\nGarcı́a-Escudero, Luis A., Alfonso Gordaliza, Carlos Matràn, and Agustin Mayo-Iscar. 2008. “A General Trimming Approach to Robust Cluster Analysis.” Annals of Statistics 36 (3): 1324–45.\n\n\nHabel, Kai, Raoul Grasman, Robert B. Gramacy, Pavlo Mozharovskyi, and David C. Sterratt. 2022. geometry: Mesh Generation and Surface Tessellation. https://CRAN.R-project.org/package=geometry.\n\n\nHartigan, J. A. 1975. Clustering Algorithms. New York: John Wiley & Sons.\n\n\nHennig, Christian. 2010. “Methods for Merging Gaussian Mixture Components.” Advances in Data Analysis and Classification 4 (1): 3–34.\n\n\nHennig, Christian, and Bernhard Hausdorf. 2020. Prabclus: Functions for Clustering and Testing of Presence-Absence, Abundance and Multilocus Genetic Data. https://CRAN.R-project.org/package=prabclus.\n\n\nHonaker, James, Gary King, and Matthew Blackwell. 2011. “Amelia II: A Program for Missing Data.” Journal of Statistical Software 45 (7): 1–47. http://www.jstatsoft.org/v45/i07/.\n\n\nLittle, Roderick JA, and Donald B Rubin. 2002. Statistical Analysis with Missing Data. 2nd ed. John Wiley & Sons.\n\n\nMaechler, Martin, Peter Rousseeuw, Anja Struyf, Mia Hubert, and Kurt Hornik. 2022. cluster: Cluster Analysis Basics and Extensions. https://CRAN.R-project.org/package=cluster.\n\n\nMcLachlan, G. J., and D. Peel. 2000. Finite Mixture Models. New York: Wiley.\n\n\nMenardi, Giovanna, and Adelchi Azzalini. 2022. pdfCluster: Cluster Analysis via Nonparametric Density Estimation. https://CRAN.R-project.org/package=pdfCluster.\n\n\nPeel, David, and Geoffrey J McLachlan. 2000. “Robust Mixture Modelling Using the \\(t\\) Distribution.” Statistics and Computing 10 (4): 339–48.\n\n\nPunzo, Antonio, and Paul D. McNicholas. 2016. “Parsimonious Mixtures of Multivariate Contaminated Normal Distributions.” Biometrical Journal 58 (6): 1506–37. https://doi.org/10.1002/bimj.201500144.\n\n\nRipley, Brian. 2022. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS.\n\n\nRoeder, Kathryn. 1990. “Density Estimation with Confidence Sets Exemplified by Superclusters and Voids in the Galaxies.” Journal of the American Statistical Association 85 (411): 617–24.\n\n\nRoeder, K., and L. Wasserman. 1997. “Practical Bayesian Density Estimation Using Mixtures of Normals.” Journal of the American Statistical Association 92 (439): 894–902.\n\n\nSchafer, Joseph L. 1997. Analysis of Incomplete Multivariate Data. London: Chapman & Hall/CRC.\n\n\nSchafer, Joseph L. 2022. mix: Estimation/Multiple Imputation for Mixed Categorical and Continuous Data. https://CRAN.R-project.org/package=mix.\n\n\nScrucca, Luca. 2016. “Identifying Connected Components in Gaussian Finite Mixture Models for Clustering.” Computational Statistics & Data Analysis 93: 5–17. https://doi.org/10.1016/j.csda.2015.01.006.\n\n\nvan Buuren, Stef. 2012. Flexible Imputation of Missing Data. Chapman & Hall/CRC.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45 (3): 1–67. http://www.jstatsoft.org/v45/i03/.\n\n\nVenables, William N., and Brian D. Ripley. 2013. Modern Applied Statistics with S-PLUS. Springer Science; Business Media.\n\n\nWang, Naisyin, and Adrian E Raftery. 2002. “Nearest Neighbor Variance Estimation (NNVE): Robust Covariance Estimation via Nearest Neighbor Cleaning (with Discussion).” Journal of the American Statistical Association 97 (460): 994–1019.\n\n\nWang, Naisyin, Adrian Raftery, and Chris Fraley. 2017. covRobust: Robust Covariance Estimation via Nearest Neighbor Cleaning. https://CRAN.R-project.org/package=covRobust.\n\n\nWehrens, R., L. M. C. Buydens, C. Fraley, and A. E. Raftery. 2004. “Model-Based Clustering for Image Segmentation and Large Datasets via Sampling.” Journal of Classification 21 (2): 231–53.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Miscellanea</span>"
    ]
  },
  {
    "objectID": "chapters/99_references.html",
    "href": "chapters/99_references.html",
    "title": "References",
    "section": "",
    "text": "Allard, Denis, and Chris Fraley. 1997. “Nonparametric Maximum\nLikelihood Estimation of Features in Spatial Point Processes Using\nVoronoı̈ Tessellation.” Journal of\nthe American Statistical Association 92 (440): 1485–93.\n\n\nAlpaydin, Ethem. 2014. Introduction to Machine Learning. 3rd\ned. MIT Press.\n\n\nAltman, Edward I. 1968. “Financial Ratios, Discriminant Analysis\nand the Prediction of Corporate Bankruptcy.” The Journal of\nFinance 23 (4): 589–609.\n\n\nAmeijeiras-Alonso, Jose, Rosa M. Crujeiras, Alberto Rodríguez-Casal, The\nR Core Team 1996-2012, and The R Foundation 2005. 2021. multimode: Mode Testing and Exploring. https://CRAN.R-project.org/package=multimode.\n\n\nAnderson, E. 1935. “The Irises of the\nGaspé Peninsula.”\nBulletin of the American Iris Society 59: 2–5.\n\n\nAnderson, T. W., I. Olkin, and L. G. Underhill. 1987. “Generation\nof Random Orthogonal Matrices.” SIAM Journal on Scientific\nand Statistical Computing 8 (4): 625–29.\n\n\nArbel, Julyan, Guillaume Kon Kam King, Antonio Lijoi, Luis\nNieto-Barajas, and Igor Prünster. 2021. “BNPdensity:\nBayesian Nonparametric Mixture Modelling in R.”\nAustralian & New Zealand Journal of Statistics 63 (3):\n542–64.\n\n\nAzzalini, A, and A W Bowman. 1990. “A Look at Some Data on the Old\nFaithful Geyser.” Applied Statistics 39 (3): 357–65.\n\n\nAzzalini, Adelchi, and Giovanna Menardi. 2014. “Clustering via\nNonparametric Density Estimation: The R Package pdfCluster.” Journal of Statistical\nSoftware 57 (11): 1–26. http://www.jstatsoft.org/v57/i11/.\n\n\nBanfield, J., and Adrian E. Raftery. 1993. “Model-Based\nGaussian and Non-Gaussian Clustering.”\nBiometrics 49: 803–21.\n\n\nBarrios, Ernesto, Guillaume Kon Kam King, Antonio Lijoi, Luis E.\nNieto-Barajas, and Igor Prünster. 2021. BNPdensity:\nFerguson-Klass Type Algorithm for Posterior Normalized Random\nMeasures. https://doi.org/10.1111/anzs.12342.\n\n\nBasford, K E, D R Greenway, G J McLachlan, and D Peel. 1997.\n“Standard Errors of Fitted Component Means of Normal\nMixtures.” Computational Statistics 12 (1): 1–18.\n\n\nBates, Stephen, Trevor Hastie, and Robert Tibshirani. 2021.\n“Cross-Validation: What Does It Estimate and How Well Does It Do\nIt?” arXiv Preprint. https://arxiv.org/abs/2104.00673.\n\n\nBaudry, J. P., A. E. Raftery, G. Celeux, K. Lo, and R. Gottardo. 2010.\n“Combining Mixture Components for Clustering.” Journal\nof Computational and Graphical Statistics 19 (2): 332–53.\n\n\nBensmail, H., and G. Celeux. 1996. “Regularized\nGaussian Discriminant Analysis Through Eigenvalue\nDecomposition.” Journal of the American Statistical\nAssociation 91: 1743–48.\n\n\nBiernacki, C., G. Celeux, and G. Govaert. 2000. “Assessing a\nMixture Model for Clustering with the Integrated Completed\nLikelihood.” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 22 (7): 719–25.\n\n\nBiernacki, Christophe, Gilles Celeux, and Gérard Govaert. 2003.\n“Choosing Starting Values for the EM Algorithm for\nGetting the Highest Likelihood in Multivariate Gaussian\nMixture Models.” Computational Statistics & Data\nAnalysis 41 (3): 561–75.\n\n\nBishop, Christopher. 2006. Pattern Recognition and Machine\nLearning. New York: Springer-Verlag Inc.\n\n\nBoldea, Otilia, and Jan R Magnus. 2009. “Maximum Likelihood\nEstimation of the Multivariate Normal Mixture Model.” Journal\nof the American Statistical Association 104 (488): 1539–49.\n\n\nBouveyron, Charles, Gilles Celeux, T. Brendan Murphy, and Adrian E.\nRaftery. 2019. Model-Based Clustering and Classification for Data\nScience: With Applications in r. Cambridge Series in Statistical\nand Probabilistic Mathematics. Cambridge: Cambridge University Press.\n\n\nBowman, A. W., and A. Azzalini. 1997. Applied Smoothing Techniques\nfor Data Analysis. Oxford: Oxford University Press.\n\n\nBowman, Adrian, Adelchi Azzalini. Ported to R by B. D. Ripley up to\nversion 2.0, version 2.1 by Adrian Bowman, Adelchi Azzalini, and version\n2.2 by Adrian Bowman. 2022. sm:\nSmoothing Methods for Nonparametric Regression and Density\nEstimation. https://CRAN.R-project.org/package=sm.\n\n\nBox, G. E., and D. R. Cox. 1964. “An Analysis of\nTransformations.” Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology) 26 (2): 211–52.\n\n\nBreiman, L., J. Friedman, R. Olshen, and C. J. Stone. 1984.\nClassification and Regression Trees. New York: Wadsworth.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in\nTerms of Probability.” Monthly Weather Review 78 (1):\n1–3.\n\n\nBrowne, Ryan P, and Paul D McNicholas. 2014. “Estimating Common\nPrincipal Components in High Dimensions.” Advances in Data\nAnalysis and Classification 8 (2): 217–26.\n\n\nByers, Simon, and Adrian E Raftery. 1998. “Nearest-Neighbor\nClutter Removal for Estimating Features in Spatial Point\nProcesses.” Journal of the American Statistical\nAssociation 93 (442): 577–84.\n\n\nCampbell, JG, Chris Fraley, Fionn Murtagh, and Adrian E Raftery. 1997.\n“Linear Flaw Detection in Woven Textiles Using Model-Based\nClustering.” Pattern Recognition Letters 18 (14):\n1539–48.\n\n\nCampbell, Jonathan G, Chris Fraley, D Stanford, Fionn Murtagh, and\nAdrian E Raftery. 1999. “Model-Based Methods for Textile Fault\nDetection.” International Journal of Imaging Systems and\nTechnology 10 (4): 339–46.\n\n\nCassie, Richard Morrison. 1954. “Some Uses of Probability Paper in\nthe Analysis of Size Frequency Distributions.” Marine and\nFreshwater Research 5 (3): 513–22.\n\n\nCeleux, G., and G. Govaert. 1995. “Gaussian\nParsimonious Clustering Models.” Pattern Recognition 28:\n781–93.\n\n\nClaeskens, Gerda, and Nils Lid Hjort. 2008. Model Selection and\nModel Averaging. Cambridge: Cambridge University Press.\n\n\nCoomans, D, and I Broeckaert. 1986. Potential Pattern Recognition in\nChemical and Medical Decision Making. Letchworth, England: Research\nStudies Press.\n\n\nCoretto, Pietro, and Christian Hennig. 2016. “Robust Improper\nMaximum Likelihood: Tuning, Computation, and a Comparison\nwith Other Methods for Robust Gaussian Clustering.”\nJournal of the American Statistical Association 111 (516):\n1648–59.\n\n\nCsárdi, Gábor. 2019. cranlogs: Download Logs\nfrom the ’RStudio’ ’CRAN’ Mirror. https://CRAN.R-project.org/package=cranlogs.\n\n\nCzekanowski, J. 1909. “Zur Differential-Diagnose Der\nNeadertalgruppe.” Korrespondenz-Blatt Der\nDeutschen Geselleschaft Für Anthropologie, Ethnologie, Und\nUrgeschichte 40: 44–47.\n\n\nDasgupta, Abhijit, and Adrian E Raftery. 1998. “Detecting Features\nin Spatial Point Processes with Clutter via Model-Based\nClustering.” Journal of the American Statistical\nAssociation 93 (441): 294–302.\n\n\nDavis, J., and M. Goadrich. 2006. “The Relationship Between\nPrecision-Recall and ROC Curves.” In Proceedings\nof the 23rd International Conference on Machine Learning, 233–40.\n\n\nDean, Nema, Thomas Brendan Murphy, and Gerard Downey. 2006. “Using\nUnlabelled Data to Update Classification Rules with Applications in Food\nAuthenticity Studies.” Journal of the Royal Statistical\nSociety: Series C (Applied Statistics) 55 (1): 1–14.\n\n\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. “Maximum\nLikelihood from Incomplete Data via the EM Algorithm (with\nDiscussion).” Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology) 39: 1–38.\n\n\nDotto, Francesco, and Alessio Farcomeni. 2019. “Robust Inference\nfor Parsimonious Model-Based Clustering.” Journal of\nStatistical Computation and Simulation 89 (3): 414–42.\n\n\nDua, Dheeru, and Casey Graff. 2017. “UCI\nMachine Learning\nRepository.” University of California, Irvine,\nSchool of Information; Computer Sciences. http://archive.ics.uci.edu/ml.\n\n\nDuong, Tarn. 2022. ks: Kernel\nSmoothing. https://CRAN.R-project.org/package=ks.\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the\nJackknife.” Annals of Statistics 7: 1–26.\n\n\nEscobar, Michael D, and Mike West. 1995. “Bayesian Density\nEstimation and Inference Using Mixtures.” Journal of the\nAmerican Statistical Association 90 (430): 577–88.\n\n\nFerguson, Thomas. 1983. “Bayesian Density Estimation by Mixtures\nof Normal Distributions.” In Recent Advances in\nStatistics, edited by M. Haseeb Rizvi, Jagdish S. Rustagi, and\nDavid Siegmund, 287–302. Academic Press.\n\n\nFlury, Bernard. 1997. A First Course in Multivariate\nStatistics. New York: Springer.\n\n\nFlury, Bernhard. 1988. Common Principal Components & Related\nMultivariate Models. John Wiley & Sons, Inc.\n\n\nFlury, Bernhard, and Hans Riedwyl. 1988. Multivariate Statistics: A\nPractical Approach. Chapman & Hall Ltd.\n\n\nForina, M., C. Armanino, M. Castino, and M. Ubigli. 1986.\n“Multivariate Data Analysis as a Discriminating Method of the\nOrigin of Wines.” Vitis 25: 189–201. ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine.\n\n\nForina, M., C. Armanino, S. Lanteri, and E. Tiscornia. 1983.\n“Classification of Olive Oils from Their Fatty Acid\nComposition.” In Food Research and Data Analysis, edited\nby H. Martens and H. Russwurm Jr., 189–214. London: Applied Science\nPublishers.\n\n\nFraley, Chris. 1998. “Algorithms for Model-Based\nGaussian Hierarchical Clustering.”\nSIAM Journal on Scientific Computing 20 (1):\n270–81.\n\n\nFraley, Chris, and Adrian E Raftery. 1999. “MCLUST:\nSoftware for Model-Based Cluster Analysis.” Journal of\nClassification 16 (2): 297–306.\n\n\n———. 2003. “Enhanced Model-Based Clustering, Density Estimation,\nand Discriminant Analysis Software: MCLUST.”\nJournal of Classification 20 (2): 263–86.\n\n\n———. 2007. “Bayesian Regularization for Normal Mixture Estimation\nand Model-Based Clustering.” Journal of Classification\n24 (2): 155–81.\n\n\nFraley, Chris, Adrian E. Raftery, Thomas Brendan Murphy, and Luca\nScrucca. 2012. “MCLUST Version 4 for R:\nNormal Mixture Modeling for Model-Based Clustering, Classification, and\nDensity Estimation.” Technical Report 597. Department of\nStatistics, University of Washington.\n\n\nFraley, Chris, Adrian E. Raftery, and Luca Scrucca. 2022. mclust: Gaussian Mixture Modelling for Model-Based\nClustering, Classification, and Density Estimation. https://CRAN.R-project.org/package=mclust.\n\n\nFraley, C., and A. E. Raftery. 1998. “How Many Clusters?\nWhich Clustering Method? Answers via\nModel-Based Cluster Analysis.” The Computer Journal 41:\n578–88.\n\n\n———. 2002. “Model-Based Clustering, Discriminant Analysis, and\nDensity Estimation.” Journal of the American Statistical\nAssociation 97 (458): 611–31.\n\n\nFraley, C., A. E. Raftery, and R. Wehrens. 2005. “Incremental\nModel-Based Clustering for Large Datasets with Small Clusters.”\nJournal of Computational and Graphical Statistics 14 (3):\n529–46.\n\n\nFriedman, Herman P, and Jerrold Rubin. 1967. “On Some Invariant\nCriteria for Grouping Data.” Journal of the American\nStatistical Association 62 (320): 1159–78.\n\n\nFrühwirth-Schnatter, Sylvia. 2006. Finite Mixture and Markov\nSwitching Models. Springer.\n\n\nGarcı́a-Escudero, Luis A., Alfonso Gordaliza, Carlos Matràn, and Agustin\nMayo-Iscar. 2008. “A General Trimming Approach to Robust Cluster\nAnalysis.” Annals of Statistics 36 (3): 1324–45.\n\n\nGarcı́a-Escudero, Luis Angel, Alfonso Gordaliza, Carlos Matrán, and\nAgustı́n Mayo-Iscar. 2015. “Avoiding Spurious Local Maximizers in\nMixture Modeling.” Statistics and Computing 25 (3):\n619–33.\n\n\nGnanadesikan, R. 1977. Methods for Statistical Data Analysis of\nMultivariate Observations. New York: John Wiley & Sons.\n\n\nGneiting, Tilmann, and Adrian E Raftery. 2007. “Strictly Proper\nScoring Rules, Prediction, and Estimation.” Journal of the\nAmerican Statistical Association 102 (477): 359–78.\n\n\nGrau, Jan, Ivo Grosse, and Jens Keilwagen. 2015.\n“PRROC: Computing and Visualizing Precision-Recall\nand Receiver Operating Characteristic Curves in R.”\nBioinformatics 31 (15): 2595–97.\n\n\nHabbema, J D F, J. Hermans, and K van den Broek. 1974. “A Stepwise\nDiscriminant Analysis Program Using Density Estimation.” In\nProceedings in Computational Statistics, 101–10. Vienna:\nPhysica-Verlag: COMPSTAT.\n\n\nHabel, Kai, Raoul Grasman, Robert B. Gramacy, Pavlo Mozharovskyi, and\nDavid C. Sterratt. 2022. geometry: Mesh\nGeneration and Surface Tessellation. https://CRAN.R-project.org/package=geometry.\n\n\nHärdle, Wolfgang Karl. 1991. Smoothing Techniques: With\nImplementation in S. Springer Science & Business\nMedia.\n\n\nHartigan, J. A. 1975. Clustering Algorithms. New York: John\nWiley & Sons.\n\n\nHastie, Trevor, and Robert Tibshirani. 1996. “Discriminant\nAnalysis by Gaussian Mixtures.” Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology) 58\n(1): 155–76.\n\n\nHastie, T., R. Tibshirani, and J. H. Friedman. 2009. The Elements of\nStatistical Learning: Data Mining, Inference, and Prediction. 2nd\ned. Springer-Verlag. http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/.\n\n\nHathaway, Richard J. 1985. “A Constrained Formulation of\nMaximum-Likelihood Estimation for Normal Mixture Distributions.”\nAnnals of Statistics 13: 795–800.\n\n\nHeiberger, Richard M. 1978. “Algorithm AS 127:\nGeneration of Random Orthogonal Matrices.” Journal of the\nRoyal Statistical Society. Series C (Applied Statistics) 27 (2):\n199–206.\n\n\nHennig, Christian. 2010. “Methods for Merging\nGaussian Mixture Components.” Advances in Data\nAnalysis and Classification 4 (1): 3–34.\n\n\nHennig, Christian, and Bernhard Hausdorf. 2020. Prabclus: Functions\nfor Clustering and Testing of Presence-Absence, Abundance and Multilocus\nGenetic Data. https://CRAN.R-project.org/package=prabclus.\n\n\nHonaker, James, Gary King, and Matthew Blackwell. 2011.\n“Amelia II: A Program for Missing Data.”\nJournal of Statistical Software 45 (7): 1–47. http://www.jstatsoft.org/v45/i07/.\n\n\nHubert, L., and P. Arabie. 1985. “Comparing Partitions.”\nJournal of Classification 2: 193–218.\n\n\nHurley, Catherine. 2019. gclus:\nClustering Graphics. https://CRAN.R-project.org/package=gclus.\n\n\nHyndman, Rob J. 1996. “Computing and Graphing Highest Density\nRegions.” The American Statistician 50 (2): 120–26.\n\n\nIhaka, Ross, Paul Murrell, Kurt Hornik, Jason C. Fisher, Reto Stauffer,\nClaus O. Wilke, Claire D. McWhite, and Achim Zeileis. 2022.\nColorspace: A Toolbox for Manipulating and Assessing Colors and\nPalettes. https://doi.org/10.18637/jss.v096.i01.\n\n\nIngrassia, Salvatore, and Roberto Rocci. 2007. “Constrained\nMonotone EM Algorithms for Finite Mixture of Multivariate\nGaussians.” Computational Statistics & Data\nAnalysis 51 (11): 5339–51.\n\n\nIzenman, Alan J, and Charles J Sommer. 1988. “Philatelic Mixtures\nand Multimodal Densities.” Journal of the American\nStatistical Association 83 (404): 941–53.\n\n\nKass, R. E., and A. E. Raftery. 1995. “Bayes Factors.”\nJournal of the American Statistical Association 90: 773–95.\n\n\nKeilwagen, Jens, Ivo Grosse, and Jan Grau. 2014. “Area Under\nPrecision-Recall Curves for Weighted and Unweighted Data.”\nPLOS ONE 9 (3).\n\n\nKeribin, C. 2000. “Consistent Estimation of the Order of Mixture\nModels.” Sankhya Series A 62 (1): 49–66.\n\n\nKohavi, Ron. 1995. “A Study of Cross-Validation and Bootstrap for\nAccuracy Estimation and Model Selection.” In Proceedings of\nthe 14th International Joint Conference on Artificial Intelligence -\nVolume 2, 1137–43. IJCAI’95. San Francisco, CA, USA: Morgan\nKaufmann Publishers Inc.\n\n\nKonishi, Sadanori, and Genshiro Kitagawa. 2008. Information Criteria\nand Statistical Modeling. Springer Science & Business Media.\n\n\nKruppa, Jochen, Yufeng Liu, Gérard Biau, Michael Kohler, Inke R König,\nJames D Malley, and Andreas Ziegler. 2014. “Probability Estimation\nwith Machine Learning Methods for Dichotomous and Multicategory Outcome:\nTheory.” Biometrical Journal 56 (4): 534–63.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive\nModeling. New York: Springer. https://doi.org/10.1007/978-1-4614-6849-3.\n\n\nLangrognet, Florent, Remi Lebret, Christian Poli, Serge Iovleff,\nBenjamin Auder, and Serge Iovleff. 2022. Rmixmod:\nClassification with Mixture Modelling. https://CRAN.R-project.org/package=Rmixmod.\n\n\nLantz, Brett. 2019. Machine Learning with R: Expert\nTechniques for Predictive Modeling. 3rd ed. Packt Publishing.\n\n\nLazarsfeld, Paul F. 1950a. “The Logical and Mathematical\nFoundation of Latent Structure Analysis.” In Measurement and\nPrediction, Volume IV of the American Soldier: Studies in Social\nPsychology in World War II, edited by S. A. Stouffer. Princeton\nUniversity Press.\n\n\n———. 1950b. “The Logical and Mathematical Foundation of Latent\nStructure Analysis.” In Measurement and Prediction,\nedited by S. A. Stouffer, 362–412. Princeton University Press.\n\n\nLittle, Roderick JA, and Donald B Rubin. 2002. Statistical Analysis\nwith Missing Data. 2nd ed. John Wiley & Sons.\n\n\nLoader, C. 1999. Local Regression and Likelihood. New York:\nSpringer Verlag.\n\n\nMaechler, Martin, Peter Rousseeuw, Anja Struyf, Mia Hubert, and Kurt\nHornik. 2022. cluster: Cluster Analysis\nBasics and Extensions. https://CRAN.R-project.org/package=cluster.\n\n\nMangasarian, Olvi L, W Nick Street, and William H Wolberg. 1995.\n“Breast Cancer Diagnosis and Prognosis via Linear\nProgramming.” Operations Research 43 (4): 570–77.\n\n\nMarron, J Steve, and Matt P Wand. 1992. “Exact Mean Integrated\nSquared Error.” Annals of Statistics 20 (2): 712–36.\n\n\nMcLachlan, G. J., and T. Krishnan. 2008. The EM\nAlgorithm and Extensions. 2nd ed. Hoboken, New Jersey:\nWiley-Interscience.\n\n\nMcLachlan, G. J., and D. Peel. 2000. Finite Mixture Models. New\nYork: Wiley.\n\n\nMcLachlan, Geoffrey. 2004. Discriminant Analysis and Statistical\nPattern Recognition. New York: John Wiley & Sons.\n\n\nMcLachlan, Geoffrey J. 1987. “On Bootstrapping the Likelihood\nRatio Test Statistic for the Number of Components in a Normal\nMixture.” Applied Statistics 36: 318–24.\n\n\nMcLachlan, Geoffrey J, and Kaye E Basford. 1988. Mixture Models:\nInference and Applications to Clustering. New York: Marcel Dekker\nInc.\n\n\nMcLachlan, Geoffrey John. 1977. “Estimating the Linear\nDiscriminant Function from Initial Samples Containing a Small Number of\nUnclassified Observations.” Journal of the American\nStatistical Association 72 (358): 403–6.\n\n\nMcLachlan, Geoffrey J, and Suren Rathnayake. 2014. “On the Number\nof Components in a Gaussian Mixture Model.”\nWiley Interdisciplinary Reviews: Data Mining and Knowledge\nDiscovery 4 (5): 341–55.\n\n\nMcNeil, D. R. 1977. Interactive Data Analysis. New York: Wiley.\n\n\nMcNicholas, Paul D. 2016. Mixture Model-Based Classification.\nCRC Press.\n\n\nMcNicholas, Paul D., Aisha ElSherbiny, Aaron F. McDaid, and T. Brendan\nMurphy. 2022. pgmm: Parsimonious\nGaussian Mixture Models. https://CRAN.R-project.org/package=pgmm.\n\n\nMenardi, Giovanna, and Adelchi Azzalini. 2022. pdfCluster: Cluster Analysis via Nonparametric\nDensity Estimation. https://CRAN.R-project.org/package=pdfCluster.\n\n\nMeng, Xiao-Li, and Donald B Rubin. 1991. “Using EM to\nObtain Asymptotic Variance-Covariance Matrices: The SEM\nAlgorithm.” Journal of the American Statistical\nAssociation 86 (416): 899–909.\n\n\nMorris, Katherine, and Paul D. McNicholas. 2013. “Dimension\nReduction for Model-Based Clustering via Mixtures of Shifted Asymmetric\nLaplace Distributions.” Statistics &\nProbability Letters 83 (9): 2088–93. https://doi.org/http://dx.doi.org/10.1016/j.spl.2013.04.011.\n\n\n———. 2016. “Clustering, Classification, Discriminant Analysis, and\nDimension Reduction via Generalized Hyperbolic Mixtures.”\nComputational Statistics & Data Analysis 97: 133–50.\nhttps://doi.org/http://dx.doi.org/10.1016/j.csda.2015.10.008.\n\n\nMorris, Katherine, PaulD. McNicholas, and Luca Scrucca. 2013.\n“Dimension Reduction for Model-Based Clustering via Mixtures of\nMultivariate t-Distributions.” Advances in Data Analysis and\nClassification 7 (3): 321–38. https://doi.org/10.1007/s11634-013-0137-3.\n\n\nMurtagh, Fionn, and Adrian E Raftery. 1984. “Fitting Straight\nLines to Point Patterns.” Pattern Recognition 17 (5):\n479–83.\n\n\nNeath, Andrew A., and Joseph E. Cavanaugh. 2012. “The\nBayesian Information Criterion: Background, Derivation, and\nApplications.” Wiley Interdisciplinary Reviews: Computational\nStatistics 4 (2): 199–203. https://doi.org/10.1002/wics.199.\n\n\nNewton, Michael A, and Adrian E Raftery. 1994. “Approximate\nBayesian Inference with the Weighted Likelihood Bootstrap\n(with Discussion).” Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology) 56: 3–48.\n\n\nO’Hagan, Adrian, Thomas Brendan Murphy, Luca Scrucca, and Isobel Claire\nGormley. 2019. “Investigation of Parameter Uncertainty in\nClustering Using a Gaussian Mixture Model via Jackknife,\nBootstrap and Weighted Likelihood Bootst Rap.” Computational\nStatistics 34 (4): 1779–1813. https://doi.org/10.1007/s00180-019-00897-9.\n\n\nO’Neill, Terence J. 1978. “Normal Discrimination with Unclassified\nObservations.” Journal of the American Statistical\nAssociation 73 (364): 821–26.\n\n\nOgle, Derek. 2022. FSAdata: Fisheries Stock Analysis,\nDatasets.\n\n\nOkabe, Masataka, and Kei Ito. 2008. “Color Universal Design\n(CUD) - How to Make Figures and Presentations That Are\nFriendly to Colorblind People.” J* Fly: Data Depository for\nDrosophila Researchers. https://jfly.uni-koeln.de/color/.\n\n\nPeel, David, and Geoffrey J McLachlan. 2000. “Robust Mixture\nModelling Using the t\nDistribution.” Statistics and Computing 10 (4): 339–48.\n\n\nPosse, C. 2001. “Hierarchical Model-Based Clustering for Large\nDatasets.” Journal of Computational and Graphical\nStatistics 10 (3): 464–86.\n\n\nPunzo, Antonio, and Paul D. McNicholas. 2016. “Parsimonious\nMixtures of Multivariate Contaminated Normal Distributions.”\nBiometrical Journal 58 (6): 1506–37. https://doi.org/10.1002/bimj.201500144.\n\n\nR Core Team. 2022. R: A Language and Environment for\nStatistical Computing. Vienna, Austria: R Foundation\nfor Statistical Computing. https://www.R-project.org/.\n\n\nRaftery, Adrian E., and N. Dean. 2006. “Variable Selection for\nModel-Based Clustering.” Journal of the American Statistical\nAssociation 101 (473): 168–78.\n\n\nReaven, GM, and RG Miller. 1979. “An Attempt to Define the Nature\nof Chemical Diabetes Using a Multidimensional Analysis.”\nDiabetologia 16 (1): 17–24.\n\n\nRichardson, Sylvia, and Peter J Green. 1997. “On\nBayesian Analysis of Mixtures with an Unknown Number of\nComponents (with Discussion).” Journal of the Royal\nStatistical Society: Series B (Statistical Methodology) 59 (4):\n731–92.\n\n\nRipley, Brian. 2022. MASS: Support Functions and\nDatasets for Venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS.\n\n\nRoeder, Kathryn. 1990. “Density Estimation with Confidence Sets\nExemplified by Superclusters and Voids in the Galaxies.”\nJournal of the American Statistical Association 85 (411):\n617–24.\n\n\nRoeder, K., and L. Wasserman. 1997. “Practical\nBayesian Density Estimation Using Mixtures of\nNormals.” Journal of the American Statistical\nAssociation 92 (439): 894–902.\n\n\nSaerens, Marco, Patrice Latinne, and Christine Decaestecker. 2002.\n“Adjusting the Outputs of a Classifier to New a Priori\nProbabilities: A Simple Procedure.” Neural Computation\n14 (1): 21–41.\n\n\nSchafer, Joseph L. 1997. Analysis of Incomplete Multivariate\nData. London: Chapman & Hall/CRC.\n\n\nSchafer, Joseph L. 2022. mix:\nEstimation/Multiple Imputation for Mixed Categorical and Continuous\nData. https://CRAN.R-project.org/package=mix.\n\n\nSchwartz, G. 1978. “Estimating the Dimension of a Model.”\nAnnals of Statistics 6: 31–38.\n\n\nScott, AJ, and Michael J Symons. 1971. “Clustering Methods Based\non Likelihood Ratio Criteria.” Biometrics 27 (2):\n387–97.\n\n\nScott, David W. 2009. Multivariate Density Estimation: Theory,\nPractice, and Visualization. 2nd ed. John Wiley & Sons.\n\n\nScrucca, L., and A. E. Raftery. 2015. “Improved Initialisation of\nModel-Based Clustering Using Gaussian Hierarchical\nPartitions.” Advances in Data Analysis and\nClassification 4 (9): 447–60. https://doi.org/10.1007/s11634-015-0220-z.\n\n\nScrucca, Luca. 2010. “Dimension Reduction for Model-Based\nClustering.” Statistics and Computing 20 (4): 471–84. https://doi.org/10.1007/s11222-009-9138-7.\n\n\n———. 2014. “Graphical Tools for Model-Based Mixture Discriminant\nAnalysis.” Advances in Data Analysis and Classification\n8 (2): 147–65. https://doi.org/10.1007/s11634-013-0147-1.\n\n\n———. 2016. “Identifying Connected Components in\nGaussian Finite Mixture Models for Clustering.”\nComputational Statistics & Data Analysis 93: 5–17. https://doi.org/10.1016/j.csda.2015.01.006.\n\n\n———. 2019. “A Transformation-Based Approach to\nGaussian Mixture Density Estimation for Bounded\nData.” Biometrical Journal 61 (4): 1–16. https://doi.org/10.1002/bimj.201800174.\n\n\n———. 2022. mclustAddons: Addons for the\n’Mclust’ Package. https://CRAN.R-project.org/package=mclustAddons.\n\n\nScrucca, Luca, Michael Fop, Thomas Brendan Murphy, and Adrian E.\nRaftery. 2016. “mclust 5: Clustering,\nClassification and Density Estimation Using Gaussian Finite\nMixture Models.” The R Journal 8 (1):\n205–33. https://journal.r-project.org/archive/2016-1/scrucca-fop-murphy-etal.pdf.\n\n\nScrucca, Luca, and Alessio Serafini. 2019. “Projection Pursuit\nBased on Gaussian Mixtures and Evolutionary\nAlgorithms.” Journal of Computational and Graphical\nStatistics 28 (4): 847–60. https://doi.org/10.1080/10618600.2019.1598871.\n\n\nSilverman, Bernard W. 1998. Density Estimation for Statistics and\nData Analysis. Chapman & Hall/CRC.\n\n\nSimonoff, J S. 1996. Smoothing Methods in Statistics. Springer.\n\n\nSing, T., O. Sander, N. Beerenwinkel, and T. Lengauer. 2005.\n“ROCR: Visualizing Classifier Performance in\nR.” Bioinformatics 21 (20): 7881. http://rocr.bioinf.mpi-sb.mpg.de.\n\n\nStahl, D., and H. Sallis. 2012. “Model-Based Cluster\nAnalysis.” Wiley Interdisciplinary Reviews: Computational\nStatistics 4 (4): 341–58. https://doi.org/10.1002/wics.1204.\n\n\nStreet, W Nick, William H Wolberg, and Olvi L Mangasarian. 1993.\n“Nuclear Feature Extraction for Breast Tumor Diagnosis.” In\nBiomedical Image Processing and Biomedical Visualization,\n1905:861–70. International Society for Optics; Photonics.\n\n\nTitterington, D Michael, Adrian FM Smith, and Udi E Makov. 1985.\nStatistical Analysis of Finite Mixture Distributions.\nChichester; New York: John Wiley & Sons.\n\n\nTodorov, Valentin. 2022. rrcov: Scalable\nRobust Estimators with High Breakdown Point. https://CRAN.R-project.org/package=rrcov.\n\n\nTortora, Cristina, Aisha ElSherbiny, Ryan P. Browne, Brian C. Franczak,\nand Paul D. McNicholas, and Donald D. Amos. 2022.\nMixGHD: Model Based Clustering, Classification and\nDiscriminant Analysis Using the Mixture of Generalized Hyperbolic\nDistributions. https://CRAN.R-project.org/package=MixGHD.\n\n\nUnwin, Antony. 2015. GDAdata: Datasets for the Book\nGraphical Data Analysis with R. https://CRAN.R-project.org/package=GDAdata.\n\n\nvan Buuren, Stef. 2012. Flexible Imputation of Missing Data.\nChapman & Hall/CRC.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations\nin r.” Journal of Statistical Software 45 (3): 1–67. http://www.jstatsoft.org/v45/i03/.\n\n\nVenables, William N., and Brian D. Ripley. 2013. Modern Applied\nStatistics with S-PLUS. Springer Science; Business\nMedia.\n\n\nWand, Matt. 2021. KernSmooth: Functions for Kernel\nSmoothing Supporting Wand & Jones (1995). https://CRAN.R-project.org/package=KernSmooth.\n\n\nWang, Naisyin, and Adrian E Raftery. 2002. “Nearest Neighbor\nVariance Estimation (NNVE): Robust Covariance Estimation\nvia Nearest Neighbor Cleaning (with Discussion).” Journal of\nthe American Statistical Association 97 (460): 994–1019.\n\n\nWang, Naisyin, Adrian Raftery, and Chris Fraley. 2017. covRobust: Robust Covariance Estimation via\nNearest Neighbor Cleaning. https://CRAN.R-project.org/package=covRobust.\n\n\nWard, Joe H. 1963. “Hierarchical Grouping to Optimize an Objective\nFunction.” Journal of the American Statistical\nAssociation 58 (301): 236–44.\n\n\nWehrens, R., L. M. C. Buydens, C. Fraley, and A. E. Raftery. 2004.\n“Model-Based Clustering for Image Segmentation and Large Datasets\nvia Sampling.” Journal of Classification 21 (2): 231–53.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. 2nd ed. New York: Springer-Verlag. http://ggplot2.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain Francois, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Dianne Cook. 2022. tourr: Implement Tour Methods in r Code. https://CRAN.R-project.org/package=tourr.\n\n\nWickham, Hadley, and Lionel Henry. 2022. tidyr: Tidy Messy Data. https://doi.org/10.18637/jss.v059.i10.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. New York:\nSpringer-Verlag. https://doi.org/10.1007/0-387-28695-0.\n\n\nWolfe, John H. 1963. “Object Cluster Analysis of Social\nAreas.” PhD thesis, Berkeley: University of California.\n\n\n———. 1965. “A Computer Program for the Maximum Likelihood Analysis\nof Types.” {USNPRA} {Technical} {Bulletin} 65-15. U.S. Naval\nPersonnel Research Activity, San Diego, CA.\n\n\n———. 1967. “NORMIX: Computational Methods for\nEstimating the Parameters of Multivariate Normal Mixtures of\nDistributions.” Naval Personnel Research Activity San Diego CA.\n\n\n———. 1970. “Pattern Clustering by Multivariate Mixture\nAnalysis.” Multivariate Behavioral Research 5: 329–50.\n\n\nWong, Bang. 2011. “Points of View: Color Blindness.”\nNature Methods 8 (441). https://doi.org/0.1038/nmeth.1618.\n\n\nZeileis, Achim, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D.\nMcWhite, Paul Murrell, Reto Stauffer, and Claus O. Wilke. 2020.\n“colorspace: A Toolbox for\nManipulating and Assessing Colors and Palettes.” Journal of\nStatistical Software 96 (1): 1–49. https://doi.org/10.18637/jss.v096.i01.\n\n\nZhu, Xiaojin, and Andrew B Goldberg. 2009. Introduction to\nSemi-Supervised Learning. Vol. 3. Synthesis Lectures on Artificial\nIntelligence and Machine Learning. Morgan & Claypool Publishers.",
    "crumbs": [
      "References"
    ]
  }
]